{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Representation Learning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Waleed-Daud/Fairness/blob/master/Representation_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePN5RKlup4EI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.contrib import eager\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ss1XEnYlOIoB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "4206c991-844d-413e-8c8f-fc2783941991"
      },
      "source": [
        "pip install grl_op_grads"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting grl_op_grads\n",
            "\u001b[31m  ERROR: Could not find a version that satisfies the requirement grl_op_grads (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for grl_op_grads\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fl0xCXPNN_Bu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "a6675cfe-f7de-417a-de14-2780a94b8c9d"
      },
      "source": [
        "# MMD special imports\n",
        "\n",
        "from functools import partial\n",
        "import utils\n",
        "slim = tf.contrib.slim"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-19c69d54e22b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfunctools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mslim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KraeQIjqLJH",
        "colab_type": "text"
      },
      "source": [
        "## Q1. Let’s start by just looking at the marginal distribution of each feature in each group (A = 0, A = 1). For each feature, fit a Gaussian to that feature for each group – this should give us two Gaussians (parameters μ 0 , σ 0 or μ 1 , σ 1 ) for each feature. Then, we’ll use these simple distributions to preprocess the features of group A = 0 and 1 so they more closely match each other. For each feature x for a point in group A = a, let the pre-processed a feature x 0 = x−μ σ a . This pre-processing step should match the first two moments of the features of each group. As in the previous question, learn a classifier g to predict Y and a classifier h to predict A from this pre-processed dataset. Report the accuracy and ∆ DP for g and accuracy and reweighted accuracy for h. What happened?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAO4CQm2qzka",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.enable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQ_542cfq1Gu",
        "colab_type": "code",
        "outputId": "59497c2f-7978-4bb1-ef4e-c64ef3fa49dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "! git clone  https://Waleed-Daud:369074125800925025880dobeedoz.22@github.com/Waleed-Daud/Fairness.git\n",
        "% cd Fairness"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Fairness'...\n",
            "remote: Enumerating objects: 16, done.\u001b[K\n",
            "remote: Counting objects: 100% (16/16), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 16 (delta 4), reused 9 (delta 1), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (16/16), done.\n",
            "/content/Fairness\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cl3Nujjq2np3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def filter_dataframe(dataset,key,label):\n",
        "  \n",
        "  female_data_binary = dataset[key] == 1\n",
        "  male_data_binary = dataset[key] == 0\n",
        "  \n",
        "  male_data = dataset[male_data_binary]\n",
        "  female_data = dataset[female_data_binary]\n",
        "  \n",
        "  sex_male_label = male_data[label]\n",
        "  sex_female_label = female_data[label]\n",
        "  \n",
        "  sex_male_data  = male_data.drop(['y','A'], axis =1)\n",
        "  sex_female_data  = female_data.drop(['y','A'], axis =1)\n",
        "  \n",
        "  \n",
        "  return sex_male_data,sex_male_label, sex_female_data,sex_female_label\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def filter_dataframe_dp(dataset,key,label):\n",
        "  \n",
        "  if key =='sex_Female':\n",
        "    female_data_binary = dataset[key] == 1\n",
        "    male_data_binary = dataset[key] == 0\n",
        "  \n",
        "    male_data = dataset[male_data_binary]\n",
        "    female_data = dataset[female_data_binary]\n",
        "    \n",
        "  else:\n",
        "    \n",
        "    female_data_binary = dataset['A'] == 1\n",
        "    male_data_binary = dataset['A'] == 0\n",
        "  \n",
        "    male_data = dataset[male_data_binary]\n",
        "    female_data = dataset[female_data_binary]\n",
        "  \n",
        "  \n",
        "  sex_male_label = male_data[label]\n",
        "  sex_female_label = female_data[label]\n",
        "  \n",
        "  \n",
        "  A_male = male_data['A']\n",
        "  A_female = female_data['A']\n",
        "  \n",
        "  \n",
        "  sex_male_data  = male_data.drop(['y','A'], axis =1)\n",
        "  sex_female_data  = female_data.drop(['y','A'], axis =1)\n",
        "  \n",
        "  \n",
        "  return sex_male_data,sex_male_label,A_male  , sex_female_data,sex_female_label,A_female\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def maximum_mean_discrepancy(x, y, kernel=utils.gaussian_kernel_matrix):\n",
        "  r\"\"\"Computes the Maximum Mean Discrepancy (MMD) of two samples: x and y.\n",
        "  Maximum Mean Discrepancy (MMD) is a distance-measure between the samples of\n",
        "  the distributions of x and y. Here we use the kernel two sample estimate\n",
        "  using the empirical mean of the two distributions.\n",
        "  MMD^2(P, Q) = || \\E{\\phi(x)} - \\E{\\phi(y)} ||^2\n",
        "              = \\E{ K(x, x) } + \\E{ K(y, y) } - 2 \\E{ K(x, y) },\n",
        "  where K = <\\phi(x), \\phi(y)>,\n",
        "    is the desired kernel function, in this case a radial basis kernel.\n",
        "  Args:\n",
        "      x: a tensor of shape [num_samples, num_features]\n",
        "      y: a tensor of shape [num_samples, num_features]\n",
        "      kernel: a function which computes the kernel in MMD. Defaults to the\n",
        "              GaussianKernelMatrix.\n",
        "  Returns:\n",
        "      a scalar denoting the squared maximum mean discrepancy loss.\n",
        "  \"\"\"\n",
        "  with tf.name_scope('MaximumMeanDiscrepancy'):\n",
        "    # \\E{ K(x, x) } + \\E{ K(y, y) } - 2 \\E{ K(x, y) }\n",
        "    cost = tf.reduce_mean(kernel(x, x))\n",
        "    cost += tf.reduce_mean(kernel(y, y))\n",
        "    cost -= 2 * tf.reduce_mean(kernel(x, y))\n",
        "\n",
        "    # We do not allow the loss to become negative.\n",
        "    cost = tf.where(cost > 0, cost, 0, name='value')\n",
        "return cost\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vk0RmGNDq7pB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reading Data\n",
        "\n",
        "train_data = np.load(\"./adult/adult_train.npz\")\n",
        "\n",
        "test_data = np.load(\"./adult/adult_test.npz\")\n",
        "\n",
        "\n",
        "X_train = train_data.f.x\n",
        "y_train = train_data.f.y\n",
        "A_train = train_data.f.a\n",
        "\n",
        "X_test = test_data.f.x\n",
        "y_test = test_data.f.y\n",
        "A_test = test_data.f.a"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4JxccDlrbaO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "columns_file = open('./adult/adult_headers.txt','r')\n",
        "columns = columns_file.read()\n",
        "columns_names = columns.split(\"\\n\")\n",
        "\n",
        "X_train_frame = pd.DataFrame(X_train,columns=columns_names) \n",
        "y_train_frame = pd.DataFrame(y_train,columns=['y'])\n",
        "A_train_frame = pd.DataFrame(A_train,columns=['A'])\n",
        "\n",
        "X_test_frame = pd.DataFrame(X_test,columns=columns_names) \n",
        "y_test_frame = pd.DataFrame(y_test,columns=['y']) \n",
        "A_test_frame = pd.DataFrame(A_test,columns=['A'])\n",
        "\n",
        "dataset_train_frame = pd.concat([X_train_frame,y_train_frame,A_train_frame],axis=1)\n",
        "dataset_train_frame.fillna(0,inplace = True)\n",
        "\n",
        "dataset_test_frame =  pd.concat([X_test_frame,y_test_frame,A_test_frame],axis=1)\n",
        "dataset_test_frame.fillna(0,inplace = True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jz6GqsAT0jyz",
        "colab_type": "text"
      },
      "source": [
        "### 1- Normalize the Data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5DUyDPps6jc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "outputId": "e4467bf0-18a6-4bd1-ccf8-f67ff1e91f44"
      },
      "source": [
        "X_train_a0, y_train_a0, X_train_a1, y_train_a1 = filter_dataframe(dataset_train_frame,'A') \n",
        "X_test_a0, y_test_a0, X_test_a1, y_test_a1  = filter_dataframe(dataset_test_frame,'A')\n",
        "\n",
        "\n",
        "X_train_a0_norm = ( X_train_a0 - X_train_a0.mean() ) / X_train_a0.std()\n",
        "X_train_a1_norm = ( X_train_a1 - X_train_a1.mean() ) / X_train_a1.std()\n",
        "\n",
        "X_test_a0_norm = ( X_test_a0 - X_train_a0.mean() ) / X_train_a0.std()\n",
        "X_test_a1_norm = ( X_test_a1 - X_train_a1.mean() ) / X_train_a1.std()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_train_frame = pd.concat([X_train_a0_norm,X_train_a1_norm], axis=0)\n",
        "X_test_frame =  pd.concat([X_test_a0_norm,X_test_a1_norm], axis=0)\n",
        "\n",
        "y_train_frame = pd.concat([y_train_a0,y_train_a1], axis=0)\n",
        "y_test_frame =  pd.concat([y_test_a0,y_test_a1], axis=0)\n",
        "\n",
        "\n",
        "# Shuffling the training data\n",
        "\n",
        "gen_nums = tf.cast(tf.linspace(0.0,X_train_frame.shape[0]-1,X_train_frame.shape[0]-1), dtype=tf.int32)\n",
        "indexes = tf.random.shuffle(gen_nums)\n",
        "\n",
        "\n",
        "X_train = X_train_frame.get_values()[indexes,:]    # numpy array\n",
        "y_train = y_train_frame.get_values()[indexes]\n",
        "A_train = A_train_frame.get_values()[indexes]\n",
        "\n",
        "\n",
        "# Shuffling the testing data\n",
        "\n",
        "gen_nums = tf.cast(tf.linspace(0.0,X_test.shape[0]-1,X_test.shape[0]-1), dtype=tf.int32)\n",
        "indexes = tf.random.shuffle(gen_nums)\n",
        "\n",
        "\n",
        "X_test = X_test_frame.get_values()[indexes,:]   # numpy array\n",
        "y_test = y_test_frame.get_values()[indexes]\n",
        "A_test = A_test_frame.get_values()[indexes]\n",
        "\n",
        "###############################################################################\n",
        "m,n = X_train.shape\n",
        "\n",
        "m2, n2 = X_test.shape\n",
        "\n",
        "\n",
        "\n",
        "# Convert to Tensors\n",
        "\n",
        "X_train = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "X_test = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
        "\n",
        "y_train = tf.convert_to_tensor(y_train, dtype= tf.float32)\n",
        "y_train = tf.reshape(y_train,(m,1))\n",
        "y_test = tf.convert_to_tensor(y_test, dtype = tf.float32)\n",
        "y_test = tf.reshape(y_test,(m2,1))\n",
        "\n",
        "\n",
        "\n",
        "A_train = tf.convert_to_tensor(A_train, dtype = tf.float32)\n",
        "A_train = tf.reshape(A_train,(m,1))\n",
        "\n",
        "A_test = tf.convert_to_tensor(A_test, dtype = tf.float32)\n",
        "A_test = tf.reshape(A_test,(m2,1))\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-fe901e308464>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train_a0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_a0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_a1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_a1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_train_frame\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'A'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_test_a0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_a0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_a1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_a1\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mfilter_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_test_frame\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'A'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_train_a0_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0mX_train_a0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mX_train_a0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mX_train_a0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: filter_dataframe() missing 1 required positional argument: 'label'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyjvqzw2GklZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Binary(logits):\n",
        "  \n",
        "  y_soft = tf.sigmoid(logits)\n",
        "  y = tf.cast((y_soft>=0.5),dtype= tf.float32)\n",
        "  \n",
        "  return y\n",
        "\n",
        "def accuracy(y,y_):\n",
        "    y_ = tf.nn.sigmoid(y_)\n",
        "    ans = tf.cast((y_>=0.5),dtype= tf.float32)\n",
        "    res = tf.cast(tf.equal(ans,y),tf.float32)\n",
        "    \n",
        "    return tf.reduce_mean(res)\n",
        "  \n",
        "\n",
        "def RW_accuracy(y1,y1_, y2, y2_):\n",
        "    rw = tf.multiply(tf.constant(0.5),(accuracy(y1,y1_) + accuracy(y2,y2_) ))\n",
        "    return rw\n",
        "\n",
        "\n",
        "def delta_dp(model, dataset,key,label):\n",
        "  \n",
        "  DP_male_data_frame,DP_male_label_frame,A_male_frame, DP_female_data_frame, DP_female_label_frame,A_female_frame = filter_dataframe_dp(dataset,key,label)\n",
        "  \n",
        "  \n",
        "  \n",
        "  DP_male_data = tf.convert_to_tensor(DP_male_data_frame.to_numpy() ,dtype=tf.float32) \n",
        "  DP_female_data = tf.convert_to_tensor(DP_female_data_frame.to_numpy() ,dtype=tf.float32) \n",
        "  \n",
        "  DP_male_label = tf.convert_to_tensor(DP_male_label_frame.to_numpy() ,dtype=tf.float32) \n",
        "  DP_female_label = tf.convert_to_tensor(DP_female_label_frame.to_numpy() ,dtype=tf.float32) \n",
        "  \n",
        "  A_DP_male = tf.convert_to_tensor(A_male_frame.to_numpy(), dtype= tf.float32)\n",
        "  A_DP_female = tf.convert_to_tensor(A_female_frame.to_numpy(), dtype= tf.float32)\n",
        "\n",
        "  DP_male_y_ = model(DP_male_data)\n",
        "  DP_female_y_ = model(DP_female_data)\n",
        "\n",
        "  \n",
        "  \n",
        "  DP_female = tf.reduce_mean(tf.multiply(DP_female_y_ , tf.subtract(tf.constant(1.0),A_DP_female)))\n",
        "  DP_male = tf.reduce_mean(tf.multiply(DP_male_y_ , A_DP_male))\n",
        "  \n",
        "  delta_DP = tf.abs(DP_female - DP_male )\n",
        " \n",
        "\n",
        "  print(\"Female Accuracy: {}\".format(accuracy(DP_female_label,DP_female_y_) ))\n",
        "  print(\"Male Accuracy: {}\".format(accuracy(DP_male_label,DP_male_y_) ))\n",
        "  \n",
        "  return delta_DP.numpy()\n",
        "\n",
        "\n",
        "\n",
        "def RW_accuracy_v2(model, dataset,key,label):\n",
        "  \n",
        "    male_data_frame,male_label_frame, female_data_frame, female_label_frame = filter_dataframe(dataset,key,label)\n",
        "   \n",
        "    male_data = tf.convert_to_tensor(male_data_frame.to_numpy() ,dtype=tf.float32) \n",
        "    female_data = tf.convert_to_tensor(female_data_frame.to_numpy() ,dtype=tf.float32) \n",
        "    \n",
        "    male_label = tf.convert_to_tensor(male_label_frame.to_numpy() ,dtype=tf.float32) \n",
        "    female_label = tf.convert_to_tensor(female_label_frame.to_numpy() ,dtype=tf.float32) \n",
        "    \n",
        "    male_y_ = model(male_data)\n",
        "    female_y_ = model(female_data)\n",
        "   \n",
        "\n",
        "    rw = RW_accuracy(female_label,female_y_,male_label,male_y_)    \n",
        "    \n",
        "    return rw"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2x4I874HXjw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MLP :\n",
        "  \n",
        "  def __init__(self,input_dim):\n",
        "    \n",
        "    self.model = tf.keras.Sequential(\n",
        "        \n",
        "    [tf.keras.layers.Dense(input_shape=(input_dim,),units=200,activation=tf.nn.relu),\n",
        "    tf.keras.layers.Dense(units = 500,activation=tf.nn.relu),\n",
        "    tf.keras.layers.Dense(units=80,activation=tf.nn.relu),\n",
        "    tf.keras.layers.Dense(units=50,activation=tf.nn.relu),\n",
        "    tf.keras.layers.Dense(units=1)]\n",
        "    \n",
        "    )\n",
        "    \n",
        "    self.optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
        "    self.global_step = tf.Variable(0)\n",
        "    \n",
        "  \n",
        "  def loss(self, y,y_):\n",
        "    print(y.shape, y_.shape)\n",
        "    mloss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y,logits=y_))\n",
        "    return mloss\n",
        "  \n",
        "  def gradient(self, x,y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        y_ = self.model(x)\n",
        "        mloss = self.loss(y,y_)\n",
        "        return mloss, tape.gradient(mloss,self.model.trainable_variables)\n",
        "      \n",
        "      \n",
        "    \n",
        "  def train(self, X, y, epochs, accuracy):\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        print(\"Epoch: \", ep)\n",
        "        y_ = self.model(X)\n",
        "        mloss, grad = self.gradient(X,y)\n",
        "        self.optimizer.apply_gradients(zip(grad, self.model.trainable_variables),global_step=self.global_step)\n",
        "\n",
        "        print(\"Loss: \",mloss.numpy(), \"######### \",\" Accuracy: \", accuracy(y,y_).numpy())\n",
        "\n",
        "    return self.model    \n",
        " \n",
        "\n",
        "  def predict(self):\n",
        "    return self.model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHpnT8APJOIK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifier = MLP(n)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsWNVk83JcfV",
        "colab_type": "code",
        "outputId": "6c65e4a1-bf70-4ed0-b500-c2731d8fdec1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 21975
        }
      },
      "source": [
        "epochs = 1000\n",
        "\n",
        "classifier.train(X_train,y_train, epochs, accuracy)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  0\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  1\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  2\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  3\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  4\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  5\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  6\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  7\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  8\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  9\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  10\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  11\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  12\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  13\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  14\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  15\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  16\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  17\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  18\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  19\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  20\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  21\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  22\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  23\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  24\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  25\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  26\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  27\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  28\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  29\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  30\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  31\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  32\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  33\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  34\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  35\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  36\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  37\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  38\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  39\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  40\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  41\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  42\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  43\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  44\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  45\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  46\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  47\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  48\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  49\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  50\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  51\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  52\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  53\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  54\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  55\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  56\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  57\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  58\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  59\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  60\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  61\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  62\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  63\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  64\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  65\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  66\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  67\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  68\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  69\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  70\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  71\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  72\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  73\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  74\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  75\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  76\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  77\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  78\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  79\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  80\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  81\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  82\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  83\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  84\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  85\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  86\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  87\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  88\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  89\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  90\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  91\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  92\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  93\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  94\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  95\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  96\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  97\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  98\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  99\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  100\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  101\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  102\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  103\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  104\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  105\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  106\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  107\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  108\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  109\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  110\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  111\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  112\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  113\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  114\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  115\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  116\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  117\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  118\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  119\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  120\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  121\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  122\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  123\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  124\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  125\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  126\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  127\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  128\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  129\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  130\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  131\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  132\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  133\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  134\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  135\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  136\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  137\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  138\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  139\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  140\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  141\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  142\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  143\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  144\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  145\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  146\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  147\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  148\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  149\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  150\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  151\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  152\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  153\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  154\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  155\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  156\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  157\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  158\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  159\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  160\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  161\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  162\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  163\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  164\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  165\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  166\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  167\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  168\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  169\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  170\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  171\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  172\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  173\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  174\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  175\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  176\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  177\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  178\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  179\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  180\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  181\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  182\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  183\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  184\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  185\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  186\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  187\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  188\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  189\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  190\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  191\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  192\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  193\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  194\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  195\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  196\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  197\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  198\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  199\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  200\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  201\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  202\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  203\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  204\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  205\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  206\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  207\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  208\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  209\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  210\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  211\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  212\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  213\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  214\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  215\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  216\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  217\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  218\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  219\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  220\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  221\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  222\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  223\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  224\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  225\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  226\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  227\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  228\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  229\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  230\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  231\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  232\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  233\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  234\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  235\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  236\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  237\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  238\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  239\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  240\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  241\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  242\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  243\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  244\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  245\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  246\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  247\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  248\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  249\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  250\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  251\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  252\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  253\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  254\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  255\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  256\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  257\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  258\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  259\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  260\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  261\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  262\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  263\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  264\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  265\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  266\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  267\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  268\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  269\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  270\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  271\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  272\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  273\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  274\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  275\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  276\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  277\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  278\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  279\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  280\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  281\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  282\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  283\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  284\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  285\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  286\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  287\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  288\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  289\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  290\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  291\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  292\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  293\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  294\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  295\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  296\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  297\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  298\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  299\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  300\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  301\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  302\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  303\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  304\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  305\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  306\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  307\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  308\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  309\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  310\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  311\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  312\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  313\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  314\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  315\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  316\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  317\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  318\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  319\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  320\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  321\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  322\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  323\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  324\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  325\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  326\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  327\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  328\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  329\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  330\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  331\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  332\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  333\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  334\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  335\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  336\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  337\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  338\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  339\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  340\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  341\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  342\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  343\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  344\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  345\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  346\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  347\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  348\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  349\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  350\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  351\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  352\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  353\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  354\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  355\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  356\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  357\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  358\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  359\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  360\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  361\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  362\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  363\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  364\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  365\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  366\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  367\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  368\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  369\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  370\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  371\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  372\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  373\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  374\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  375\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  376\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  377\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  378\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  379\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  380\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  381\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  382\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  383\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  384\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  385\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  386\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  387\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  388\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  389\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  390\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  391\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  392\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.5520198 #########   Accuracy:  0.75918305\n",
            "Epoch:  393\n",
            "(32560, 1) (32560, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-58b00c919657>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-5680f70cb7d8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, y, epochs, accuracy)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0my_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mmloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loss: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"######### \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" Accuracy: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, global_step, name)\u001b[0m\n\u001b[1;32m    610\u001b[0m           \u001b[0mscope_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"update_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mscope_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m           \u001b[0mupdate_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m         \u001b[0mapply_updates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_finish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mupdate_op\u001b[0;34m(self, optimizer, g)\u001b[0m\n\u001b[1;32m    169\u001b[0m       return optimizer._resource_apply_sparse_duplicate_indices(\n\u001b[1;32m    170\u001b[0m           g.values, self._v, g.indices)\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0mupdate_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resource_apply_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstraint\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mupdate_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/adam.py\u001b[0m in \u001b[0;36m_resource_apply_dense\u001b[0;34m(self, grad, var)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_beta2_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epsilon_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         grad, use_locking=self._use_locking)\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_apply_sparse_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscatter_add\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/gen_training_ops.py\u001b[0m in \u001b[0;36mresource_apply_adam\u001b[0;34m(var, m, v, beta1_power, beta2_power, lr, beta1, beta2, epsilon, grad, use_locking, use_nesterov, name)\u001b[0m\n\u001b[1;32m   1272\u001b[0m         \u001b[0;34m\"ResourceApplyAdam\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1273\u001b[0m         \u001b[0mbeta1_power\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta2_power\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1274\u001b[0;31m         \"use_locking\", use_locking, \"use_nesterov\", use_nesterov)\n\u001b[0m\u001b[1;32m   1275\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLUIOZiEJiUr",
        "colab_type": "code",
        "outputId": "92d440b7-7dda-4b98-eacd-df443c514d9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "print(\"Total test Accuracy: \", accuracy(y_test,classifier.predict()(X_test)).numpy() )\n",
        "\n",
        "print(\"Delta DP: \", delta_dp(classifier.predict(), dataset_test_frame,'sex_Female','y' )) \n",
        "\n",
        "print('Reweighted Accuracy: {}'.format(RW_accuracy_v2(classifier.predict(), dataset_test_frame,'A','y')))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total test Accuracy:  0.7637592\n",
            "Female Accuracy: 0.8911640048027039\n",
            "Male Accuracy: 0.700184166431427\n",
            "Delta DP:  8.106232e-06\n",
            "Reweighted Accuracy: 0.7956740856170654\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6fyn2nSkgro",
        "colab_type": "text"
      },
      "source": [
        "### For A:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cc4-5anxWMkQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8cb51e2f-4003-4cdb-8492-c1d1da1db57f"
      },
      "source": [
        "X_train_uncr2 = X_train_frame.drop(['sex_Male', 'sex_Female'], axis = 1).to_numpy()\n",
        "X_test_uncr2 = X_test_frame.drop([],axis = 1).to_numpy()\n",
        "\n",
        "X_test_uncr2 = X_test_frame.drop(['sex_Male', 'sex_Female'], axis = 1).to_numpy()\n",
        "\n",
        "dataset_test_frame_uncr2 = dataset_test_frame.drop(['sex_Male', 'sex_Female'], axis = 1) \n",
        "\n",
        "X_train_uncr2 = tf.convert_to_tensor(X_train_uncr2, dtype=tf.float32)\n",
        "X_test_uncr2 = tf.convert_to_tensor(X_test_uncr2, dtype= tf.float32)\n",
        "\n",
        "\n",
        "n3= X_train_uncr2.shape[1]\n",
        "\n",
        "print(n3)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "111\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xflhgvkmky82",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4e5ba69d-6f9a-43c0-8d56-b2f7f4983136"
      },
      "source": [
        "classifier3 = MLP(n3)\n",
        "\n",
        "print(y_train.shape)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32560, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-uktlmekyqL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54885
        },
        "outputId": "361f3cab-73e1-4e5a-e9c4-a811ae42ab9c"
      },
      "source": [
        "epochs = 1000\n",
        "classifier3.train(X_train_uncr2[1:],A_train,epochs,accuracy)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  0\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6931471 #########   Accuracy:  0.66919535\n",
            "Epoch:  1\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.69146776 #########   Accuracy:  0.66919535\n",
            "Epoch:  2\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.689814 #########   Accuracy:  0.66919535\n",
            "Epoch:  3\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.68818635 #########   Accuracy:  0.66919535\n",
            "Epoch:  4\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6865851 #########   Accuracy:  0.66919535\n",
            "Epoch:  5\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.68501085 #########   Accuracy:  0.66919535\n",
            "Epoch:  6\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.68346393 #########   Accuracy:  0.66919535\n",
            "Epoch:  7\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6819445 #########   Accuracy:  0.66919535\n",
            "Epoch:  8\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.680453 #########   Accuracy:  0.66919535\n",
            "Epoch:  9\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.67898965 #########   Accuracy:  0.66919535\n",
            "Epoch:  10\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.67755467 #########   Accuracy:  0.66919535\n",
            "Epoch:  11\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.67614836 #########   Accuracy:  0.66919535\n",
            "Epoch:  12\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6747709 #########   Accuracy:  0.66919535\n",
            "Epoch:  13\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6734224 #########   Accuracy:  0.66919535\n",
            "Epoch:  14\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6721028 #########   Accuracy:  0.66919535\n",
            "Epoch:  15\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.67081255 #########   Accuracy:  0.66919535\n",
            "Epoch:  16\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.66955155 #########   Accuracy:  0.66919535\n",
            "Epoch:  17\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.66831964 #########   Accuracy:  0.66919535\n",
            "Epoch:  18\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.66711706 #########   Accuracy:  0.66919535\n",
            "Epoch:  19\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6659437 #########   Accuracy:  0.66919535\n",
            "Epoch:  20\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.66479945 #########   Accuracy:  0.66919535\n",
            "Epoch:  21\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.66368425 #########   Accuracy:  0.66919535\n",
            "Epoch:  22\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.662598 #########   Accuracy:  0.66919535\n",
            "Epoch:  23\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.66154045 #########   Accuracy:  0.66919535\n",
            "Epoch:  24\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6605115 #########   Accuracy:  0.66919535\n",
            "Epoch:  25\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.65951085 #########   Accuracy:  0.66919535\n",
            "Epoch:  26\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.65853846 #########   Accuracy:  0.66919535\n",
            "Epoch:  27\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6575937 #########   Accuracy:  0.66919535\n",
            "Epoch:  28\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6566766 #########   Accuracy:  0.66919535\n",
            "Epoch:  29\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6557867 #########   Accuracy:  0.66919535\n",
            "Epoch:  30\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6549237 #########   Accuracy:  0.66919535\n",
            "Epoch:  31\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6540873 #########   Accuracy:  0.66919535\n",
            "Epoch:  32\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.653277 #########   Accuracy:  0.66919535\n",
            "Epoch:  33\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6524925 #########   Accuracy:  0.66919535\n",
            "Epoch:  34\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.65173346 #########   Accuracy:  0.66919535\n",
            "Epoch:  35\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6509993 #########   Accuracy:  0.66919535\n",
            "Epoch:  36\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.65028965 #########   Accuracy:  0.66919535\n",
            "Epoch:  37\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6496041 #########   Accuracy:  0.66919535\n",
            "Epoch:  38\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6489423 #########   Accuracy:  0.66919535\n",
            "Epoch:  39\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.64830345 #########   Accuracy:  0.66919535\n",
            "Epoch:  40\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.64768744 #########   Accuracy:  0.66919535\n",
            "Epoch:  41\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.64709353 #########   Accuracy:  0.66919535\n",
            "Epoch:  42\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6465213 #########   Accuracy:  0.66919535\n",
            "Epoch:  43\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6459703 #########   Accuracy:  0.66919535\n",
            "Epoch:  44\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6454399 #########   Accuracy:  0.66919535\n",
            "Epoch:  45\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6449298 #########   Accuracy:  0.66919535\n",
            "Epoch:  46\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.64443946 #########   Accuracy:  0.66919535\n",
            "Epoch:  47\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6439681 #########   Accuracy:  0.66919535\n",
            "Epoch:  48\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6435156 #########   Accuracy:  0.66919535\n",
            "Epoch:  49\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6430811 #########   Accuracy:  0.66919535\n",
            "Epoch:  50\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6426644 #########   Accuracy:  0.66919535\n",
            "Epoch:  51\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.64226484 #########   Accuracy:  0.66919535\n",
            "Epoch:  52\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6418817 #########   Accuracy:  0.66919535\n",
            "Epoch:  53\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.641515 #########   Accuracy:  0.66919535\n",
            "Epoch:  54\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6411639 #########   Accuracy:  0.66919535\n",
            "Epoch:  55\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.640828 #########   Accuracy:  0.66919535\n",
            "Epoch:  56\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6405067 #########   Accuracy:  0.66919535\n",
            "Epoch:  57\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.64019966 #########   Accuracy:  0.66919535\n",
            "Epoch:  58\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.63990635 #########   Accuracy:  0.66919535\n",
            "Epoch:  59\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6396264 #########   Accuracy:  0.66919535\n",
            "Epoch:  60\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6393592 #########   Accuracy:  0.66919535\n",
            "Epoch:  61\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6391044 #########   Accuracy:  0.66919535\n",
            "Epoch:  62\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6388616 #########   Accuracy:  0.66919535\n",
            "Epoch:  63\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.63863015 #########   Accuracy:  0.66919535\n",
            "Epoch:  64\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6384099 #########   Accuracy:  0.66919535\n",
            "Epoch:  65\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.63820034 #########   Accuracy:  0.66919535\n",
            "Epoch:  66\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.638001 #########   Accuracy:  0.66919535\n",
            "Epoch:  67\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.63781154 #########   Accuracy:  0.66919535\n",
            "Epoch:  68\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6376315 #########   Accuracy:  0.66919535\n",
            "Epoch:  69\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6374606 #########   Accuracy:  0.66919535\n",
            "Epoch:  70\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.63729846 #########   Accuracy:  0.66919535\n",
            "Epoch:  71\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.63714457 #########   Accuracy:  0.66919535\n",
            "Epoch:  72\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.63699883 #########   Accuracy:  0.66919535\n",
            "Epoch:  73\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6368607 #########   Accuracy:  0.66919535\n",
            "Epoch:  74\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.63673 #########   Accuracy:  0.66919535\n",
            "Epoch:  75\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.63660634 #########   Accuracy:  0.66919535\n",
            "Epoch:  76\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6364893 #########   Accuracy:  0.66919535\n",
            "Epoch:  77\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.63637877 #########   Accuracy:  0.66919535\n",
            "Epoch:  78\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6362743 #########   Accuracy:  0.66919535\n",
            "Epoch:  79\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6361757 #########   Accuracy:  0.66919535\n",
            "Epoch:  80\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.63608277 #########   Accuracy:  0.66919535\n",
            "Epoch:  81\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.635995 #########   Accuracy:  0.66919535\n",
            "Epoch:  82\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.63591236 #########   Accuracy:  0.66919535\n",
            "Epoch:  83\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6358346 #########   Accuracy:  0.66919535\n",
            "Epoch:  84\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6357613 #########   Accuracy:  0.66919535\n",
            "Epoch:  85\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6356924 #########   Accuracy:  0.66919535\n",
            "Epoch:  86\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6356277 #########   Accuracy:  0.66919535\n",
            "Epoch:  87\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6355668 #########   Accuracy:  0.66919535\n",
            "Epoch:  88\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.63550967 #########   Accuracy:  0.66919535\n",
            "Epoch:  89\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6354561 #########   Accuracy:  0.66919535\n",
            "Epoch:  90\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6354058 #########   Accuracy:  0.66919535\n",
            "Epoch:  91\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.63535875 #########   Accuracy:  0.66919535\n",
            "Epoch:  92\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6353146 #########   Accuracy:  0.66919535\n",
            "Epoch:  93\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6352734 #########   Accuracy:  0.66919535\n",
            "Epoch:  94\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6352347 #########   Accuracy:  0.66919535\n",
            "Epoch:  95\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6351987 #########   Accuracy:  0.66919535\n",
            "Epoch:  96\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.63516504 #########   Accuracy:  0.66919535\n",
            "Epoch:  97\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.63513356 #########   Accuracy:  0.66919535\n",
            "Epoch:  98\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6351043 #########   Accuracy:  0.66919535\n",
            "Epoch:  99\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6350769 #########   Accuracy:  0.66919535\n",
            "Epoch:  100\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6350514 #########   Accuracy:  0.66919535\n",
            "Epoch:  101\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.63502777 #########   Accuracy:  0.66919535\n",
            "Epoch:  102\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6350057 #########   Accuracy:  0.66919535\n",
            "Epoch:  103\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.63498515 #########   Accuracy:  0.66919535\n",
            "Epoch:  104\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6349661 #########   Accuracy:  0.66919535\n",
            "Epoch:  105\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6349485 #########   Accuracy:  0.66919535\n",
            "Epoch:  106\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.63493204 #########   Accuracy:  0.66919535\n",
            "Epoch:  107\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.63491684 #########   Accuracy:  0.66919535\n",
            "Epoch:  108\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.63490283 #########   Accuracy:  0.66919535\n",
            "Epoch:  109\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6348898 #########   Accuracy:  0.66919535\n",
            "Epoch:  110\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6348778 #########   Accuracy:  0.66919535\n",
            "Epoch:  111\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.63486665 #########   Accuracy:  0.66919535\n",
            "Epoch:  112\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.63485634 #########   Accuracy:  0.66919535\n",
            "Epoch:  113\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.63484687 #########   Accuracy:  0.66919535\n",
            "Epoch:  114\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6348381 #########   Accuracy:  0.66919535\n",
            "Epoch:  115\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.63483006 #########   Accuracy:  0.66919535\n",
            "Epoch:  116\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6348227 #########   Accuracy:  0.66919535\n",
            "Epoch:  117\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6348159 #########   Accuracy:  0.66919535\n",
            "Epoch:  118\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6348097 #########   Accuracy:  0.66919535\n",
            "Epoch:  119\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6348039 #########   Accuracy:  0.66919535\n",
            "Epoch:  120\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.63479877 #########   Accuracy:  0.66919535\n",
            "Epoch:  121\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6347939 #########   Accuracy:  0.66919535\n",
            "Epoch:  122\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.63478947 #########   Accuracy:  0.66919535\n",
            "Epoch:  123\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6347854 #########   Accuracy:  0.66919535\n",
            "Epoch:  124\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6347817 #########   Accuracy:  0.66919535\n",
            "Epoch:  125\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6347783 #########   Accuracy:  0.66919535\n",
            "Epoch:  126\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6347752 #########   Accuracy:  0.66919535\n",
            "Epoch:  127\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6347725 #########   Accuracy:  0.66919535\n",
            "Epoch:  128\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6347699 #########   Accuracy:  0.66919535\n",
            "Epoch:  129\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.63476765 #########   Accuracy:  0.66919535\n",
            "Epoch:  130\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6347655 #########   Accuracy:  0.66919535\n",
            "Epoch:  131\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.63476354 #########   Accuracy:  0.66919535\n",
            "Epoch:  132\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.63476187 #########   Accuracy:  0.66919535\n",
            "Epoch:  133\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.63476026 #########   Accuracy:  0.66919535\n",
            "Epoch:  134\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6347588 #########   Accuracy:  0.66919535\n",
            "Epoch:  135\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6347576 #########   Accuracy:  0.66919535\n",
            "Epoch:  136\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6347564 #########   Accuracy:  0.66919535\n",
            "Epoch:  137\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.63475543 #########   Accuracy:  0.66919535\n",
            "Epoch:  138\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6347545 #########   Accuracy:  0.66919535\n",
            "Epoch:  139\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.63475364 #########   Accuracy:  0.66919535\n",
            "Epoch:  140\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6347529 #########   Accuracy:  0.66919535\n",
            "Epoch:  141\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6347522 #########   Accuracy:  0.66919535\n",
            "Epoch:  142\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6347516 #########   Accuracy:  0.66919535\n",
            "Epoch:  143\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.63475096 #########   Accuracy:  0.66919535\n",
            "Epoch:  144\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6347505 #########   Accuracy:  0.66919535\n",
            "Epoch:  145\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6347501 #########   Accuracy:  0.66919535\n",
            "Epoch:  146\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.63474977 #########   Accuracy:  0.66919535\n",
            "Epoch:  147\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6347493 #########   Accuracy:  0.66919535\n",
            "Epoch:  148\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.63474905 #########   Accuracy:  0.66919535\n",
            "Epoch:  149\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6347488 #########   Accuracy:  0.66919535\n",
            "Epoch:  150\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6347486 #########   Accuracy:  0.66919535\n",
            "Epoch:  151\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.63474834 #########   Accuracy:  0.66919535\n",
            "Epoch:  152\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6347482 #########   Accuracy:  0.66919535\n",
            "Epoch:  153\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634748 #########   Accuracy:  0.66919535\n",
            "Epoch:  154\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.63474786 #########   Accuracy:  0.66919535\n",
            "Epoch:  155\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.63474774 #########   Accuracy:  0.66919535\n",
            "Epoch:  156\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.63474774 #########   Accuracy:  0.66919535\n",
            "Epoch:  157\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.63474756 #########   Accuracy:  0.66919535\n",
            "Epoch:  158\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6347474 #########   Accuracy:  0.66919535\n",
            "Epoch:  159\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6347474 #########   Accuracy:  0.66919535\n",
            "Epoch:  160\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6347474 #########   Accuracy:  0.66919535\n",
            "Epoch:  161\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.63474727 #########   Accuracy:  0.66919535\n",
            "Epoch:  162\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.63474727 #########   Accuracy:  0.66919535\n",
            "Epoch:  163\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.63474715 #########   Accuracy:  0.66919535\n",
            "Epoch:  164\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.63474715 #########   Accuracy:  0.66919535\n",
            "Epoch:  165\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.63474715 #########   Accuracy:  0.66919535\n",
            "Epoch:  166\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6347471 #########   Accuracy:  0.66919535\n",
            "Epoch:  167\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6347471 #########   Accuracy:  0.66919535\n",
            "Epoch:  168\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.63474715 #########   Accuracy:  0.66919535\n",
            "Epoch:  169\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6347471 #########   Accuracy:  0.66919535\n",
            "Epoch:  170\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6347471 #########   Accuracy:  0.66919535\n",
            "Epoch:  171\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  172\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  173\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.63474715 #########   Accuracy:  0.66919535\n",
            "Epoch:  174\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  175\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  176\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  177\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  178\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  179\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  180\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  181\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  182\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  183\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  184\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.63474697 #########   Accuracy:  0.66919535\n",
            "Epoch:  185\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  186\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  187\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  188\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  189\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  190\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  191\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  192\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  193\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6347471 #########   Accuracy:  0.66919535\n",
            "Epoch:  194\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.63474697 #########   Accuracy:  0.66919535\n",
            "Epoch:  195\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  196\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  197\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  198\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  199\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  200\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  201\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  202\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  203\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.63474697 #########   Accuracy:  0.66919535\n",
            "Epoch:  204\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  205\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  206\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6347471 #########   Accuracy:  0.66919535\n",
            "Epoch:  207\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  208\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  209\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  210\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  211\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  212\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  213\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  214\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  215\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  216\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  217\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  218\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  219\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  220\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  221\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  222\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  223\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6347471 #########   Accuracy:  0.66919535\n",
            "Epoch:  224\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  225\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  226\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  227\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  228\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  229\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  230\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.63474697 #########   Accuracy:  0.66919535\n",
            "Epoch:  231\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  232\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.63474697 #########   Accuracy:  0.66919535\n",
            "Epoch:  233\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  234\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  235\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  236\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.63474697 #########   Accuracy:  0.66919535\n",
            "Epoch:  237\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.63474697 #########   Accuracy:  0.66919535\n",
            "Epoch:  238\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  239\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  240\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  241\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  242\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  243\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.63474697 #########   Accuracy:  0.66919535\n",
            "Epoch:  244\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  245\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  246\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  247\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  248\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  249\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6347471 #########   Accuracy:  0.66919535\n",
            "Epoch:  250\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  251\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  252\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  253\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  254\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  255\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  256\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.63474697 #########   Accuracy:  0.66919535\n",
            "Epoch:  257\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  258\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  259\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.63474697 #########   Accuracy:  0.66919535\n",
            "Epoch:  260\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  261\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  262\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  263\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  264\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  265\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  266\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6347471 #########   Accuracy:  0.66919535\n",
            "Epoch:  267\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  268\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  269\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  270\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  271\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  272\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  273\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  274\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  275\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  276\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  277\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  278\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  279\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  280\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  281\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  282\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  283\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  284\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  285\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  286\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  287\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  288\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  289\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  290\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  291\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  292\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  293\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  294\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6347471 #########   Accuracy:  0.66919535\n",
            "Epoch:  295\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  296\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  297\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  298\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  299\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  300\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  301\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  302\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  303\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.63474697 #########   Accuracy:  0.66919535\n",
            "Epoch:  304\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  305\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  306\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  307\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  308\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  309\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  310\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  311\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  312\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.6347471 #########   Accuracy:  0.66919535\n",
            "Epoch:  313\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  314\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  315\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.63474697 #########   Accuracy:  0.66919535\n",
            "Epoch:  316\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  317\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  318\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  319\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  320\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  321\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  322\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  323\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  324\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  325\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  326\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  327\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  328\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  329\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  330\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  331\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  332\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  333\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  334\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  335\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  336\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  337\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  338\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  339\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  340\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  341\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  342\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  343\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  344\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  345\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  346\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  347\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  348\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  349\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  350\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  351\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  352\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  353\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  354\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  355\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  356\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  357\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  358\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  359\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  360\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  361\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  362\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  363\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  364\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  365\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  366\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  367\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  368\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  369\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  370\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  371\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  372\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  373\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  374\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  375\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  376\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  377\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  378\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  379\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  380\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  381\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  382\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  383\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  384\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  385\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  386\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  387\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  388\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  389\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  390\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  391\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  392\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  393\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  394\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  395\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  396\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  397\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  398\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  399\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  400\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  401\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  402\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  403\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  404\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  405\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  406\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  407\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  408\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  409\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  410\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  411\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  412\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  413\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  414\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  415\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  416\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  417\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  418\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  419\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  420\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  421\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  422\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  423\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  424\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  425\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  426\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  427\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  428\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  429\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  430\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  431\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  432\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  433\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  434\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  435\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  436\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  437\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  438\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  439\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  440\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  441\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  442\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  443\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  444\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  445\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  446\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  447\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  448\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  449\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  450\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  451\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  452\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  453\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  454\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  455\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  456\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  457\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  458\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  459\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  460\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  461\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  462\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  463\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  464\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  465\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  466\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  467\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  468\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  469\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  470\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  471\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  472\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  473\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  474\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  475\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  476\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  477\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  478\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  479\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  480\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  481\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  482\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  483\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  484\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  485\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  486\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  487\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  488\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  489\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  490\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  491\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  492\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  493\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  494\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  495\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  496\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  497\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  498\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  499\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  500\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  501\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  502\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  503\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  504\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  505\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  506\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  507\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  508\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  509\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  510\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  511\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  512\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  513\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  514\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  515\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  516\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  517\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  518\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  519\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  520\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  521\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  522\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  523\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  524\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  525\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  526\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  527\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  528\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  529\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  530\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  531\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  532\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  533\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  534\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  535\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  536\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  537\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  538\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  539\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  540\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  541\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  542\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  543\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  544\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  545\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  546\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  547\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  548\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  549\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  550\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  551\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  552\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  553\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  554\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  555\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  556\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  557\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  558\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  559\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  560\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  561\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  562\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  563\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  564\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  565\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  566\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  567\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  568\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  569\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  570\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  571\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  572\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  573\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  574\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  575\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  576\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  577\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  578\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  579\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  580\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  581\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  582\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  583\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  584\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  585\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  586\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  587\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  588\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  589\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  590\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  591\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  592\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  593\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  594\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  595\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  596\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  597\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  598\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  599\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  600\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  601\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  602\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  603\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  604\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  605\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  606\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  607\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  608\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  609\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  610\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  611\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  612\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  613\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  614\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  615\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  616\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  617\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  618\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  619\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  620\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  621\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  622\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  623\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  624\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  625\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  626\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  627\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  628\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  629\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  630\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  631\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  632\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  633\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  634\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  635\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  636\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  637\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  638\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  639\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  640\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  641\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  642\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  643\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  644\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  645\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  646\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  647\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  648\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  649\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  650\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  651\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  652\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  653\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  654\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  655\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  656\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  657\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  658\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  659\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  660\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  661\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  662\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  663\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  664\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  665\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  666\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  667\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  668\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  669\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  670\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  671\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  672\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  673\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  674\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  675\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  676\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  677\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  678\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  679\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  680\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  681\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  682\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  683\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  684\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  685\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  686\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  687\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  688\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  689\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  690\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  691\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  692\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  693\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  694\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  695\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  696\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  697\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  698\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  699\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  700\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  701\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  702\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  703\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  704\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  705\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  706\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  707\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  708\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  709\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  710\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  711\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  712\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  713\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  714\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  715\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  716\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  717\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  718\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  719\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  720\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  721\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  722\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  723\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  724\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  725\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  726\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  727\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  728\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  729\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  730\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  731\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  732\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  733\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  734\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  735\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  736\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  737\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  738\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  739\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  740\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  741\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  742\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  743\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  744\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  745\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  746\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  747\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  748\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  749\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  750\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  751\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  752\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  753\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  754\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  755\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  756\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  757\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  758\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  759\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  760\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  761\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  762\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  763\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  764\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  765\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  766\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  767\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  768\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  769\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  770\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  771\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  772\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  773\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  774\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  775\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  776\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  777\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  778\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  779\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  780\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  781\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  782\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  783\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  784\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  785\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  786\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  787\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  788\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  789\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  790\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  791\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  792\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  793\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  794\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  795\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  796\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  797\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  798\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  799\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  800\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  801\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  802\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  803\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  804\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  805\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  806\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  807\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  808\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  809\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  810\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  811\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  812\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  813\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  814\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  815\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  816\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  817\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  818\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  819\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  820\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  821\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  822\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  823\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  824\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  825\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  826\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  827\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  828\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  829\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  830\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  831\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  832\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  833\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  834\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  835\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  836\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  837\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  838\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  839\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  840\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  841\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  842\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  843\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  844\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  845\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  846\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  847\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  848\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  849\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  850\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  851\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  852\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  853\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  854\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  855\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  856\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  857\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  858\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  859\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  860\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  861\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  862\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  863\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  864\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  865\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  866\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  867\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  868\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  869\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  870\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  871\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  872\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  873\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  874\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  875\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  876\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  877\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  878\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  879\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  880\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  881\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  882\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  883\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  884\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  885\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  886\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  887\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  888\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  889\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  890\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  891\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  892\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  893\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  894\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  895\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  896\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  897\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  898\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  899\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  900\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  901\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  902\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  903\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  904\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  905\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  906\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  907\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  908\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  909\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  910\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  911\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  912\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  913\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  914\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  915\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  916\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  917\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  918\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  919\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  920\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  921\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  922\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  923\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  924\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  925\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  926\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  927\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  928\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  929\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  930\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  931\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  932\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  933\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  934\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  935\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  936\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  937\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  938\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  939\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  940\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  941\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  942\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  943\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  944\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  945\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  946\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  947\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  948\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  949\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  950\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  951\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  952\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  953\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  954\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  955\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  956\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  957\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  958\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  959\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  960\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  961\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  962\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  963\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  964\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  965\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  966\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  967\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  968\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  969\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  970\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  971\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  972\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  973\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  974\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  975\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  976\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  977\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  978\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  979\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  980\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  981\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  982\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  983\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  984\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  985\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  986\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  987\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  988\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  989\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  990\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  991\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  992\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  993\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  994\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  995\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  996\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  997\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  998\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n",
            "Epoch:  999\n",
            "(32560, 1) (32560, 1)\n",
            "Loss:  0.634747 #########   Accuracy:  0.66919535\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.engine.sequential.Sequential at 0x7f748011d748>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CctYiHszk5I3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "525b3263-2db8-4642-e5ca-1c334f4157fb"
      },
      "source": [
        "print(\"Test Accuracy: \", accuracy(A_test,classifier3.predict()(X_test_uncr2[1:])).numpy() )\n",
        "\n",
        "print(\"Delta DP: \", delta_dp(classifier3.predict(), dataset_test_frame_uncr2,'A','A' )) \n",
        "\n",
        "print('Reweighted Accuracy: {}'.format(RW_accuracy_v2(classifier3.predict(), dataset_test_frame_uncr2,'A','A')))"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy:  0.6670762\n",
            "Female Accuracy: 1.0\n",
            "Male Accuracy: 0.0\n",
            "Delta DP:  0.0\n",
            "Reweighted Accuracy: 0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gA-Q80BRLTJS",
        "colab_type": "text"
      },
      "source": [
        "## Q2. Now, let’s try a neural network solution. Train a neural network with at least one hidden layer as your binary classifier. Use a cross-entropy loss, and include an MMD regularizer on the final hidden layer of the network. You can find Pytorch code for MMD in mmd.py in the assignment folder. The regularizer should measure the MMD between the internal repre- sentation of the network for each group (A = 0 or 1), multiplied by a coefficient hyperparameter α. Start with α = 0.1. Note, this is not really pre-processing, since we’re learning the solution end-to-end. As in the previous question, learn to predict Y and A with this model, and report the same results. Which method was better?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1WFMRBbtMPJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.nn."
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}