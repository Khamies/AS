{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import eager\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"./adult/adult_train.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32561, 113)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data.f.x\n",
    "y = data.f.y\n",
    "A = data.f.a\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_file = open('./adult/adult_headers.txt','r')\n",
    "columns = columns_file.read()\n",
    "columns_names = columns.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(X,columns=columns_names) \n",
    "y = pd.DataFrame(y,columns=['income'])\n",
    "A = pd.DataFrame(A,columns=['A'])\n",
    "dataset = pd.concat([X,y,A],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age_u20</th>\n",
       "      <th>age_u30</th>\n",
       "      <th>age_u40</th>\n",
       "      <th>age_u50</th>\n",
       "      <th>age_u60</th>\n",
       "      <th>age_u70</th>\n",
       "      <th>age_u80</th>\n",
       "      <th>workclass_Private</th>\n",
       "      <th>workclass_Self-emp-not-inc</th>\n",
       "      <th>workclass_Self-emp-inc</th>\n",
       "      <th>...</th>\n",
       "      <th>country_Thailand</th>\n",
       "      <th>country_Yugoslavia</th>\n",
       "      <th>country_El-Salvador</th>\n",
       "      <th>country_Trinadad&amp;Tobago</th>\n",
       "      <th>country_Peru</th>\n",
       "      <th>country_Hong</th>\n",
       "      <th>country_Holand-Netherlands</th>\n",
       "      <th>country_?</th>\n",
       "      <th>income</th>\n",
       "      <th>A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 115 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age_u20  age_u30  age_u40  age_u50  age_u60  age_u70  age_u80  \\\n",
       "0      1.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1      0.0      1.0      0.0      0.0      0.0      0.0      0.0   \n",
       "2      0.0      0.0      0.0      0.0      1.0      0.0      0.0   \n",
       "\n",
       "   workclass_Private  workclass_Self-emp-not-inc  workclass_Self-emp-inc  ...  \\\n",
       "0                1.0                         0.0                     0.0  ...   \n",
       "1                1.0                         0.0                     0.0  ...   \n",
       "2                0.0                         0.0                     0.0  ...   \n",
       "\n",
       "   country_Thailand  country_Yugoslavia  country_El-Salvador  \\\n",
       "0               0.0                 0.0                  0.0   \n",
       "1               0.0                 0.0                  0.0   \n",
       "2               0.0                 0.0                  0.0   \n",
       "\n",
       "   country_Trinadad&Tobago  country_Peru  country_Hong  \\\n",
       "0                      0.0           0.0           0.0   \n",
       "1                      0.0           0.0           0.0   \n",
       "2                      0.0           0.0           0.0   \n",
       "\n",
       "   country_Holand-Netherlands  country_?  income    A  \n",
       "0                         0.0        0.0     0.0  0.0  \n",
       "1                         0.0        0.0     0.0  0.0  \n",
       "2                         0.0        0.0     1.0  1.0  \n",
       "\n",
       "[3 rows x 115 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- Name the 10 features which are most correlated with Y, and the 10 which are most correlated with A, as measured by (absolute) Pearson correlation (ignore any NaN correlations you see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_corr_income = dataset.corr()['income']\n",
    "data_corr_A = dataset.corr()['A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer1 = data_corr_income.sort_values(ascending=False)[1:11]\n",
    "answer2= data_corr_A.sort_values(ascending=False)[1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "marital-status_Married-civ-spouse    0.444696\n",
      "relationship_Husband                 0.401035\n",
      "education_num                        0.335154\n",
      "hours-per-week                       0.229689\n",
      "capital-gain                         0.223329\n",
      "sex_Male                             0.215980\n",
      "A                                    0.215980\n",
      "occupation_Exec-managerial           0.214861\n",
      "occupation_Prof-specialty            0.185866\n",
      "education_Bachelors                  0.180485\n",
      "Name: income, dtype: float64\n",
      "#####################################################\n",
      "sex_Male                             1.000000\n",
      "relationship_Husband                 0.580135\n",
      "marital-status_Married-civ-spouse    0.431805\n",
      "hours-per-week                       0.229309\n",
      "occupation_Craft-repair              0.223128\n",
      "income                               0.215980\n",
      "occupation_Transport-moving          0.132468\n",
      "workclass_Self-emp-not-inc           0.107451\n",
      "race_White                           0.103486\n",
      "occupation_Farming-fishing           0.100097\n",
      "Name: A, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(answer1)\n",
    "print(\"#####################################################\")\n",
    "print(answer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tf.convert_to_tensor(X.to_numpy(),dtype=tf.float32)\n",
    "y_train = tf.convert_to_tensor(y.to_numpy(),dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "m,n = X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = tf.keras.Sequential(\n",
    "    [tf.keras.layers.Dense(input_shape=(n,),units=200,activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(units = 300,activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(units=50,activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(units=20,activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(units=1)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y,y_):\n",
    "    mloss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y,logits=y_))\n",
    "    return mloss\n",
    "\n",
    "\n",
    "def gradient(model,x,y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_ = model(x)\n",
    "        mloss = loss(y,y_)\n",
    "        return mloss, tape.gradient(mloss,model.trainable_variables)\n",
    "    \n",
    "def accuracy(y,y_):\n",
    "    y_ = tf.nn.sigmoid(y_)\n",
    "    ans = tf.cast((y_>=0.5),dtype= tf.float32)\n",
    "    res = tf.cast(tf.equal(ans,y),tf.float32)\n",
    "    \n",
    "    return tf.reduce_mean(res)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "global_step = tf.Variable(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "Loss:  3.8204904 #########   Accuracy:  0.7756519\n",
      "Epoch:  1\n",
      "Loss:  300.10458 #########   Accuracy:  0.75919044\n",
      "Epoch:  2\n",
      "Loss:  76.60125 #########   Accuracy:  0.76001966\n",
      "Epoch:  3\n",
      "Loss:  14.3391905 #########   Accuracy:  0.76001966\n",
      "Epoch:  4\n",
      "Loss:  1.7395462 #########   Accuracy:  0.7789073\n",
      "Epoch:  5\n",
      "Loss:  1.838211 #########   Accuracy:  0.7789073\n",
      "Epoch:  6\n",
      "Loss:  2.2259562 #########   Accuracy:  0.7797365\n",
      "Epoch:  7\n",
      "Loss:  2.3061419 #########   Accuracy:  0.7797365\n",
      "Epoch:  8\n",
      "Loss:  0.8978553 #########   Accuracy:  0.7810571\n",
      "Epoch:  9\n",
      "Loss:  38.237816 #########   Accuracy:  0.75919044\n",
      "Epoch:  10\n",
      "Loss:  2.666266 #########   Accuracy:  0.7789073\n",
      "Epoch:  11\n",
      "Loss:  3.8449075 #########   Accuracy:  0.7789073\n",
      "Epoch:  12\n",
      "Loss:  4.7512865 #########   Accuracy:  0.7797365\n",
      "Epoch:  13\n",
      "Loss:  5.5746374 #########   Accuracy:  0.7797365\n",
      "Epoch:  14\n",
      "Loss:  5.5526175 #########   Accuracy:  0.7805043\n",
      "Epoch:  15\n",
      "Loss:  5.877717 #########   Accuracy:  0.7789073\n",
      "Epoch:  16\n",
      "Loss:  5.251256 #########   Accuracy:  0.78308403\n",
      "Epoch:  17\n",
      "Loss:  4.913125 #########   Accuracy:  0.7797365\n",
      "Epoch:  18\n",
      "Loss:  3.9340122 #########   Accuracy:  0.7797365\n",
      "Epoch:  19\n",
      "Loss:  3.5307424 #########   Accuracy:  0.7789073\n",
      "Epoch:  20\n",
      "Loss:  2.403228 #########   Accuracy:  0.7789073\n",
      "Epoch:  21\n",
      "Loss:  1.1939007 #########   Accuracy:  0.7797365\n",
      "Epoch:  22\n",
      "Loss:  0.93492776 #########   Accuracy:  0.7797365\n",
      "Epoch:  23\n",
      "Loss:  1.0381594 #########   Accuracy:  0.75919044\n",
      "Epoch:  24\n",
      "Loss:  0.8168462 #########   Accuracy:  0.77992076\n",
      "Epoch:  25\n",
      "Loss:  0.90460515 #########   Accuracy:  0.77909154\n",
      "Epoch:  26\n",
      "Loss:  5.7322197 #########   Accuracy:  0.76001966\n",
      "Epoch:  27\n",
      "Loss:  2.190646 #########   Accuracy:  0.7824084\n",
      "Epoch:  28\n",
      "Loss:  3.1006908 #########   Accuracy:  0.77903014\n",
      "Epoch:  29\n",
      "Loss:  3.416402 #########   Accuracy:  0.81462485\n",
      "Epoch:  30\n",
      "Loss:  3.9759297 #########   Accuracy:  0.814717\n",
      "Epoch:  31\n",
      "Loss:  4.060291 #########   Accuracy:  0.8133657\n",
      "Epoch:  32\n",
      "Loss:  3.4820755 #########   Accuracy:  0.81465554\n",
      "Epoch:  33\n",
      "Loss:  2.790544 #########   Accuracy:  0.8087897\n",
      "Epoch:  34\n",
      "Loss:  1.4928279 #########   Accuracy:  0.8032923\n",
      "Epoch:  35\n",
      "Loss:  10.925429 #########   Accuracy:  0.78971773\n",
      "Epoch:  36\n",
      "Loss:  0.7099205 #########   Accuracy:  0.8166825\n",
      "Epoch:  37\n",
      "Loss:  1.9105417 #########   Accuracy:  0.81831026\n",
      "Epoch:  38\n",
      "Loss:  2.486382 #########   Accuracy:  0.8147477\n",
      "Epoch:  39\n",
      "Loss:  2.7704194 #########   Accuracy:  0.8127207\n",
      "Epoch:  40\n",
      "Loss:  2.756093 #########   Accuracy:  0.8180338\n",
      "Epoch:  41\n",
      "Loss:  2.409286 #########   Accuracy:  0.8203986\n",
      "Epoch:  42\n",
      "Loss:  1.9792966 #########   Accuracy:  0.820675\n",
      "Epoch:  43\n",
      "Loss:  1.3407907 #########   Accuracy:  0.81963086\n",
      "Epoch:  44\n",
      "Loss:  0.5923033 #########   Accuracy:  0.8188938\n",
      "Epoch:  45\n",
      "Loss:  4.841755 #########   Accuracy:  0.8017567\n",
      "Epoch:  46\n",
      "Loss:  0.5055046 #########   Accuracy:  0.82829154\n",
      "Epoch:  47\n",
      "Loss:  1.0269636 #########   Accuracy:  0.8205829\n",
      "Epoch:  48\n",
      "Loss:  1.3048644 #########   Accuracy:  0.81944656\n",
      "Epoch:  49\n",
      "Loss:  1.4346133 #########   Accuracy:  0.8221492\n",
      "Epoch:  50\n",
      "Loss:  1.4669809 #########   Accuracy:  0.8189245\n",
      "Epoch:  51\n",
      "Loss:  1.3368987 #########   Accuracy:  0.8232855\n",
      "Epoch:  52\n",
      "Loss:  1.1713115 #########   Accuracy:  0.82012224\n",
      "Epoch:  53\n",
      "Loss:  0.90771335 #########   Accuracy:  0.82104355\n",
      "Epoch:  54\n",
      "Loss:  0.60281616 #########   Accuracy:  0.82101285\n",
      "Epoch:  55\n",
      "Loss:  0.5160313 #########   Accuracy:  0.80261666\n",
      "Epoch:  56\n",
      "Loss:  0.59654546 #########   Accuracy:  0.8199687\n",
      "Epoch:  57\n",
      "Loss:  0.81329125 #########   Accuracy:  0.82147354\n",
      "Epoch:  58\n",
      "Loss:  0.9422915 #########   Accuracy:  0.8219035\n",
      "Epoch:  59\n",
      "Loss:  0.9684637 #########   Accuracy:  0.8298578\n",
      "Epoch:  60\n",
      "Loss:  0.9411379 #########   Accuracy:  0.82079786\n",
      "Epoch:  61\n",
      "Loss:  0.8441697 #########   Accuracy:  0.8222413\n",
      "Epoch:  62\n",
      "Loss:  0.6984591 #########   Accuracy:  0.8239919\n",
      "Epoch:  63\n",
      "Loss:  0.5217827 #########   Accuracy:  0.82116646\n",
      "Epoch:  64\n",
      "Loss:  0.33443904 #########   Accuracy:  0.8340653\n",
      "Epoch:  65\n",
      "Loss:  1.731063 #########   Accuracy:  0.80289304\n",
      "Epoch:  66\n",
      "Loss:  0.39022648 #########   Accuracy:  0.82233346\n",
      "Epoch:  67\n",
      "Loss:  0.56792897 #########   Accuracy:  0.8232855\n",
      "Epoch:  68\n",
      "Loss:  0.69011927 #########   Accuracy:  0.8219956\n",
      "Epoch:  69\n",
      "Loss:  0.75148386 #########   Accuracy:  0.8207058\n",
      "Epoch:  70\n",
      "Loss:  0.7482343 #########   Accuracy:  0.8273087\n",
      "Epoch:  71\n",
      "Loss:  0.70278424 #########   Accuracy:  0.8219035\n",
      "Epoch:  72\n",
      "Loss:  0.60426074 #########   Accuracy:  0.8213507\n",
      "Epoch:  73\n",
      "Loss:  0.4626736 #########   Accuracy:  0.830902\n",
      "Epoch:  74\n",
      "Loss:  0.33272645 #########   Accuracy:  0.8425724\n",
      "Epoch:  75\n",
      "Loss:  1.3200195 #########   Accuracy:  0.8046129\n",
      "Epoch:  76\n",
      "Loss:  0.40839788 #########   Accuracy:  0.8237155\n",
      "Epoch:  77\n",
      "Loss:  0.5991401 #########   Accuracy:  0.8222413\n",
      "Epoch:  78\n",
      "Loss:  0.7239464 #########   Accuracy:  0.8284758\n",
      "Epoch:  79\n",
      "Loss:  0.80646074 #########   Accuracy:  0.8210743\n",
      "Epoch:  80\n",
      "Loss:  0.83394486 #########   Accuracy:  0.822702\n",
      "Epoch:  81\n",
      "Loss:  0.8208045 #########   Accuracy:  0.8248825\n",
      "Epoch:  82\n",
      "Loss:  0.7796379 #########   Accuracy:  0.82101285\n",
      "Epoch:  83\n",
      "Loss:  0.70289624 #########   Accuracy:  0.8238076\n",
      "Epoch:  84\n",
      "Loss:  0.60606533 #########   Accuracy:  0.82432973\n",
      "Epoch:  85\n",
      "Loss:  0.4944084 #########   Accuracy:  0.82230276\n",
      "Epoch:  86\n",
      "Loss:  0.36858398 #########   Accuracy:  0.8290286\n",
      "Epoch:  87\n",
      "Loss:  0.55833143 #########   Accuracy:  0.80556494\n",
      "Epoch:  88\n",
      "Loss:  0.3541126 #########   Accuracy:  0.82647955\n",
      "Epoch:  89\n",
      "Loss:  0.42202562 #########   Accuracy:  0.82515895\n",
      "Epoch:  90\n",
      "Loss:  0.4647161 #########   Accuracy:  0.8264488\n",
      "Epoch:  91\n",
      "Loss:  0.48283505 #########   Accuracy:  0.82254845\n",
      "Epoch:  92\n",
      "Loss:  0.47003308 #########   Accuracy:  0.8255582\n",
      "Epoch:  93\n",
      "Loss:  0.43637368 #########   Accuracy:  0.8285372\n",
      "Epoch:  94\n",
      "Loss:  0.39109936 #########   Accuracy:  0.82604957\n",
      "Epoch:  95\n",
      "Loss:  0.33837643 #########   Accuracy:  0.83492523\n",
      "Epoch:  96\n",
      "Loss:  0.355599 #########   Accuracy:  0.82168853\n",
      "Epoch:  97\n",
      "Loss:  0.52707285 #########   Accuracy:  0.8047357\n",
      "Epoch:  98\n",
      "Loss:  0.4091175 #########   Accuracy:  0.83022636\n",
      "Epoch:  99\n",
      "Loss:  0.53877276 #########   Accuracy:  0.8290286\n",
      "Epoch:  100\n",
      "Loss:  0.6393315 #########   Accuracy:  0.82869077\n",
      "Epoch:  101\n",
      "Loss:  0.7054274 #########   Accuracy:  0.82764655\n",
      "Epoch:  102\n",
      "Loss:  0.7401877 #########   Accuracy:  0.83157766\n",
      "Epoch:  103\n",
      "Loss:  0.7499218 #########   Accuracy:  0.83209974\n",
      "Epoch:  104\n",
      "Loss:  0.73498195 #########   Accuracy:  0.82737017\n",
      "Epoch:  105\n",
      "Loss:  0.6982088 #########   Accuracy:  0.8328368\n",
      "Epoch:  106\n",
      "Loss:  0.64566815 #########   Accuracy:  0.8330211\n",
      "Epoch:  107\n",
      "Loss:  0.58100855 #########   Accuracy:  0.82896715\n",
      "Epoch:  108\n",
      "Loss:  0.50719357 #########   Accuracy:  0.833881\n",
      "Epoch:  109\n",
      "Loss:  0.42854774 #########   Accuracy:  0.83428025\n",
      "Epoch:  110\n",
      "Loss:  0.35353982 #########   Accuracy:  0.83627653\n",
      "Epoch:  111\n",
      "Loss:  0.3304582 #########   Accuracy:  0.84088326\n",
      "Epoch:  112\n",
      "Loss:  0.65771854 #########   Accuracy:  0.812905\n",
      "Epoch:  113\n",
      "Loss:  0.32443026 #########   Accuracy:  0.8534443\n",
      "Epoch:  114\n",
      "Loss:  0.34598532 #########   Accuracy:  0.83729\n",
      "Epoch:  115\n",
      "Loss:  0.38627118 #########   Accuracy:  0.8372286\n",
      "Epoch:  116\n",
      "Loss:  0.4151987 #########   Accuracy:  0.8364301\n",
      "Epoch:  117\n",
      "Loss:  0.42970526 #########   Accuracy:  0.8338196\n",
      "Epoch:  118\n",
      "Loss:  0.43024382 #########   Accuracy:  0.8364915\n",
      "Epoch:  119\n",
      "Loss:  0.41514012 #########   Accuracy:  0.83535516\n",
      "Epoch:  120\n",
      "Loss:  0.38019085 #########   Accuracy:  0.8376585\n",
      "Epoch:  121\n",
      "Loss:  0.32837185 #########   Accuracy:  0.84754765\n",
      "Epoch:  122\n",
      "Loss:  0.656253 #########   Accuracy:  0.8138264\n",
      "Epoch:  123\n",
      "Loss:  0.35229653 #########   Accuracy:  0.8417125\n",
      "Epoch:  124\n",
      "Loss:  0.43669984 #########   Accuracy:  0.8330211\n",
      "Epoch:  125\n",
      "Loss:  0.49720412 #########   Accuracy:  0.83495593\n",
      "Epoch:  126\n",
      "Loss:  0.534543 #########   Accuracy:  0.8349866\n",
      "Epoch:  127\n",
      "Loss:  0.5474706 #########   Accuracy:  0.8352631\n",
      "Epoch:  128\n",
      "Loss:  0.53678894 #########   Accuracy:  0.8365222\n",
      "Epoch:  129\n",
      "Loss:  0.50202674 #########   Accuracy:  0.8362151\n",
      "Epoch:  130\n",
      "Loss:  0.4406956 #########   Accuracy:  0.8380885\n",
      "Epoch:  131\n",
      "Loss:  0.3574043 #########   Accuracy:  0.84085256\n",
      "Epoch:  132\n",
      "Loss:  0.3649576 #########   Accuracy:  0.82393044\n",
      "Epoch:  133\n",
      "Loss:  0.3277851 #########   Accuracy:  0.8400848\n",
      "Epoch:  134\n",
      "Loss:  0.32000583 #########   Accuracy:  0.8522158\n",
      "Epoch:  135\n",
      "Loss:  0.33450472 #########   Accuracy:  0.8476091\n",
      "Epoch:  136\n",
      "Loss:  0.3406167 #########   Accuracy:  0.8441387\n",
      "Epoch:  137\n",
      "Loss:  0.33543125 #########   Accuracy:  0.8471484\n",
      "Epoch:  138\n",
      "Loss:  0.32338205 #########   Accuracy:  0.8493904\n",
      "Epoch:  139\n",
      "Loss:  0.31507066 #########   Accuracy:  0.8582046\n",
      "Epoch:  140\n",
      "Loss:  0.3251272 #########   Accuracy:  0.8431866\n",
      "Epoch:  141\n",
      "Loss:  0.32716045 #########   Accuracy:  0.8412518\n",
      "Epoch:  142\n",
      "Loss:  0.3159499 #########   Accuracy:  0.858266\n",
      "Epoch:  143\n",
      "Loss:  0.3216138 #########   Accuracy:  0.85258436\n",
      "Epoch:  144\n",
      "Loss:  0.33178145 #########   Accuracy:  0.84684134\n",
      "Epoch:  145\n",
      "Loss:  0.33135673 #########   Accuracy:  0.8414975\n",
      "Epoch:  146\n",
      "Loss:  0.3216912 #########   Accuracy:  0.8468106\n",
      "Epoch:  147\n",
      "Loss:  0.32080245 #########   Accuracy:  0.84856117\n",
      "Epoch:  148\n",
      "Loss:  0.31821615 #########   Accuracy:  0.8574982\n",
      "Epoch:  149\n",
      "Loss:  0.31474075 #########   Accuracy:  0.85786676\n",
      "Epoch:  150\n",
      "Loss:  0.3182721 #########   Accuracy:  0.85058814\n",
      "Epoch:  151\n",
      "Loss:  0.31734252 #########   Accuracy:  0.8517859\n",
      "Epoch:  152\n",
      "Loss:  0.31501853 #########   Accuracy:  0.85347503\n",
      "Epoch:  153\n",
      "Loss:  0.3169254 #########   Accuracy:  0.8569147\n",
      "Epoch:  154\n",
      "Loss:  0.31625834 #########   Accuracy:  0.85783607\n",
      "Epoch:  155\n",
      "Loss:  0.3132009 #########   Accuracy:  0.8579589\n",
      "Epoch:  156\n",
      "Loss:  0.31377155 #########   Accuracy:  0.8566997\n",
      "Epoch:  157\n",
      "Loss:  0.3150778 #########   Accuracy:  0.85743684\n",
      "Epoch:  158\n",
      "Loss:  0.31377766 #########   Accuracy:  0.85676116\n",
      "Epoch:  159\n",
      "Loss:  0.31358874 #########   Accuracy:  0.8545192\n",
      "Epoch:  160\n",
      "Loss:  0.3143136 #########   Accuracy:  0.85519487\n",
      "Epoch:  161\n",
      "Loss:  0.313365 #########   Accuracy:  0.8539971\n",
      "Epoch:  162\n",
      "Loss:  0.31178984 #########   Accuracy:  0.85970944\n",
      "Epoch:  163\n",
      "Loss:  0.31146982 #########   Accuracy:  0.85915667\n",
      "Epoch:  164\n",
      "Loss:  0.31180122 #########   Accuracy:  0.8574982\n",
      "Epoch:  165\n",
      "Loss:  0.31141657 #########   Accuracy:  0.85918736\n",
      "Epoch:  166\n",
      "Loss:  0.31088167 #########   Accuracy:  0.85725254\n",
      "Epoch:  167\n",
      "Loss:  0.31096065 #########   Accuracy:  0.85854244\n",
      "Epoch:  168\n",
      "Loss:  0.31152746 #########   Accuracy:  0.85722184\n",
      "Epoch:  169\n",
      "Loss:  0.3116611 #########   Accuracy:  0.856454\n",
      "Epoch:  170\n",
      "Loss:  0.31140715 #########   Accuracy:  0.85633117\n",
      "Epoch:  171\n",
      "Loss:  0.31696987 #########   Accuracy:  0.8462885\n",
      "Epoch:  172\n",
      "Loss:  0.3396785 #########   Accuracy:  0.84705627\n",
      "Epoch:  173\n",
      "Loss:  0.31357005 #########   Accuracy:  0.85166305\n",
      "Epoch:  174\n",
      "Loss:  0.31273764 #########   Accuracy:  0.8557477\n",
      "Epoch:  175\n",
      "Loss:  0.32964036 #########   Accuracy:  0.84549\n",
      "Epoch:  176\n",
      "Loss:  0.31515154 #########   Accuracy:  0.8481312\n",
      "Epoch:  177\n",
      "Loss:  0.30866417 #########   Accuracy:  0.85964805\n",
      "Epoch:  178\n",
      "Loss:  0.31413466 #########   Accuracy:  0.8516323\n",
      "Epoch:  179\n",
      "Loss:  0.32040134 #########   Accuracy:  0.8465342\n",
      "Epoch:  180\n",
      "Loss:  0.31653547 #########   Accuracy:  0.84905255\n",
      "Epoch:  181\n",
      "Loss:  0.30919293 #########   Accuracy:  0.85786676\n",
      "Epoch:  182\n",
      "Loss:  0.31128252 #########   Accuracy:  0.8597402\n",
      "Epoch:  183\n",
      "Loss:  0.31500372 #########   Accuracy:  0.8526458\n",
      "Epoch:  184\n",
      "Loss:  0.31266546 #########   Accuracy:  0.8542121\n",
      "Epoch:  185\n",
      "Loss:  0.30985066 #########   Accuracy:  0.85614693\n",
      "Epoch:  186\n",
      "Loss:  0.31067654 #########   Accuracy:  0.85651547\n",
      "Epoch:  187\n",
      "Loss:  0.31103423 #########   Accuracy:  0.8579282\n",
      "Epoch:  188\n",
      "Loss:  0.3069619 #########   Accuracy:  0.8595252\n",
      "Epoch:  189\n",
      "Loss:  0.30567792 #########   Accuracy:  0.8598323\n",
      "Epoch:  190\n",
      "Loss:  0.3137632 #########   Accuracy:  0.8523694\n",
      "Epoch:  191\n",
      "Loss:  0.3670685 #########   Accuracy:  0.8460428\n",
      "Epoch:  192\n",
      "Loss:  0.30771598 #########   Accuracy:  0.85571694\n",
      "Epoch:  193\n",
      "Loss:  0.38754645 #########   Accuracy:  0.8473634\n",
      "Epoch:  194\n",
      "Loss:  0.310058 #########   Accuracy:  0.8529529\n",
      "Epoch:  195\n",
      "Loss:  0.40178728 #########   Accuracy:  0.84687203\n",
      "Epoch:  196\n",
      "Loss:  0.37146962 #########   Accuracy:  0.84776264\n",
      "Epoch:  197\n",
      "Loss:  0.36716712 #########   Accuracy:  0.84456867\n",
      "Epoch:  198\n",
      "Loss:  0.35696948 #########   Accuracy:  0.847087\n",
      "Epoch:  199\n",
      "Loss:  0.3620149 #########   Accuracy:  0.84862256\n",
      "Epoch:  200\n",
      "Loss:  0.37106195 #########   Accuracy:  0.84512144\n",
      "Epoch:  201\n",
      "Loss:  0.32425326 #########   Accuracy:  0.8449372\n",
      "Epoch:  202\n",
      "Loss:  0.33301204 #########   Accuracy:  0.8462885\n",
      "Epoch:  203\n",
      "Loss:  0.32923767 #########   Accuracy:  0.8469642\n",
      "Epoch:  204\n",
      "Loss:  0.32540265 #########   Accuracy:  0.84908324\n",
      "Epoch:  205\n",
      "Loss:  0.3297021 #########   Accuracy:  0.8462885\n",
      "Epoch:  206\n",
      "Loss:  0.31257817 #########   Accuracy:  0.8483155\n",
      "Epoch:  207\n",
      "Loss:  0.33344233 #########   Accuracy:  0.8489911\n",
      "Epoch:  208\n",
      "Loss:  0.32181972 #########   Accuracy:  0.8469642\n",
      "Epoch:  209\n",
      "Loss:  0.32639566 #########   Accuracy:  0.8481005\n",
      "Epoch:  210\n",
      "Loss:  0.31915793 #########   Accuracy:  0.8481005\n",
      "Epoch:  211\n",
      "Loss:  0.32192186 #########   Accuracy:  0.8476091\n",
      "Epoch:  212\n",
      "Loss:  0.32412806 #########   Accuracy:  0.8496975\n",
      "Epoch:  213\n",
      "Loss:  0.30625135 #########   Accuracy:  0.8560548\n",
      "Epoch:  214\n",
      "Loss:  0.31676656 #########   Accuracy:  0.84727126\n",
      "Epoch:  215\n",
      "Loss:  0.3059296 #########   Accuracy:  0.85783607\n",
      "Epoch:  216\n",
      "Loss:  0.31283316 #########   Accuracy:  0.85064954\n",
      "Epoch:  217\n",
      "Loss:  0.30331928 #########   Accuracy:  0.861675\n",
      "Epoch:  218\n",
      "Loss:  0.3083632 #########   Accuracy:  0.85009676\n",
      "Epoch:  219\n",
      "Loss:  0.30329677 #########   Accuracy:  0.8586346\n",
      "Epoch:  220\n",
      "Loss:  0.30662704 #########   Accuracy:  0.85633117\n",
      "Epoch:  221\n",
      "Loss:  0.30267748 #########   Accuracy:  0.86093795\n",
      "Epoch:  222\n",
      "Loss:  0.30417642 #########   Accuracy:  0.8574982\n",
      "Epoch:  223\n",
      "Loss:  0.30172905 #########   Accuracy:  0.8608151\n",
      "Epoch:  224\n",
      "Loss:  0.30210575 #########   Accuracy:  0.8595559\n",
      "Epoch:  225\n",
      "Loss:  0.30107918 #########   Accuracy:  0.86274993\n",
      "Epoch:  226\n",
      "Loss:  0.3010074 #########   Accuracy:  0.86146003\n",
      "Epoch:  227\n",
      "Loss:  0.30097583 #########   Accuracy:  0.86093795\n",
      "Epoch:  228\n",
      "Loss:  0.30008414 #########   Accuracy:  0.86124504\n",
      "Epoch:  229\n",
      "Loss:  0.30034554 #########   Accuracy:  0.86296487\n",
      "Epoch:  230\n",
      "Loss:  0.29906908 #########   Accuracy:  0.8628113\n",
      "Epoch:  231\n",
      "Loss:  0.29968667 #########   Accuracy:  0.8608765\n",
      "Epoch:  232\n",
      "Loss:  0.2985661 #########   Accuracy:  0.86256564\n",
      "Epoch:  233\n",
      "Loss:  0.30036008 #########   Accuracy:  0.86231995\n",
      "Epoch:  234\n",
      "Loss:  0.30147824 #########   Accuracy:  0.8616136\n",
      "Epoch:  235\n",
      "Loss:  0.3102378 #########   Accuracy:  0.85479563\n",
      "Epoch:  236\n",
      "Loss:  0.3180294 #########   Accuracy:  0.8536593\n",
      "Epoch:  237\n",
      "Loss:  0.3209432 #########   Accuracy:  0.85307574\n",
      "Epoch:  238\n",
      "Loss:  0.30047655 #########   Accuracy:  0.8619821\n",
      "Epoch:  239\n",
      "Loss:  0.310834 #########   Accuracy:  0.8551334\n",
      "Epoch:  240\n",
      "Loss:  0.30567503 #########   Accuracy:  0.85786676\n",
      "Epoch:  241\n",
      "Loss:  0.302944 #########   Accuracy:  0.8609072\n",
      "Epoch:  242\n",
      "Loss:  0.31014705 #########   Accuracy:  0.85943305\n",
      "Epoch:  243\n",
      "Loss:  0.30100003 #########   Accuracy:  0.8609072\n",
      "Epoch:  244\n",
      "Loss:  0.3036277 #########   Accuracy:  0.86201286\n",
      "Epoch:  245\n",
      "Loss:  0.3035057 #########   Accuracy:  0.8590645\n",
      "Epoch:  246\n",
      "Loss:  0.301881 #########   Accuracy:  0.86096865\n",
      "Epoch:  247\n",
      "Loss:  0.30162904 #########   Accuracy:  0.8613065\n",
      "Epoch:  248\n",
      "Loss:  0.2986179 #########   Accuracy:  0.8635484\n",
      "Epoch:  249\n",
      "Loss:  0.30143824 #########   Accuracy:  0.8608151\n",
      "Epoch:  250\n",
      "Loss:  0.2971851 #########   Accuracy:  0.86333346\n",
      "Epoch:  251\n",
      "Loss:  0.2982749 #########   Accuracy:  0.8628113\n",
      "Epoch:  252\n",
      "Loss:  0.2984838 #########   Accuracy:  0.8617672\n",
      "Epoch:  253\n",
      "Loss:  0.296084 #########   Accuracy:  0.86474615\n",
      "Epoch:  254\n",
      "Loss:  0.2967347 #########   Accuracy:  0.86391693\n",
      "Epoch:  255\n",
      "Loss:  0.2958842 #########   Accuracy:  0.8635484\n",
      "Epoch:  256\n",
      "Loss:  0.2955344 #########   Accuracy:  0.863702\n",
      "Epoch:  257\n",
      "Loss:  0.29552236 #########   Accuracy:  0.8653297\n",
      "Epoch:  258\n",
      "Loss:  0.29489145 #########   Accuracy:  0.8630263\n",
      "Epoch:  259\n",
      "Loss:  0.29375216 #########   Accuracy:  0.86446977\n",
      "Epoch:  260\n",
      "Loss:  0.29438233 #########   Accuracy:  0.8658825\n",
      "Epoch:  261\n",
      "Loss:  0.29377392 #########   Accuracy:  0.8641626\n",
      "Epoch:  262\n",
      "Loss:  0.2930695 #########   Accuracy:  0.8643776\n",
      "Epoch:  263\n",
      "Loss:  0.29243606 #########   Accuracy:  0.8656675\n",
      "Epoch:  264\n",
      "Loss:  0.2926338 #########   Accuracy:  0.8651147\n",
      "Epoch:  265\n",
      "Loss:  0.29246327 #########   Accuracy:  0.8652683\n",
      "Epoch:  266\n",
      "Loss:  0.2916508 #########   Accuracy:  0.8645619\n",
      "Epoch:  267\n",
      "Loss:  0.2912862 #########   Accuracy:  0.8655754\n",
      "Epoch:  268\n",
      "Loss:  0.29123807 #########   Accuracy:  0.86671174\n",
      "Epoch:  269\n",
      "Loss:  0.29122245 #########   Accuracy:  0.86560607\n",
      "Epoch:  270\n",
      "Loss:  0.2907622 #########   Accuracy:  0.86652744\n",
      "Epoch:  271\n",
      "Loss:  0.29053342 #########   Accuracy:  0.8664046\n",
      "Epoch:  272\n",
      "Loss:  0.2900417 #########   Accuracy:  0.8666503\n",
      "Epoch:  273\n",
      "Loss:  0.28956282 #########   Accuracy:  0.8664046\n",
      "Epoch:  274\n",
      "Loss:  0.28940818 #########   Accuracy:  0.86631244\n",
      "Epoch:  275\n",
      "Loss:  0.28916728 #########   Accuracy:  0.8665889\n",
      "Epoch:  276\n",
      "Loss:  0.28918368 #########   Accuracy:  0.86603606\n",
      "Epoch:  277\n",
      "Loss:  0.2891299 #########   Accuracy:  0.8668346\n",
      "Epoch:  278\n",
      "Loss:  0.2895213 #########   Accuracy:  0.86609745\n",
      "Epoch:  279\n",
      "Loss:  0.29085276 #########   Accuracy:  0.86674243\n",
      "Epoch:  280\n",
      "Loss:  0.29285383 #########   Accuracy:  0.863487\n",
      "Epoch:  281\n",
      "Loss:  0.297812 #########   Accuracy:  0.8624735\n",
      "Epoch:  282\n",
      "Loss:  0.29764745 #########   Accuracy:  0.86093795\n",
      "Epoch:  283\n",
      "Loss:  0.29336306 #########   Accuracy:  0.8657597\n",
      "Epoch:  284\n",
      "Loss:  0.28840363 #########   Accuracy:  0.86603606\n",
      "Epoch:  285\n",
      "Loss:  0.29148918 #########   Accuracy:  0.8651147\n",
      "Epoch:  286\n",
      "Loss:  0.29446727 #########   Accuracy:  0.86367124\n",
      "Epoch:  287\n",
      "Loss:  0.2891319 #########   Accuracy:  0.86609745\n",
      "Epoch:  288\n",
      "Loss:  0.28720853 #########   Accuracy:  0.86585176\n",
      "Epoch:  289\n",
      "Loss:  0.2901217 #########   Accuracy:  0.8661896\n",
      "Epoch:  290\n",
      "Loss:  0.29098982 #########   Accuracy:  0.8645926\n",
      "Epoch:  291\n",
      "Loss:  0.28961068 #########   Accuracy:  0.866681\n",
      "Epoch:  292\n",
      "Loss:  0.28663316 #########   Accuracy:  0.8666503\n",
      "Epoch:  293\n",
      "Loss:  0.28675234 #########   Accuracy:  0.86600536\n",
      "Epoch:  294\n",
      "Loss:  0.28856885 #########   Accuracy:  0.8672338\n",
      "Epoch:  295\n",
      "Loss:  0.2881325 #########   Accuracy:  0.86600536\n",
      "Epoch:  296\n",
      "Loss:  0.28642142 #########   Accuracy:  0.8681859\n",
      "Epoch:  297\n",
      "Loss:  0.28540146 #########   Accuracy:  0.86711097\n",
      "Epoch:  298\n",
      "Loss:  0.285739 #########   Accuracy:  0.86741805\n",
      "Epoch:  299\n",
      "Loss:  0.28753608 #########   Accuracy:  0.86766374\n",
      "Epoch:  300\n",
      "Loss:  0.28808448 #########   Accuracy:  0.8655754\n",
      "Epoch:  301\n",
      "Loss:  0.2894769 #########   Accuracy:  0.86652744\n",
      "Epoch:  302\n",
      "Loss:  0.28683937 #########   Accuracy:  0.86514544\n",
      "Epoch:  303\n",
      "Loss:  0.28436303 #########   Accuracy:  0.8685544\n",
      "Epoch:  304\n",
      "Loss:  0.28612462 #########   Accuracy:  0.8681244\n",
      "Epoch:  305\n",
      "Loss:  0.28667837 #########   Accuracy:  0.865514\n",
      "Epoch:  306\n",
      "Loss:  0.28850958 #########   Accuracy:  0.86446977\n",
      "Epoch:  307\n",
      "Loss:  0.28636912 #########   Accuracy:  0.86520684\n",
      "Epoch:  308\n",
      "Loss:  0.28438362 #########   Accuracy:  0.8690765\n",
      "Epoch:  309\n",
      "Loss:  0.28542876 #########   Accuracy:  0.86864656\n",
      "Epoch:  310\n",
      "Loss:  0.28344306 #########   Accuracy:  0.8694143\n",
      "Epoch:  311\n",
      "Loss:  0.28335518 #########   Accuracy:  0.8685544\n",
      "Epoch:  312\n",
      "Loss:  0.28414205 #########   Accuracy:  0.8669574\n",
      "Epoch:  313\n",
      "Loss:  0.28354785 #########   Accuracy:  0.8681552\n",
      "Epoch:  314\n",
      "Loss:  0.28310785 #########   Accuracy:  0.8690765\n",
      "Epoch:  315\n",
      "Loss:  0.28553233 #########   Accuracy:  0.8694758\n",
      "Epoch:  316\n",
      "Loss:  0.28997552 #########   Accuracy:  0.86468476\n",
      "Epoch:  317\n",
      "Loss:  0.3041018 #########   Accuracy:  0.8588803\n",
      "Epoch:  318\n",
      "Loss:  0.30277687 #########   Accuracy:  0.85743684\n",
      "Epoch:  319\n",
      "Loss:  0.28612834 #########   Accuracy:  0.8679709\n",
      "Epoch:  320\n",
      "Loss:  0.28863484 #########   Accuracy:  0.866681\n",
      "Epoch:  321\n",
      "Loss:  0.2971197 #########   Accuracy:  0.86317986\n",
      "Epoch:  322\n",
      "Loss:  0.28876406 #########   Accuracy:  0.86757165\n",
      "Epoch:  323\n",
      "Loss:  0.28861615 #########   Accuracy:  0.86760235\n",
      "Epoch:  324\n",
      "Loss:  0.29430634 #########   Accuracy:  0.8610915\n",
      "Epoch:  325\n",
      "Loss:  0.28761616 #########   Accuracy:  0.86766374\n",
      "Epoch:  326\n",
      "Loss:  0.2898583 #########   Accuracy:  0.8657597\n",
      "Epoch:  327\n",
      "Loss:  0.29275912 #########   Accuracy:  0.86523753\n",
      "Epoch:  328\n",
      "Loss:  0.28866404 #########   Accuracy:  0.8666196\n",
      "Epoch:  329\n",
      "Loss:  0.28585613 #########   Accuracy:  0.8679709\n",
      "Epoch:  330\n",
      "Loss:  0.28954348 #########   Accuracy:  0.8643162\n",
      "Epoch:  331\n",
      "Loss:  0.28609926 #########   Accuracy:  0.86803234\n",
      "Epoch:  332\n",
      "Loss:  0.28479388 #########   Accuracy:  0.86760235\n",
      "Epoch:  333\n",
      "Loss:  0.28642395 #########   Accuracy:  0.8653297\n",
      "Epoch:  334\n",
      "Loss:  0.28363314 #########   Accuracy:  0.8685237\n",
      "Epoch:  335\n",
      "Loss:  0.28279793 #########   Accuracy:  0.86867726\n",
      "Epoch:  336\n",
      "Loss:  0.28418055 #########   Accuracy:  0.8679402\n",
      "Epoch:  337\n",
      "Loss:  0.28202853 #########   Accuracy:  0.8695986\n",
      "Epoch:  338\n",
      "Loss:  0.2815954 #########   Accuracy:  0.8688308\n",
      "Epoch:  339\n",
      "Loss:  0.28227872 #########   Accuracy:  0.86738735\n",
      "Epoch:  340\n",
      "Loss:  0.28148964 #########   Accuracy:  0.86966\n",
      "Epoch:  341\n",
      "Loss:  0.2836791 #########   Accuracy:  0.8681244\n",
      "Epoch:  342\n",
      "Loss:  0.29057857 #########   Accuracy:  0.8645619\n",
      "Epoch:  343\n",
      "Loss:  0.30382574 #########   Accuracy:  0.8624735\n",
      "Epoch:  344\n",
      "Loss:  0.2914525 #########   Accuracy:  0.86407053\n",
      "Epoch:  345\n",
      "Loss:  0.28407198 #########   Accuracy:  0.8675102\n",
      "Epoch:  346\n",
      "Loss:  0.2796965 #########   Accuracy:  0.8699364\n",
      "Epoch:  347\n",
      "Loss:  0.279708 #########   Accuracy:  0.86898434\n",
      "Epoch:  348\n",
      "Loss:  0.2825708 #########   Accuracy:  0.8680016\n",
      "Epoch:  349\n",
      "Loss:  0.28534317 #########   Accuracy:  0.86711097\n",
      "Epoch:  350\n",
      "Loss:  0.28668156 #########   Accuracy:  0.8669881\n",
      "Epoch:  351\n",
      "Loss:  0.28072482 #########   Accuracy:  0.86919934\n",
      "Epoch:  352\n",
      "Loss:  0.27733624 #########   Accuracy:  0.87024355\n",
      "Epoch:  353\n",
      "Loss:  0.27738357 #########   Accuracy:  0.87051994\n",
      "Epoch:  354\n",
      "Loss:  0.2794778 #########   Accuracy:  0.87002856\n",
      "Epoch:  355\n",
      "Loss:  0.28192866 #########   Accuracy:  0.86864656\n",
      "Epoch:  356\n",
      "Loss:  0.27970248 #########   Accuracy:  0.8692915\n",
      "Epoch:  357\n",
      "Loss:  0.27795297 #########   Accuracy:  0.8696907\n",
      "Epoch:  358\n",
      "Loss:  0.27778566 #########   Accuracy:  0.8710113\n",
      "Epoch:  359\n",
      "Loss:  0.28083068 #########   Accuracy:  0.8677252\n",
      "Epoch:  360\n",
      "Loss:  0.28661948 #########   Accuracy:  0.8675409\n",
      "Epoch:  361\n",
      "Loss:  0.2877581 #########   Accuracy:  0.86382484\n",
      "Epoch:  362\n",
      "Loss:  0.2867679 #########   Accuracy:  0.8676945\n",
      "Epoch:  363\n",
      "Loss:  0.2790933 #########   Accuracy:  0.8695679\n",
      "Epoch:  364\n",
      "Loss:  0.2772978 #########   Accuracy:  0.8705814\n",
      "Epoch:  365\n",
      "Loss:  0.27990973 #########   Accuracy:  0.86889225\n",
      "Epoch:  366\n",
      "Loss:  0.28071645 #########   Accuracy:  0.8681552\n",
      "Epoch:  367\n",
      "Loss:  0.2812611 #########   Accuracy:  0.8693529\n",
      "Epoch:  368\n",
      "Loss:  0.2779587 #########   Accuracy:  0.86919934\n",
      "Epoch:  369\n",
      "Loss:  0.27518567 #########   Accuracy:  0.8719634\n",
      "Epoch:  370\n",
      "Loss:  0.27506948 #########   Accuracy:  0.8709192\n",
      "Epoch:  371\n",
      "Loss:  0.27804574 #########   Accuracy:  0.8708885\n",
      "Epoch:  372\n",
      "Loss:  0.28405485 #########   Accuracy:  0.8690151\n",
      "Epoch:  373\n",
      "Loss:  0.28805315 #########   Accuracy:  0.8632106\n",
      "Epoch:  374\n",
      "Loss:  0.28680927 #########   Accuracy:  0.8672952\n",
      "Epoch:  375\n",
      "Loss:  0.27749994 #########   Accuracy:  0.86938363\n",
      "Epoch:  376\n",
      "Loss:  0.27886882 #########   Accuracy:  0.8693222\n",
      "Epoch:  377\n",
      "Loss:  0.28309053 #########   Accuracy:  0.86714166\n",
      "Epoch:  378\n",
      "Loss:  0.27810895 #########   Accuracy:  0.86923003\n",
      "Epoch:  379\n",
      "Loss:  0.27860963 #########   Accuracy:  0.86846226\n",
      "Epoch:  380\n",
      "Loss:  0.28086415 #########   Accuracy:  0.8699364\n",
      "Epoch:  381\n",
      "Loss:  0.27548257 #########   Accuracy:  0.8695986\n",
      "Epoch:  382\n",
      "Loss:  0.2745213 #########   Accuracy:  0.87153345\n",
      "Epoch:  383\n",
      "Loss:  0.27895465 #########   Accuracy:  0.87048924\n",
      "Epoch:  384\n",
      "Loss:  0.27756906 #########   Accuracy:  0.8682166\n",
      "Epoch:  385\n",
      "Loss:  0.2732404 #########   Accuracy:  0.87270045\n",
      "Epoch:  386\n",
      "Loss:  0.27276552 #########   Accuracy:  0.87239337\n",
      "Epoch:  387\n",
      "Loss:  0.2743074 #########   Accuracy:  0.87107277\n",
      "Epoch:  388\n",
      "Loss:  0.27423003 #########   Accuracy:  0.87294614\n",
      "Epoch:  389\n",
      "Loss:  0.27289146 #########   Accuracy:  0.8719327\n",
      "Epoch:  390\n",
      "Loss:  0.2720386 #########   Accuracy:  0.8732533\n",
      "Epoch:  391\n",
      "Loss:  0.2729935 #########   Accuracy:  0.87270045\n",
      "Epoch:  392\n",
      "Loss:  0.273739 #########   Accuracy:  0.87070423\n",
      "Epoch:  393\n",
      "Loss:  0.27393493 #########   Accuracy:  0.8734375\n",
      "Epoch:  394\n",
      "Loss:  0.27230272 #########   Accuracy:  0.87156415\n",
      "Epoch:  395\n",
      "Loss:  0.27062348 #########   Accuracy:  0.87319183\n",
      "Epoch:  396\n",
      "Loss:  0.26995966 #########   Accuracy:  0.8732226\n",
      "Epoch:  397\n",
      "Loss:  0.2704312 #########   Accuracy:  0.87334543\n",
      "Epoch:  398\n",
      "Loss:  0.27103192 #########   Accuracy:  0.8738368\n",
      "Epoch:  399\n",
      "Loss:  0.2711262 #########   Accuracy:  0.87184054\n",
      "Epoch:  400\n",
      "Loss:  0.27070168 #########   Accuracy:  0.8738982\n",
      "Epoch:  401\n",
      "Loss:  0.27057403 #########   Accuracy:  0.8719941\n",
      "Epoch:  402\n",
      "Loss:  0.2699737 #########   Accuracy:  0.87402105\n",
      "Epoch:  403\n",
      "Loss:  0.27018905 #########   Accuracy:  0.8720862\n",
      "Epoch:  404\n",
      "Loss:  0.27120078 #########   Accuracy:  0.8738675\n",
      "Epoch:  405\n",
      "Loss:  0.27354354 #########   Accuracy:  0.8707963\n",
      "Epoch:  406\n",
      "Loss:  0.28218812 #########   Accuracy:  0.8706735\n",
      "Epoch:  407\n",
      "Loss:  0.28337637 #########   Accuracy:  0.86514544\n",
      "Epoch:  408\n",
      "Loss:  0.278873 #########   Accuracy:  0.8708271\n",
      "Epoch:  409\n",
      "Loss:  0.27176568 #########   Accuracy:  0.87288475\n",
      "Epoch:  410\n",
      "Loss:  0.2762821 #########   Accuracy:  0.87024355\n",
      "Epoch:  411\n",
      "Loss:  0.28304154 #########   Accuracy:  0.86867726\n",
      "Epoch:  412\n",
      "Loss:  0.27544686 #########   Accuracy:  0.8682473\n",
      "Epoch:  413\n",
      "Loss:  0.27465338 #########   Accuracy:  0.86938363\n",
      "Epoch:  414\n",
      "Loss:  0.27951348 #########   Accuracy:  0.86889225\n",
      "Epoch:  415\n",
      "Loss:  0.27539897 #########   Accuracy:  0.8710113\n",
      "Epoch:  416\n",
      "Loss:  0.26967564 #########   Accuracy:  0.8743282\n",
      "Epoch:  417\n",
      "Loss:  0.27206737 #########   Accuracy:  0.87285405\n",
      "Epoch:  418\n",
      "Loss:  0.2745252 #########   Accuracy:  0.8704278\n",
      "Epoch:  419\n",
      "Loss:  0.27300227 #########   Accuracy:  0.87337613\n",
      "Epoch:  420\n",
      "Loss:  0.27287656 #########   Accuracy:  0.87291545\n",
      "Epoch:  421\n",
      "Loss:  0.26977304 #########   Accuracy:  0.8749424\n",
      "Epoch:  422\n",
      "Loss:  0.2683136 #########   Accuracy:  0.8741439\n",
      "Epoch:  423\n",
      "Loss:  0.2684435 #########   Accuracy:  0.8734068\n",
      "Epoch:  424\n",
      "Loss:  0.2683778 #########   Accuracy:  0.8746353\n",
      "Epoch:  425\n",
      "Loss:  0.26912996 #########   Accuracy:  0.87316114\n",
      "Epoch:  426\n",
      "Loss:  0.26757038 #########   Accuracy:  0.8745432\n",
      "Epoch:  427\n",
      "Loss:  0.2667657 #########   Accuracy:  0.87448174\n",
      "Epoch:  428\n",
      "Loss:  0.26603216 #########   Accuracy:  0.8758945\n",
      "Epoch:  429\n",
      "Loss:  0.2664002 #########   Accuracy:  0.8754952\n",
      "Epoch:  430\n",
      "Loss:  0.26599964 #########   Accuracy:  0.8747889\n",
      "Epoch:  431\n",
      "Loss:  0.2668088 #########   Accuracy:  0.87638587\n",
      "Epoch:  432\n",
      "Loss:  0.2702605 #########   Accuracy:  0.87113416\n",
      "Epoch:  433\n",
      "Loss:  0.2697688 #########   Accuracy:  0.871472\n",
      "Epoch:  434\n",
      "Loss:  0.26772633 #########   Accuracy:  0.8746353\n",
      "Epoch:  435\n",
      "Loss:  0.27271044 #########   Accuracy:  0.8740518\n",
      "Epoch:  436\n",
      "Loss:  0.28069338 #########   Accuracy:  0.8697215\n",
      "Epoch:  437\n",
      "Loss:  0.29241097 #########   Accuracy:  0.86204356\n",
      "Epoch:  438\n",
      "Loss:  0.29599854 #########   Accuracy:  0.8627192\n",
      "Epoch:  439\n",
      "Loss:  0.28050524 #########   Accuracy:  0.8672031\n",
      "Epoch:  440\n",
      "Loss:  0.31924233 #########   Accuracy:  0.856454\n",
      "Epoch:  441\n",
      "Loss:  0.28353143 #########   Accuracy:  0.8656675\n",
      "Epoch:  442\n",
      "Loss:  0.28260466 #########   Accuracy:  0.86357915\n",
      "Epoch:  443\n",
      "Loss:  0.29523242 #########   Accuracy:  0.86763304\n",
      "Epoch:  444\n",
      "Loss:  0.28250882 #########   Accuracy:  0.86314917\n",
      "Epoch:  445\n",
      "Loss:  0.27839977 #########   Accuracy:  0.8670495\n",
      "Epoch:  446\n",
      "Loss:  0.2790879 #########   Accuracy:  0.86975217\n",
      "Epoch:  447\n",
      "Loss:  0.28167605 #########   Accuracy:  0.8667731\n",
      "Epoch:  448\n",
      "Loss:  0.27836466 #########   Accuracy:  0.8707656\n",
      "Epoch:  449\n",
      "Loss:  0.2727842 #########   Accuracy:  0.87288475\n",
      "Epoch:  450\n",
      "Loss:  0.2762324 #########   Accuracy:  0.871257\n",
      "Epoch:  451\n",
      "Loss:  0.27914336 #########   Accuracy:  0.8708578\n",
      "Epoch:  452\n",
      "Loss:  0.27251673 #########   Accuracy:  0.8736218\n",
      "Epoch:  453\n",
      "Loss:  0.2711961 #########   Accuracy:  0.8748196\n",
      "Epoch:  454\n",
      "Loss:  0.2737319 #########   Accuracy:  0.8727312\n",
      "Epoch:  455\n",
      "Loss:  0.2718266 #########   Accuracy:  0.8736525\n",
      "Epoch:  456\n",
      "Loss:  0.26949313 #########   Accuracy:  0.8738982\n",
      "Epoch:  457\n",
      "Loss:  0.2686588 #########   Accuracy:  0.87426674\n",
      "Epoch:  458\n",
      "Loss:  0.26888177 #########   Accuracy:  0.8735297\n",
      "Epoch:  459\n",
      "Loss:  0.26971805 #########   Accuracy:  0.87423605\n",
      "Epoch:  460\n",
      "Loss:  0.26823953 #########   Accuracy:  0.87334543\n",
      "Epoch:  461\n",
      "Loss:  0.26922542 #########   Accuracy:  0.871687\n",
      "Epoch:  462\n",
      "Loss:  0.2686739 #########   Accuracy:  0.87402105\n",
      "Epoch:  463\n",
      "Loss:  0.26764795 #########   Accuracy:  0.8751881\n",
      "Epoch:  464\n",
      "Loss:  0.2656259 #########   Accuracy:  0.8767851\n",
      "Epoch:  465\n",
      "Loss:  0.26558787 #########   Accuracy:  0.8760787\n",
      "Epoch:  466\n",
      "Loss:  0.26709697 #########   Accuracy:  0.87402105\n",
      "Epoch:  467\n",
      "Loss:  0.26606622 #########   Accuracy:  0.87442034\n",
      "Epoch:  468\n",
      "Loss:  0.26447392 #########   Accuracy:  0.87601733\n",
      "Epoch:  469\n",
      "Loss:  0.26541194 #########   Accuracy:  0.8773686\n",
      "Epoch:  470\n",
      "Loss:  0.26423347 #########   Accuracy:  0.8756181\n",
      "Epoch:  471\n",
      "Loss:  0.26342845 #########   Accuracy:  0.8762937\n",
      "Epoch:  472\n",
      "Loss:  0.26301903 #########   Accuracy:  0.8765087\n",
      "Epoch:  473\n",
      "Loss:  0.26370752 #########   Accuracy:  0.87730724\n",
      "Epoch:  474\n",
      "Loss:  0.26511636 #########   Accuracy:  0.87334543\n",
      "Epoch:  475\n",
      "Loss:  0.2704875 #########   Accuracy:  0.87377536\n",
      "Epoch:  476\n",
      "Loss:  0.27500018 #########   Accuracy:  0.8695986\n",
      "Epoch:  477\n",
      "Loss:  0.28228414 #########   Accuracy:  0.871257\n",
      "Epoch:  478\n",
      "Loss:  0.2670381 #########   Accuracy:  0.87205553\n",
      "Epoch:  479\n",
      "Loss:  0.27359256 #########   Accuracy:  0.869445\n",
      "Epoch:  480\n",
      "Loss:  0.27593848 #########   Accuracy:  0.8699057\n",
      "Epoch:  481\n",
      "Loss:  0.26327208 #########   Accuracy:  0.8773686\n",
      "Epoch:  482\n",
      "Loss:  0.2802209 #########   Accuracy:  0.8664353\n",
      "Epoch:  483\n",
      "Loss:  0.2861451 #########   Accuracy:  0.86916864\n",
      "Epoch:  484\n",
      "Loss:  0.27525967 #########   Accuracy:  0.87448174\n",
      "Epoch:  485\n",
      "Loss:  0.2913478 #########   Accuracy:  0.85878813\n",
      "Epoch:  486\n",
      "Loss:  0.27659208 #########   Accuracy:  0.86898434\n",
      "Epoch:  487\n",
      "Loss:  0.27842826 #########   Accuracy:  0.8672031\n",
      "Epoch:  488\n",
      "Loss:  0.28218117 #########   Accuracy:  0.873069\n",
      "Epoch:  489\n",
      "Loss:  0.26807514 #########   Accuracy:  0.87402105\n",
      "Epoch:  490\n",
      "Loss:  0.27409863 #########   Accuracy:  0.8694758\n",
      "Epoch:  491\n",
      "Loss:  0.27354804 #########   Accuracy:  0.8704278\n",
      "Epoch:  492\n",
      "Loss:  0.26993278 #########   Accuracy:  0.8743589\n",
      "Epoch:  493\n",
      "Loss:  0.2721998 #########   Accuracy:  0.87669295\n",
      "Epoch:  494\n",
      "Loss:  0.26752847 #########   Accuracy:  0.8767544\n",
      "Epoch:  495\n",
      "Loss:  0.2705589 #########   Accuracy:  0.8722705\n",
      "Epoch:  496\n",
      "Loss:  0.2693868 #########   Accuracy:  0.8738368\n",
      "Epoch:  497\n",
      "Loss:  0.26707384 #########   Accuracy:  0.87660086\n",
      "Epoch:  498\n",
      "Loss:  0.2694143 #########   Accuracy:  0.8749424\n",
      "Epoch:  499\n",
      "Loss:  0.2664718 #########   Accuracy:  0.8787199\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "for ep in range(epochs):\n",
    "    print(\"Epoch: \", ep)\n",
    "    y_ = classifier(X_train)\n",
    "    mloss, grad = gradient(classifier,X_train,y_train)\n",
    "    optimizer.apply_gradients(zip(grad, classifier.trainable_variables),global_step=global_step)\n",
    "    \n",
    "    print(\"Loss: \",mloss.numpy(), \"######### \",\" Accuracy: \", accuracy(y_train,y_).numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF",
   "language": "python",
   "name": "tensorflow-cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
