{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classification.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqbi3WIN4cbz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.contrib import eager\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lH3rLFd4ccA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.enable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfAGtib-4tqV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "0d8eab81-5b6f-4674-99c4-bccfeff5f4eb"
      },
      "source": [
        "! git clone  https://Waleed-Daud:369074125800925025880dobeedoz.22@github.com/Waleed-Daud/Fairness.git\n",
        "% cd Fairness"
      ],
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Fairness'...\n",
            "remote: Enumerating objects: 10, done.\u001b[K\n",
            "remote: Counting objects:  10% (1/10)   \u001b[K\rremote: Counting objects:  20% (2/10)   \u001b[K\rremote: Counting objects:  30% (3/10)   \u001b[K\rremote: Counting objects:  40% (4/10)   \u001b[K\rremote: Counting objects:  50% (5/10)   \u001b[K\rremote: Counting objects:  60% (6/10)   \u001b[K\rremote: Counting objects:  70% (7/10)   \u001b[K\rremote: Counting objects:  80% (8/10)   \u001b[K\rremote: Counting objects:  90% (9/10)   \u001b[K\rremote: Counting objects: 100% (10/10)   \u001b[K\rremote: Counting objects: 100% (10/10), done.\u001b[K\n",
            "remote: Compressing objects:  11% (1/9)   \u001b[K\rremote: Compressing objects:  22% (2/9)   \u001b[K\rremote: Compressing objects:  33% (3/9)   \u001b[K\rremote: Compressing objects:  44% (4/9)   \u001b[K\rremote: Compressing objects:  55% (5/9)   \u001b[K\rremote: Compressing objects:  66% (6/9)   \u001b[K\rremote: Compressing objects:  77% (7/9)   \u001b[K\rremote: Compressing objects:  88% (8/9)   \u001b[K\rremote: Compressing objects: 100% (9/9)   \u001b[K\rremote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 10 (delta 1), reused 10 (delta 1), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (10/10), done.\n",
            "/content/Fairness/Fairness/Fairness/Fairness/Fairness\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sGXnFhu4ccG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = np.load(\"./adult/adult_train.npz\")\n",
        "\n",
        "test_data_ = np.load(\"./adult/adult_test.npz\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYwrG9-Q4ccP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5ed6b669-1322-4303-c23e-42d1e760f78e"
      },
      "source": [
        "X = data.f.x\n",
        "y = data.f.y\n",
        "A = data.f.a\n",
        "\n",
        "test_data = test_data_.f.x\n",
        "y_test_data = test_data_.f.y\n",
        "A_test_data = test_data_.f.a\n",
        "\n",
        "X.shape"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32561, 113)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hRDWBJJ4ccZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "columns_file = open('./adult/adult_headers.txt','r')\n",
        "columns = columns_file.read()\n",
        "columns_names = columns.split(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8f9v0N64cce",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_frame = pd.DataFrame(X,columns=columns_names) \n",
        "y_frame = pd.DataFrame(y,columns=['income'])\n",
        "A_frame = pd.DataFrame(A,columns=['A'])\n",
        "\n",
        "X_test_frame = pd.DataFrame(test_data,columns=columns_names) \n",
        "y_test_frame = pd.DataFrame(y_test_data,columns=['income']) \n",
        "A_test_frame = pd.DataFrame(A_test_data,columns=['A'])\n",
        "\n",
        "dataset_frame = pd.concat([X_frame,y_frame,A_frame],axis=1)\n",
        "\n",
        "dataset_test =  pd.concat([X_test_frame,y_test_frame,A_test_frame],axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyDOkMsY4cci",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "outputId": "6697edec-7513-49aa-da99-28e666e9ed4d"
      },
      "source": [
        "dataset_frame[:3]"
      ],
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age_u20</th>\n",
              "      <th>age_u30</th>\n",
              "      <th>age_u40</th>\n",
              "      <th>age_u50</th>\n",
              "      <th>age_u60</th>\n",
              "      <th>age_u70</th>\n",
              "      <th>age_u80</th>\n",
              "      <th>workclass_Private</th>\n",
              "      <th>workclass_Self-emp-not-inc</th>\n",
              "      <th>workclass_Self-emp-inc</th>\n",
              "      <th>workclass_Federal-gov</th>\n",
              "      <th>workclass_Local-gov</th>\n",
              "      <th>workclass_State-gov</th>\n",
              "      <th>workclass_Without-pay</th>\n",
              "      <th>workclass_Never-worked</th>\n",
              "      <th>workclass_?</th>\n",
              "      <th>education_Bachelors</th>\n",
              "      <th>education_Some-college</th>\n",
              "      <th>education_11th</th>\n",
              "      <th>education_HS-grad</th>\n",
              "      <th>education_Prof-school</th>\n",
              "      <th>education_Assoc-acdm</th>\n",
              "      <th>education_Assoc-voc</th>\n",
              "      <th>education_9th</th>\n",
              "      <th>education_7th-8th</th>\n",
              "      <th>education_12th</th>\n",
              "      <th>education_Masters</th>\n",
              "      <th>education_1st-4th</th>\n",
              "      <th>education_10th</th>\n",
              "      <th>education_Doctorate</th>\n",
              "      <th>education_5th-6th</th>\n",
              "      <th>education_Preschool</th>\n",
              "      <th>education_num</th>\n",
              "      <th>marital-status_Married-civ-spouse</th>\n",
              "      <th>marital-status_Divorced</th>\n",
              "      <th>marital-status_Never-married</th>\n",
              "      <th>marital-status_Separated</th>\n",
              "      <th>marital-status_Widowed</th>\n",
              "      <th>marital-status_Married-spouse-absent</th>\n",
              "      <th>marital-status_Married-AF-spouse</th>\n",
              "      <th>...</th>\n",
              "      <th>country_Canada</th>\n",
              "      <th>country_Germany</th>\n",
              "      <th>country_Outlying-US(Guam-USVI-etc)</th>\n",
              "      <th>country_India</th>\n",
              "      <th>country_Japan</th>\n",
              "      <th>country_Greece</th>\n",
              "      <th>country_South</th>\n",
              "      <th>country_China</th>\n",
              "      <th>country_Cuba</th>\n",
              "      <th>country_Iran</th>\n",
              "      <th>country_Honduras</th>\n",
              "      <th>country_Philippines</th>\n",
              "      <th>country_Italy</th>\n",
              "      <th>country_Poland</th>\n",
              "      <th>country_Jamaica</th>\n",
              "      <th>country_Vietnam</th>\n",
              "      <th>country_Mexico</th>\n",
              "      <th>country_Portugal</th>\n",
              "      <th>country_Ireland</th>\n",
              "      <th>country_France</th>\n",
              "      <th>country_Dominican-Republic</th>\n",
              "      <th>country_Laos</th>\n",
              "      <th>country_Ecuador</th>\n",
              "      <th>country_Taiwan</th>\n",
              "      <th>country_Haiti</th>\n",
              "      <th>country_Columbia</th>\n",
              "      <th>country_Hungary</th>\n",
              "      <th>country_Guatemala</th>\n",
              "      <th>country_Nicaragua</th>\n",
              "      <th>country_Scotland</th>\n",
              "      <th>country_Thailand</th>\n",
              "      <th>country_Yugoslavia</th>\n",
              "      <th>country_El-Salvador</th>\n",
              "      <th>country_Trinadad&amp;Tobago</th>\n",
              "      <th>country_Peru</th>\n",
              "      <th>country_Hong</th>\n",
              "      <th>country_Holand-Netherlands</th>\n",
              "      <th>country_?</th>\n",
              "      <th>income</th>\n",
              "      <th>A</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows Ã— 115 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   age_u20  age_u30  age_u40  ...  country_?  income    A\n",
              "0      1.0      0.0      0.0  ...        0.0     0.0  0.0\n",
              "1      0.0      1.0      0.0  ...        0.0     0.0  0.0\n",
              "2      0.0      0.0      0.0  ...        0.0     1.0  1.0\n",
              "\n",
              "[3 rows x 115 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaQaqECV4ccq",
        "colab_type": "text"
      },
      "source": [
        "1- Name the 10 features which are most correlated with Y, and the 10 which are most correlated with A, as measured by (absolute) Pearson correlation (ignore any NaN correlations you see."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7z_wubfp4ccr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_corr_income = dataset_frame.corr()['income']\n",
        "data_corr_A = dataset_frame.corr()['A']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcpFJ_5v4ccw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "answer1 = data_corr_income.sort_values(ascending=False)[1:11]\n",
        "answer2= data_corr_A.sort_values(ascending=False)[1:11]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mr7kCKge4cc9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "outputId": "891c61ad-e4be-4b62-8cd0-50afa03d2d09"
      },
      "source": [
        "print(answer1)\n",
        "print(\"#####################################################\")\n",
        "print(answer2)"
      ],
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "marital-status_Married-civ-spouse    0.444696\n",
            "relationship_Husband                 0.401035\n",
            "education_num                        0.335154\n",
            "hours-per-week                       0.229689\n",
            "capital-gain                         0.223329\n",
            "sex_Male                             0.215980\n",
            "A                                    0.215980\n",
            "occupation_Exec-managerial           0.214861\n",
            "occupation_Prof-specialty            0.185866\n",
            "education_Bachelors                  0.180485\n",
            "Name: income, dtype: float64\n",
            "#####################################################\n",
            "sex_Male                             1.000000\n",
            "relationship_Husband                 0.580135\n",
            "marital-status_Married-civ-spouse    0.431805\n",
            "hours-per-week                       0.229309\n",
            "occupation_Craft-repair              0.223128\n",
            "income                               0.215980\n",
            "occupation_Transport-moving          0.132468\n",
            "workclass_Self-emp-not-inc           0.107451\n",
            "race_White                           0.103486\n",
            "occupation_Farming-fishing           0.100097\n",
            "Name: A, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVQicRDy1Km4",
        "colab_type": "text"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHmRGO8V1QhI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Binary(logits):\n",
        "  \n",
        "  y = tf.sigmoid(logits)\n",
        "  y = tf.cast((y_>=0.5),dtype= tf.float32)\n",
        "  \n",
        "  return y\n",
        "\n",
        "\n",
        "def delta_dp(model, dataset, sesn_atrr):\n",
        "  \n",
        "  DP_0_selection = dataset['A'] == 0\n",
        "  DP_1_selection = dataset['A'] == 1\n",
        "  \n",
        "  DP_0_data_frame = dataset[DP_0_selection]\n",
        "  DP_1_data_frame = dataset[DP_1_selection]\n",
        "  \n",
        "  DP_0_data_frame  = DP_0_data_frame.drop(['income','A'], axis =1)\n",
        "  DP_1_data_frame = DP_1_data_frame.drop(['income','A'], axis =1)\n",
        "  \n",
        "  \n",
        "  DP_0_data = tf.convert_to_tensor(DP_0_data_frame.to_numpy() ,dtype=tf.float32) \n",
        "  DP_1_data = tf.convert_to_tensor(DP_1_data_frame.to_numpy() ,dtype=tf.float32) \n",
        "\n",
        "\n",
        "\n",
        "  DP_0_y_ = Binary(model(DP_0_data)) \n",
        "  DP_1_y_ = Binary(model(DP_1_data))\n",
        "  \n",
        "#   print(DP_0_y_)\n",
        " \n",
        "  \n",
        "  DP_0 = tf.reduce_mean(tf.multiply(DP_0_y_ , tf.subtract(tf.constant(1.0),sesn_atrr)))\n",
        "  DP_1 = tf.reduce_mean(tf.multiply(DP_1_y_ , sesn_atrr))\n",
        "  \n",
        "  delta_DP = tf.abs(DP_0 - DP_1)\n",
        "  \n",
        "  \n",
        "  return delta_DP"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCstF2pK4cdM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_validation, y_train, y_validation = train_test_split(X_frame.to_numpy(),y_frame.to_numpy(),test_size = 0.2, random_state = 42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfxMbT-w4cdH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = tf.convert_to_tensor(X_train,dtype=tf.float32)\n",
        "y_train = tf.convert_to_tensor(y_train,dtype=tf.float32)\n",
        "\n",
        "X_validation = tf.convert_to_tensor(X_validation,dtype=tf.float32)\n",
        "y_validation = tf.convert_to_tensor(y_validation,dtype=tf.float32)\n",
        "\n",
        "\n",
        "X_test = tf.convert_to_tensor(test_data,dtype=tf.float32)\n",
        "y_test = tf.convert_to_tensor(y_test_data,dtype=tf.float32)\n",
        "A_test = tf.convert_to_tensor(A_test_data,dtype=tf.float32)\n",
        "\n",
        "A = tf.convert_to_tensor(A_frame.to_numpy(),dtype=tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvY6op1v4cdQ",
        "colab_type": "text"
      },
      "source": [
        "# Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Stau0XCl4cdR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifier = tf.keras.Sequential(\n",
        "    [tf.keras.layers.Dense(input_shape=(n,),units=200,activation=tf.nn.relu),\n",
        "    tf.keras.layers.Dense(units = 300,activation=tf.nn.relu),\n",
        "    tf.keras.layers.Dense(units=50,activation=tf.nn.relu),\n",
        "    tf.keras.layers.Dense(units=20,activation=tf.nn.relu),\n",
        "    tf.keras.layers.Dense(units=1)]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCKgabmq4cdV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss(y,y_):\n",
        "    mloss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y,logits=y_))\n",
        "    return mloss\n",
        "\n",
        "\n",
        "def gradient(model,x,y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        y_ = model(x)\n",
        "        mloss = loss(y,y_)\n",
        "        return mloss, tape.gradient(mloss,model.trainable_variables)\n",
        "    \n",
        "def accuracy(y,y_):\n",
        "    y_ = tf.nn.sigmoid(y_)\n",
        "    ans = tf.cast((y_>=0.5),dtype= tf.float32)\n",
        "    res = tf.cast(tf.equal(ans,y),tf.float32)\n",
        "    \n",
        "    return tf.reduce_mean(res)\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
        "global_step = tf.Variable(0)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CCh96gPYi04",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miJfHXDP4cdY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36584
        },
        "outputId": "57e8775d-0242-4997-c277-e11e21883a55"
      },
      "source": [
        "epochs = 1000\n",
        "for ep in range(epochs):\n",
        "    print(\"Epoch: \", ep)\n",
        "    y_ = classifier(X_train)\n",
        "    mloss, grad = gradient(classifier,X_train,y_train)\n",
        "    optimizer.apply_gradients(zip(grad, classifier.trainable_variables),global_step=global_step)\n",
        "    \n",
        "    print(\"Loss: \",mloss.numpy(), \"######### \",\" Accuracy: \", accuracy(y_train,y_).numpy())\n"
      ],
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  0\n",
            "Loss:  31.265812 #########   Accuracy:  0.7577549\n",
            "Epoch:  1\n",
            "Loss:  37.64563 #########   Accuracy:  0.2422451\n",
            "Epoch:  2\n",
            "Loss:  11.30976 #########   Accuracy:  0.24236026\n",
            "Epoch:  3\n",
            "Loss:  113.36815 #########   Accuracy:  0.7577549\n",
            "Epoch:  4\n",
            "Loss:  13.795362 #########   Accuracy:  0.2433968\n",
            "Epoch:  5\n",
            "Loss:  22.912376 #########   Accuracy:  0.24800369\n",
            "Epoch:  6\n",
            "Loss:  24.88549 #########   Accuracy:  0.3050906\n",
            "Epoch:  7\n",
            "Loss:  22.385368 #########   Accuracy:  0.7785243\n",
            "Epoch:  8\n",
            "Loss:  13.7127285 #########   Accuracy:  0.7793305\n",
            "Epoch:  9\n",
            "Loss:  5.3676686 #########   Accuracy:  0.77798676\n",
            "Epoch:  10\n",
            "Loss:  38.474762 #########   Accuracy:  0.7577549\n",
            "Epoch:  11\n",
            "Loss:  8.934523 #########   Accuracy:  0.26255375\n",
            "Epoch:  12\n",
            "Loss:  17.938673 #########   Accuracy:  0.2520347\n",
            "Epoch:  13\n",
            "Loss:  22.46314 #########   Accuracy:  0.31111795\n",
            "Epoch:  14\n",
            "Loss:  23.570398 #########   Accuracy:  0.7793305\n",
            "Epoch:  15\n",
            "Loss:  22.563522 #########   Accuracy:  0.7793305\n",
            "Epoch:  16\n",
            "Loss:  20.065777 #########   Accuracy:  0.7793305\n",
            "Epoch:  17\n",
            "Loss:  17.079935 #########   Accuracy:  0.77798676\n",
            "Epoch:  18\n",
            "Loss:  14.610296 #########   Accuracy:  0.77798676\n",
            "Epoch:  19\n",
            "Loss:  11.5807495 #########   Accuracy:  0.77798676\n",
            "Epoch:  20\n",
            "Loss:  8.312029 #########   Accuracy:  0.77798676\n",
            "Epoch:  21\n",
            "Loss:  5.0947876 #########   Accuracy:  0.7780636\n",
            "Epoch:  22\n",
            "Loss:  2.3900554 #########   Accuracy:  0.7780636\n",
            "Epoch:  23\n",
            "Loss:  9.7227125 #########   Accuracy:  0.7577549\n",
            "Epoch:  24\n",
            "Loss:  0.9880691 #########   Accuracy:  0.7590986\n",
            "Epoch:  25\n",
            "Loss:  3.7446907 #########   Accuracy:  0.7793305\n",
            "Epoch:  26\n",
            "Loss:  5.938635 #########   Accuracy:  0.7793305\n",
            "Epoch:  27\n",
            "Loss:  7.40661 #########   Accuracy:  0.7693105\n",
            "Epoch:  28\n",
            "Loss:  8.239997 #########   Accuracy:  0.37684274\n",
            "Epoch:  29\n",
            "Loss:  8.534036 #########   Accuracy:  0.3700092\n",
            "Epoch:  30\n",
            "Loss:  8.409246 #########   Accuracy:  0.4580774\n",
            "Epoch:  31\n",
            "Loss:  7.9847 #########   Accuracy:  0.7486947\n",
            "Epoch:  32\n",
            "Loss:  7.384722 #########   Accuracy:  0.7774109\n",
            "Epoch:  33\n",
            "Loss:  6.4627857 #########   Accuracy:  0.7793305\n",
            "Epoch:  34\n",
            "Loss:  5.4480963 #########   Accuracy:  0.7793305\n",
            "Epoch:  35\n",
            "Loss:  4.416734 #########   Accuracy:  0.7793305\n",
            "Epoch:  36\n",
            "Loss:  3.4159112 #########   Accuracy:  0.7793305\n",
            "Epoch:  37\n",
            "Loss:  2.5834296 #########   Accuracy:  0.78032863\n",
            "Epoch:  38\n",
            "Loss:  1.9084553 #########   Accuracy:  0.7780636\n",
            "Epoch:  39\n",
            "Loss:  0.99511343 #########   Accuracy:  0.7780636\n",
            "Epoch:  40\n",
            "Loss:  2.4980597 #########   Accuracy:  0.7577549\n",
            "Epoch:  41\n",
            "Loss:  0.7979121 #########   Accuracy:  0.7783707\n",
            "Epoch:  42\n",
            "Loss:  1.1850079 #########   Accuracy:  0.7795992\n",
            "Epoch:  43\n",
            "Loss:  1.2441157 #########   Accuracy:  0.7795608\n",
            "Epoch:  44\n",
            "Loss:  1.0479354 #########   Accuracy:  0.7795608\n",
            "Epoch:  45\n",
            "Loss:  0.6961772 #########   Accuracy:  0.7800215\n",
            "Epoch:  46\n",
            "Loss:  0.88348764 #########   Accuracy:  0.7577549\n",
            "Epoch:  47\n",
            "Loss:  0.8866098 #########   Accuracy:  0.7780636\n",
            "Epoch:  48\n",
            "Loss:  1.2066555 #########   Accuracy:  0.7780636\n",
            "Epoch:  49\n",
            "Loss:  1.3878535 #########   Accuracy:  0.7800599\n",
            "Epoch:  50\n",
            "Loss:  1.4277565 #########   Accuracy:  0.77967596\n",
            "Epoch:  51\n",
            "Loss:  1.4293897 #########   Accuracy:  0.7795992\n",
            "Epoch:  52\n",
            "Loss:  1.2822464 #########   Accuracy:  0.7795992\n",
            "Epoch:  53\n",
            "Loss:  1.0799121 #########   Accuracy:  0.78113484\n",
            "Epoch:  54\n",
            "Loss:  0.8915931 #########   Accuracy:  0.8095055\n",
            "Epoch:  55\n",
            "Loss:  0.69126546 #########   Accuracy:  0.81299907\n",
            "Epoch:  56\n",
            "Loss:  0.4799008 #########   Accuracy:  0.8162239\n",
            "Epoch:  57\n",
            "Loss:  0.39314035 #########   Accuracy:  0.8040541\n",
            "Epoch:  58\n",
            "Loss:  0.6448184 #########   Accuracy:  0.79257524\n",
            "Epoch:  59\n",
            "Loss:  0.43124667 #########   Accuracy:  0.82140666\n",
            "Epoch:  60\n",
            "Loss:  0.5663444 #########   Accuracy:  0.8186809\n",
            "Epoch:  61\n",
            "Loss:  0.6530249 #########   Accuracy:  0.8177211\n",
            "Epoch:  62\n",
            "Loss:  0.6885719 #########   Accuracy:  0.8183738\n",
            "Epoch:  63\n",
            "Loss:  0.67855716 #########   Accuracy:  0.8194871\n",
            "Epoch:  64\n",
            "Loss:  0.6210938 #########   Accuracy:  0.81894964\n",
            "Epoch:  65\n",
            "Loss:  0.5308108 #########   Accuracy:  0.8203317\n",
            "Epoch:  66\n",
            "Loss:  0.41595164 #########   Accuracy:  0.8255912\n",
            "Epoch:  67\n",
            "Loss:  0.34246865 #########   Accuracy:  0.83407557\n",
            "Epoch:  68\n",
            "Loss:  0.69499105 #########   Accuracy:  0.79948556\n",
            "Epoch:  69\n",
            "Loss:  0.5084177 #########   Accuracy:  0.8222512\n",
            "Epoch:  70\n",
            "Loss:  0.7045855 #########   Accuracy:  0.8209459\n",
            "Epoch:  71\n",
            "Loss:  0.7831822 #########   Accuracy:  0.8203317\n",
            "Epoch:  72\n",
            "Loss:  0.7570287 #########   Accuracy:  0.82098436\n",
            "Epoch:  73\n",
            "Loss:  0.63038486 #########   Accuracy:  0.8215986\n",
            "Epoch:  74\n",
            "Loss:  0.45602533 #########   Accuracy:  0.8239404\n",
            "Epoch:  75\n",
            "Loss:  0.3729883 #########   Accuracy:  0.8236717\n",
            "Epoch:  76\n",
            "Loss:  0.55282384 #########   Accuracy:  0.8050906\n",
            "Epoch:  77\n",
            "Loss:  0.40344644 #########   Accuracy:  0.82881606\n",
            "Epoch:  78\n",
            "Loss:  0.54313433 #########   Accuracy:  0.8222512\n",
            "Epoch:  79\n",
            "Loss:  0.62056196 #########   Accuracy:  0.82340294\n",
            "Epoch:  80\n",
            "Loss:  0.6405728 #########   Accuracy:  0.8264742\n",
            "Epoch:  81\n",
            "Loss:  0.6064517 #########   Accuracy:  0.8245547\n",
            "Epoch:  82\n",
            "Loss:  0.52149385 #########   Accuracy:  0.8273188\n",
            "Epoch:  83\n",
            "Loss:  0.4042566 #########   Accuracy:  0.83311576\n",
            "Epoch:  84\n",
            "Loss:  0.3386893 #########   Accuracy:  0.8425599\n",
            "Epoch:  85\n",
            "Loss:  0.69425195 #########   Accuracy:  0.80413085\n",
            "Epoch:  86\n",
            "Loss:  0.41929233 #########   Accuracy:  0.8315034\n",
            "Epoch:  87\n",
            "Loss:  0.62806165 #########   Accuracy:  0.8238636\n",
            "Epoch:  88\n",
            "Loss:  0.76221466 #########   Accuracy:  0.82486176\n",
            "Epoch:  89\n",
            "Loss:  0.8298214 #########   Accuracy:  0.8226351\n",
            "Epoch:  90\n",
            "Loss:  0.8293508 #########   Accuracy:  0.8230958\n",
            "Epoch:  91\n",
            "Loss:  0.81100935 #########   Accuracy:  0.82466984\n",
            "Epoch:  92\n",
            "Loss:  0.775129 #########   Accuracy:  0.8232878\n",
            "Epoch:  93\n",
            "Loss:  0.71834135 #########   Accuracy:  0.824785\n",
            "Epoch:  94\n",
            "Loss:  0.6515108 #########   Accuracy:  0.8346514\n",
            "Epoch:  95\n",
            "Loss:  0.577339 #########   Accuracy:  0.8247466\n",
            "Epoch:  96\n",
            "Loss:  0.49078208 #########   Accuracy:  0.8239404\n",
            "Epoch:  97\n",
            "Loss:  0.39292532 #########   Accuracy:  0.82655096\n",
            "Epoch:  98\n",
            "Loss:  0.3431318 #########   Accuracy:  0.8273572\n",
            "Epoch:  99\n",
            "Loss:  0.63329375 #########   Accuracy:  0.80205774\n",
            "Epoch:  100\n",
            "Loss:  0.33788842 #########   Accuracy:  0.8350737\n",
            "Epoch:  101\n",
            "Loss:  0.39049813 #########   Accuracy:  0.82670456\n",
            "Epoch:  102\n",
            "Loss:  0.4203503 #########   Accuracy:  0.8280866\n",
            "Epoch:  103\n",
            "Loss:  0.42770702 #########   Accuracy:  0.8389128\n",
            "Epoch:  104\n",
            "Loss:  0.41551167 #########   Accuracy:  0.8313498\n",
            "Epoch:  105\n",
            "Loss:  0.38978782 #########   Accuracy:  0.8318489\n",
            "Epoch:  106\n",
            "Loss:  0.36215326 #########   Accuracy:  0.83584154\n",
            "Epoch:  107\n",
            "Loss:  0.33597198 #########   Accuracy:  0.841293\n",
            "Epoch:  108\n",
            "Loss:  0.3345373 #########   Accuracy:  0.8443642\n",
            "Epoch:  109\n",
            "Loss:  0.36460918 #########   Accuracy:  0.82969904\n",
            "Epoch:  110\n",
            "Loss:  0.33275065 #########   Accuracy:  0.84348124\n",
            "Epoch:  111\n",
            "Loss:  0.32769874 #########   Accuracy:  0.8420992\n",
            "Epoch:  112\n",
            "Loss:  0.33318248 #########   Accuracy:  0.83772266\n",
            "Epoch:  113\n",
            "Loss:  0.33362442 #########   Accuracy:  0.83737713\n",
            "Epoch:  114\n",
            "Loss:  0.33166316 #########   Accuracy:  0.83898956\n",
            "Epoch:  115\n",
            "Loss:  0.32950062 #########   Accuracy:  0.8420224\n",
            "Epoch:  116\n",
            "Loss:  0.32733405 #########   Accuracy:  0.8427135\n",
            "Epoch:  117\n",
            "Loss:  0.32583615 #########   Accuracy:  0.84597665\n",
            "Epoch:  118\n",
            "Loss:  0.32498288 #########   Accuracy:  0.8452088\n",
            "Epoch:  119\n",
            "Loss:  0.3255681 #########   Accuracy:  0.84728193\n",
            "Epoch:  120\n",
            "Loss:  0.32677904 #########   Accuracy:  0.8466677\n",
            "Epoch:  121\n",
            "Loss:  0.32705674 #########   Accuracy:  0.84643734\n",
            "Epoch:  122\n",
            "Loss:  0.32614008 #########   Accuracy:  0.847781\n",
            "Epoch:  123\n",
            "Loss:  0.32505366 #########   Accuracy:  0.84874076\n",
            "Epoch:  124\n",
            "Loss:  0.3245857 #########   Accuracy:  0.84762746\n",
            "Epoch:  125\n",
            "Loss:  0.32472646 #########   Accuracy:  0.8475507\n",
            "Epoch:  126\n",
            "Loss:  0.32491872 #########   Accuracy:  0.8471284\n",
            "Epoch:  127\n",
            "Loss:  0.32481143 #########   Accuracy:  0.84685963\n",
            "Epoch:  128\n",
            "Loss:  0.32438004 #########   Accuracy:  0.8463222\n",
            "Epoch:  129\n",
            "Loss:  0.32375708 #########   Accuracy:  0.84682125\n",
            "Epoch:  130\n",
            "Loss:  0.32334056 #########   Accuracy:  0.84678286\n",
            "Epoch:  131\n",
            "Loss:  0.32320964 #########   Accuracy:  0.8467061\n",
            "Epoch:  132\n",
            "Loss:  0.32275537 #########   Accuracy:  0.84682125\n",
            "Epoch:  133\n",
            "Loss:  0.32229456 #########   Accuracy:  0.84682125\n",
            "Epoch:  134\n",
            "Loss:  0.32210726 #########   Accuracy:  0.84636056\n",
            "Epoch:  135\n",
            "Loss:  0.3221794 #########   Accuracy:  0.84659094\n",
            "Epoch:  136\n",
            "Loss:  0.32233018 #########   Accuracy:  0.84659094\n",
            "Epoch:  137\n",
            "Loss:  0.322362 #########   Accuracy:  0.8461302\n",
            "Epoch:  138\n",
            "Loss:  0.3218593 #########   Accuracy:  0.84682125\n",
            "Epoch:  139\n",
            "Loss:  0.32108256 #########   Accuracy:  0.84682125\n",
            "Epoch:  140\n",
            "Loss:  0.32083604 #########   Accuracy:  0.846898\n",
            "Epoch:  141\n",
            "Loss:  0.32094896 #########   Accuracy:  0.8483185\n",
            "Epoch:  142\n",
            "Loss:  0.32170856 #########   Accuracy:  0.8475507\n",
            "Epoch:  143\n",
            "Loss:  0.4552313 #########   Accuracy:  0.811617\n",
            "Epoch:  144\n",
            "Loss:  0.3395444 #########   Accuracy:  0.83710843\n",
            "Epoch:  145\n",
            "Loss:  0.36187106 #########   Accuracy:  0.83115786\n",
            "Epoch:  146\n",
            "Loss:  0.39405885 #########   Accuracy:  0.8308891\n",
            "Epoch:  147\n",
            "Loss:  0.4293396 #########   Accuracy:  0.8300061\n",
            "Epoch:  148\n",
            "Loss:  0.44174674 #########   Accuracy:  0.82904637\n",
            "Epoch:  149\n",
            "Loss:  0.4261742 #########   Accuracy:  0.8311195\n",
            "Epoch:  150\n",
            "Loss:  0.39567184 #########   Accuracy:  0.8326167\n",
            "Epoch:  151\n",
            "Loss:  0.36025184 #########   Accuracy:  0.8333461\n",
            "Epoch:  152\n",
            "Loss:  0.33413216 #########   Accuracy:  0.8401029\n",
            "Epoch:  153\n",
            "Loss:  0.32337025 #########   Accuracy:  0.8482033\n",
            "Epoch:  154\n",
            "Loss:  0.3360196 #########   Accuracy:  0.8330006\n",
            "Epoch:  155\n",
            "Loss:  0.3445407 #########   Accuracy:  0.81791306\n",
            "Epoch:  156\n",
            "Loss:  0.3294883 #########   Accuracy:  0.84067875\n",
            "Epoch:  157\n",
            "Loss:  0.32127556 #########   Accuracy:  0.8474355\n",
            "Epoch:  158\n",
            "Loss:  0.32065517 #########   Accuracy:  0.84263664\n",
            "Epoch:  159\n",
            "Loss:  0.31946614 #########   Accuracy:  0.8462454\n",
            "Epoch:  160\n",
            "Loss:  0.32141945 #########   Accuracy:  0.8496622\n",
            "Epoch:  161\n",
            "Loss:  0.3188391 #########   Accuracy:  0.85054517\n",
            "Epoch:  162\n",
            "Loss:  0.32070968 #########   Accuracy:  0.8544226\n",
            "Epoch:  163\n",
            "Loss:  0.31862804 #########   Accuracy:  0.854461\n",
            "Epoch:  164\n",
            "Loss:  0.3195539 #########   Accuracy:  0.85296375\n",
            "Epoch:  165\n",
            "Loss:  0.31811032 #########   Accuracy:  0.85181206\n",
            "Epoch:  166\n",
            "Loss:  0.31891063 #########   Accuracy:  0.8505068\n",
            "Epoch:  167\n",
            "Loss:  0.31907728 #########   Accuracy:  0.8535012\n",
            "Epoch:  168\n",
            "Loss:  0.31850472 #########   Accuracy:  0.8524263\n",
            "Epoch:  169\n",
            "Loss:  0.31876174 #########   Accuracy:  0.8551136\n",
            "Epoch:  170\n",
            "Loss:  0.3169157 #########   Accuracy:  0.85300213\n",
            "Epoch:  171\n",
            "Loss:  0.31692708 #########   Accuracy:  0.85342443\n",
            "Epoch:  172\n",
            "Loss:  0.3165346 #########   Accuracy:  0.85423064\n",
            "Epoch:  173\n",
            "Loss:  0.31707606 #########   Accuracy:  0.8572251\n",
            "Epoch:  174\n",
            "Loss:  0.31710202 #########   Accuracy:  0.8572251\n",
            "Epoch:  175\n",
            "Loss:  0.31735167 #########   Accuracy:  0.856227\n",
            "Epoch:  176\n",
            "Loss:  0.31488642 #########   Accuracy:  0.8548833\n",
            "Epoch:  177\n",
            "Loss:  0.3168687 #########   Accuracy:  0.8581849\n",
            "Epoch:  178\n",
            "Loss:  0.31554887 #########   Accuracy:  0.85653406\n",
            "Epoch:  179\n",
            "Loss:  0.31767967 #########   Accuracy:  0.8495086\n",
            "Epoch:  180\n",
            "Loss:  0.3176287 #########   Accuracy:  0.8523879\n",
            "Epoch:  181\n",
            "Loss:  0.31507224 #########   Accuracy:  0.85653406\n",
            "Epoch:  182\n",
            "Loss:  0.31361428 #########   Accuracy:  0.85753226\n",
            "Epoch:  183\n",
            "Loss:  0.31469968 #########   Accuracy:  0.8563805\n",
            "Epoch:  184\n",
            "Loss:  0.3148844 #########   Accuracy:  0.8569948\n",
            "Epoch:  185\n",
            "Loss:  0.31510785 #########   Accuracy:  0.85054517\n",
            "Epoch:  186\n",
            "Loss:  0.3176145 #########   Accuracy:  0.8486256\n",
            "Epoch:  187\n",
            "Loss:  0.31727335 #########   Accuracy:  0.84709\n",
            "Epoch:  188\n",
            "Loss:  0.31550625 #########   Accuracy:  0.8467445\n",
            "Epoch:  189\n",
            "Loss:  0.3145726 #########   Accuracy:  0.85545915\n",
            "Epoch:  190\n",
            "Loss:  0.31629926 #########   Accuracy:  0.8449401\n",
            "Epoch:  191\n",
            "Loss:  0.3158447 #########   Accuracy:  0.8470516\n",
            "Epoch:  192\n",
            "Loss:  0.3140109 #########   Accuracy:  0.856227\n",
            "Epoch:  193\n",
            "Loss:  0.31393012 #########   Accuracy:  0.85085225\n",
            "Epoch:  194\n",
            "Loss:  0.31360623 #########   Accuracy:  0.8483953\n",
            "Epoch:  195\n",
            "Loss:  0.31284738 #########   Accuracy:  0.84977734\n",
            "Epoch:  196\n",
            "Loss:  0.3126784 #########   Accuracy:  0.85085225\n",
            "Epoch:  197\n",
            "Loss:  0.31199607 #########   Accuracy:  0.852887\n",
            "Epoch:  198\n",
            "Loss:  0.31276715 #########   Accuracy:  0.8587608\n",
            "Epoch:  199\n",
            "Loss:  0.31472972 #########   Accuracy:  0.8460534\n",
            "Epoch:  200\n",
            "Loss:  0.31773463 #########   Accuracy:  0.8443642\n",
            "Epoch:  201\n",
            "Loss:  0.32128993 #########   Accuracy:  0.8412546\n",
            "Epoch:  202\n",
            "Loss:  0.3150837 #########   Accuracy:  0.85822326\n",
            "Epoch:  203\n",
            "Loss:  0.3115892 #########   Accuracy:  0.8527718\n",
            "Epoch:  204\n",
            "Loss:  0.31613925 #########   Accuracy:  0.846898\n",
            "Epoch:  205\n",
            "Loss:  0.31517273 #########   Accuracy:  0.8479346\n",
            "Epoch:  206\n",
            "Loss:  0.3109744 #########   Accuracy:  0.85081387\n",
            "Epoch:  207\n",
            "Loss:  0.31235296 #########   Accuracy:  0.84889436\n",
            "Epoch:  208\n",
            "Loss:  0.313794 #########   Accuracy:  0.855344\n",
            "Epoch:  209\n",
            "Loss:  0.31101748 #########   Accuracy:  0.85757065\n",
            "Epoch:  210\n",
            "Loss:  0.31031165 #########   Accuracy:  0.85964376\n",
            "Epoch:  211\n",
            "Loss:  0.3121692 #########   Accuracy:  0.8563805\n",
            "Epoch:  212\n",
            "Loss:  0.3113952 #########   Accuracy:  0.85211915\n",
            "Epoch:  213\n",
            "Loss:  0.30926058 #########   Accuracy:  0.85296375\n",
            "Epoch:  214\n",
            "Loss:  0.3104651 #########   Accuracy:  0.8515817\n",
            "Epoch:  215\n",
            "Loss:  0.3116034 #########   Accuracy:  0.84935504\n",
            "Epoch:  216\n",
            "Loss:  0.30924335 #########   Accuracy:  0.8531941\n",
            "Epoch:  217\n",
            "Loss:  0.30885783 #########   Accuracy:  0.8544226\n",
            "Epoch:  218\n",
            "Loss:  0.31025535 #########   Accuracy:  0.85342443\n",
            "Epoch:  219\n",
            "Loss:  0.3096585 #########   Accuracy:  0.8570332\n",
            "Epoch:  220\n",
            "Loss:  0.3082012 #########   Accuracy:  0.8566109\n",
            "Epoch:  221\n",
            "Loss:  0.30802628 #########   Accuracy:  0.8564189\n",
            "Epoch:  222\n",
            "Loss:  0.30857784 #########   Accuracy:  0.8563805\n",
            "Epoch:  223\n",
            "Loss:  0.30876946 #########   Accuracy:  0.854461\n",
            "Epoch:  224\n",
            "Loss:  0.30757937 #########   Accuracy:  0.8548833\n",
            "Epoch:  225\n",
            "Loss:  0.30712545 #########   Accuracy:  0.8544994\n",
            "Epoch:  226\n",
            "Loss:  0.30745518 #########   Accuracy:  0.85296375\n",
            "Epoch:  227\n",
            "Loss:  0.30784693 #########   Accuracy:  0.854269\n",
            "Epoch:  228\n",
            "Loss:  0.3078697 #########   Accuracy:  0.8540387\n",
            "Epoch:  229\n",
            "Loss:  0.3071507 #########   Accuracy:  0.8568412\n",
            "Epoch:  230\n",
            "Loss:  0.30637318 #########   Accuracy:  0.856035\n",
            "Epoch:  231\n",
            "Loss:  0.30598128 #########   Accuracy:  0.85822326\n",
            "Epoch:  232\n",
            "Loss:  0.30618393 #########   Accuracy:  0.85910624\n",
            "Epoch:  233\n",
            "Loss:  0.30657598 #########   Accuracy:  0.85653406\n",
            "Epoch:  234\n",
            "Loss:  0.30660254 #########   Accuracy:  0.856918\n",
            "Epoch:  235\n",
            "Loss:  0.3066752 #########   Accuracy:  0.85549754\n",
            "Epoch:  236\n",
            "Loss:  0.30621544 #########   Accuracy:  0.8592982\n",
            "Epoch:  237\n",
            "Loss:  0.30557027 #########   Accuracy:  0.85741705\n",
            "Epoch:  238\n",
            "Loss:  0.3049068 #########   Accuracy:  0.85757065\n",
            "Epoch:  239\n",
            "Loss:  0.30459678 #########   Accuracy:  0.8574939\n",
            "Epoch:  240\n",
            "Loss:  0.30447406 #########   Accuracy:  0.8576858\n",
            "Epoch:  241\n",
            "Loss:  0.30464044 #########   Accuracy:  0.85952854\n",
            "Epoch:  242\n",
            "Loss:  0.30539483 #########   Accuracy:  0.8576474\n",
            "Epoch:  243\n",
            "Loss:  0.30632818 #########   Accuracy:  0.85695636\n",
            "Epoch:  244\n",
            "Loss:  0.30893543 #########   Accuracy:  0.8548065\n",
            "Epoch:  245\n",
            "Loss:  0.30919802 #########   Accuracy:  0.85653406\n",
            "Epoch:  246\n",
            "Loss:  0.3073927 #########   Accuracy:  0.85545915\n",
            "Epoch:  247\n",
            "Loss:  0.3038679 #########   Accuracy:  0.8598357\n",
            "Epoch:  248\n",
            "Loss:  0.30512917 #########   Accuracy:  0.85995084\n",
            "Epoch:  249\n",
            "Loss:  0.30892697 #########   Accuracy:  0.85591984\n",
            "Epoch:  250\n",
            "Loss:  0.30860025 #########   Accuracy:  0.8568412\n",
            "Epoch:  251\n",
            "Loss:  0.30737594 #########   Accuracy:  0.8566109\n",
            "Epoch:  252\n",
            "Loss:  0.30290577 #########   Accuracy:  0.8638667\n",
            "Epoch:  253\n",
            "Loss:  0.30364218 #########   Accuracy:  0.863598\n",
            "Epoch:  254\n",
            "Loss:  0.30631542 #########   Accuracy:  0.85760903\n",
            "Epoch:  255\n",
            "Loss:  0.30396307 #########   Accuracy:  0.8634444\n",
            "Epoch:  256\n",
            "Loss:  0.3014743 #########   Accuracy:  0.86283016\n",
            "Epoch:  257\n",
            "Loss:  0.3017537 #########   Accuracy:  0.86052674\n",
            "Epoch:  258\n",
            "Loss:  0.30338407 #########   Accuracy:  0.8617168\n",
            "Epoch:  259\n",
            "Loss:  0.30459982 #########   Accuracy:  0.86029637\n",
            "Epoch:  260\n",
            "Loss:  0.30182785 #########   Accuracy:  0.86198556\n",
            "Epoch:  261\n",
            "Loss:  0.30012056 #########   Accuracy:  0.86325246\n",
            "Epoch:  262\n",
            "Loss:  0.30105087 #########   Accuracy:  0.859375\n",
            "Epoch:  263\n",
            "Loss:  0.30554307 #########   Accuracy:  0.8513514\n",
            "Epoch:  264\n",
            "Loss:  0.30843195 #########   Accuracy:  0.8477426\n",
            "Epoch:  265\n",
            "Loss:  0.30885208 #########   Accuracy:  0.84808815\n",
            "Epoch:  266\n",
            "Loss:  0.3070802 #########   Accuracy:  0.84804976\n",
            "Epoch:  267\n",
            "Loss:  0.30615053 #########   Accuracy:  0.84851044\n",
            "Epoch:  268\n",
            "Loss:  0.30690905 #########   Accuracy:  0.8530789\n",
            "Epoch:  269\n",
            "Loss:  0.30749133 #########   Accuracy:  0.85461456\n",
            "Epoch:  270\n",
            "Loss:  0.30448335 #########   Accuracy:  0.8622927\n",
            "Epoch:  271\n",
            "Loss:  0.3026715 #########   Accuracy:  0.8553056\n",
            "Epoch:  272\n",
            "Loss:  0.3018156 #########   Accuracy:  0.85626537\n",
            "Epoch:  273\n",
            "Loss:  0.30083984 #########   Accuracy:  0.85588145\n",
            "Epoch:  274\n",
            "Loss:  0.301822 #########   Accuracy:  0.864289\n",
            "Epoch:  275\n",
            "Loss:  0.30210292 #########   Accuracy:  0.8655559\n",
            "Epoch:  276\n",
            "Loss:  0.30603042 #########   Accuracy:  0.86244625\n",
            "Epoch:  277\n",
            "Loss:  0.3063323 #########   Accuracy:  0.8565725\n",
            "Epoch:  278\n",
            "Loss:  0.30771533 #########   Accuracy:  0.8608338\n",
            "Epoch:  279\n",
            "Loss:  0.30099773 #########   Accuracy:  0.86789775\n",
            "Epoch:  280\n",
            "Loss:  0.301642 #########   Accuracy:  0.8650184\n",
            "Epoch:  281\n",
            "Loss:  0.30523452 #########   Accuracy:  0.86160165\n",
            "Epoch:  282\n",
            "Loss:  0.30054745 #########   Accuracy:  0.86812806\n",
            "Epoch:  283\n",
            "Loss:  0.29757774 #########   Accuracy:  0.86574787\n",
            "Epoch:  284\n",
            "Loss:  0.2991052 #########   Accuracy:  0.8623695\n",
            "Epoch:  285\n",
            "Loss:  0.30183515 #########   Accuracy:  0.86137134\n",
            "Epoch:  286\n",
            "Loss:  0.30422053 #########   Accuracy:  0.8622927\n",
            "Epoch:  287\n",
            "Loss:  0.29823887 #########   Accuracy:  0.8633676\n",
            "Epoch:  288\n",
            "Loss:  0.29748872 #########   Accuracy:  0.8684736\n",
            "Epoch:  289\n",
            "Loss:  0.30102637 #########   Accuracy:  0.860258\n",
            "Epoch:  290\n",
            "Loss:  0.30245367 #########   Accuracy:  0.85311735\n",
            "Epoch:  291\n",
            "Loss:  0.2993852 #########   Accuracy:  0.8551904\n",
            "Epoch:  292\n",
            "Loss:  0.30038106 #########   Accuracy:  0.8540387\n",
            "Epoch:  293\n",
            "Loss:  0.3029614 #########   Accuracy:  0.8539235\n",
            "Epoch:  294\n",
            "Loss:  0.30195603 #########   Accuracy:  0.85423064\n",
            "Epoch:  295\n",
            "Loss:  0.29812258 #########   Accuracy:  0.8564573\n",
            "Epoch:  296\n",
            "Loss:  0.29707682 #########   Accuracy:  0.8576474\n",
            "Epoch:  297\n",
            "Loss:  0.29780197 #########   Accuracy:  0.86498004\n",
            "Epoch:  298\n",
            "Loss:  0.29778242 #########   Accuracy:  0.856918\n",
            "Epoch:  299\n",
            "Loss:  0.29739958 #########   Accuracy:  0.85591984\n",
            "Epoch:  300\n",
            "Loss:  0.29663086 #########   Accuracy:  0.85676444\n",
            "Epoch:  301\n",
            "Loss:  0.29620153 #########   Accuracy:  0.8564189\n",
            "Epoch:  302\n",
            "Loss:  0.2959103 #########   Accuracy:  0.85672605\n",
            "Epoch:  303\n",
            "Loss:  0.29584435 #########   Accuracy:  0.8572251\n",
            "Epoch:  304\n",
            "Loss:  0.29668346 #########   Accuracy:  0.8592982\n",
            "Epoch:  305\n",
            "Loss:  0.29794657 #########   Accuracy:  0.86778253\n",
            "Epoch:  306\n",
            "Loss:  0.30305484 #########   Accuracy:  0.8577242\n",
            "Epoch:  307\n",
            "Loss:  0.30173254 #########   Accuracy:  0.8601812\n",
            "Epoch:  308\n",
            "Loss:  0.29954314 #########   Accuracy:  0.8596054\n",
            "Epoch:  309\n",
            "Loss:  0.29500517 #########   Accuracy:  0.8680897\n",
            "Epoch:  310\n",
            "Loss:  0.298435 #########   Accuracy:  0.8552288\n",
            "Epoch:  311\n",
            "Loss:  0.2986217 #########   Accuracy:  0.85741705\n",
            "Epoch:  312\n",
            "Loss:  0.29866335 #########   Accuracy:  0.85841525\n",
            "Epoch:  313\n",
            "Loss:  0.2932998 #########   Accuracy:  0.8683968\n",
            "Epoch:  314\n",
            "Loss:  0.29398823 #########   Accuracy:  0.8568412\n",
            "Epoch:  315\n",
            "Loss:  0.29245144 #########   Accuracy:  0.86521035\n",
            "Epoch:  316\n",
            "Loss:  0.29239422 #########   Accuracy:  0.8650184\n",
            "Epoch:  317\n",
            "Loss:  0.29349524 #########   Accuracy:  0.8663621\n",
            "Epoch:  318\n",
            "Loss:  0.2944129 #########   Accuracy:  0.86329085\n",
            "Epoch:  319\n",
            "Loss:  0.29646072 #########   Accuracy:  0.8638283\n",
            "Epoch:  320\n",
            "Loss:  0.30022606 #########   Accuracy:  0.86052674\n",
            "Epoch:  321\n",
            "Loss:  0.29293993 #########   Accuracy:  0.8684352\n",
            "Epoch:  322\n",
            "Loss:  0.29106936 #########   Accuracy:  0.8689343\n",
            "Epoch:  323\n",
            "Loss:  0.29361552 #########   Accuracy:  0.8643274\n",
            "Epoch:  324\n",
            "Loss:  0.29648608 #########   Accuracy:  0.8680513\n",
            "Epoch:  325\n",
            "Loss:  0.29652447 #########   Accuracy:  0.8652872\n",
            "Epoch:  326\n",
            "Loss:  0.28929716 #########   Accuracy:  0.87166\n",
            "Epoch:  327\n",
            "Loss:  0.29122835 #########   Accuracy:  0.86793613\n",
            "Epoch:  328\n",
            "Loss:  0.29477435 #########   Accuracy:  0.86409706\n",
            "Epoch:  329\n",
            "Loss:  0.28981748 #########   Accuracy:  0.8688191\n",
            "Epoch:  330\n",
            "Loss:  0.287952 #########   Accuracy:  0.8644042\n",
            "Epoch:  331\n",
            "Loss:  0.29047832 #########   Accuracy:  0.86751384\n",
            "Epoch:  332\n",
            "Loss:  0.2914807 #########   Accuracy:  0.86663085\n",
            "Epoch:  333\n",
            "Loss:  0.2945533 #########   Accuracy:  0.85968214\n",
            "Epoch:  334\n",
            "Loss:  0.28944328 #########   Accuracy:  0.86517197\n",
            "Epoch:  335\n",
            "Loss:  0.28829312 #########   Accuracy:  0.86958694\n",
            "Epoch:  336\n",
            "Loss:  0.29549432 #########   Accuracy:  0.85438424\n",
            "Epoch:  337\n",
            "Loss:  0.29327744 #########   Accuracy:  0.86037314\n",
            "Epoch:  338\n",
            "Loss:  0.2945234 #########   Accuracy:  0.8583001\n",
            "Epoch:  339\n",
            "Loss:  0.29535812 #########   Accuracy:  0.8587223\n",
            "Epoch:  340\n",
            "Loss:  0.29315695 #########   Accuracy:  0.85864556\n",
            "Epoch:  341\n",
            "Loss:  0.29056096 #########   Accuracy:  0.86517197\n",
            "Epoch:  342\n",
            "Loss:  0.29132658 #########   Accuracy:  0.8604499\n",
            "Epoch:  343\n",
            "Loss:  0.2916046 #########   Accuracy:  0.8585304\n",
            "Epoch:  344\n",
            "Loss:  0.29578584 #########   Accuracy:  0.8583001\n",
            "Epoch:  345\n",
            "Loss:  0.29786366 #########   Accuracy:  0.85737866\n",
            "Epoch:  346\n",
            "Loss:  0.29753724 #########   Accuracy:  0.8569948\n",
            "Epoch:  347\n",
            "Loss:  0.293723 #########   Accuracy:  0.8583385\n",
            "Epoch:  348\n",
            "Loss:  0.29234105 #########   Accuracy:  0.85864556\n",
            "Epoch:  349\n",
            "Loss:  0.2933651 #########   Accuracy:  0.8579162\n",
            "Epoch:  350\n",
            "Loss:  0.29134214 #########   Accuracy:  0.8580697\n",
            "Epoch:  351\n",
            "Loss:  0.2902921 #########   Accuracy:  0.8625614\n",
            "Epoch:  352\n",
            "Loss:  0.2887141 #########   Accuracy:  0.8703163\n",
            "Epoch:  353\n",
            "Loss:  0.2898202 #########   Accuracy:  0.8675906\n",
            "Epoch:  354\n",
            "Loss:  0.29103503 #########   Accuracy:  0.85783935\n",
            "Epoch:  355\n",
            "Loss:  0.29281807 #########   Accuracy:  0.85668766\n",
            "Epoch:  356\n",
            "Loss:  0.29449886 #########   Accuracy:  0.85342443\n",
            "Epoch:  357\n",
            "Loss:  0.2913085 #########   Accuracy:  0.85588145\n",
            "Epoch:  358\n",
            "Loss:  0.28909895 #########   Accuracy:  0.8566493\n",
            "Epoch:  359\n",
            "Loss:  0.288879 #########   Accuracy:  0.8564573\n",
            "Epoch:  360\n",
            "Loss:  0.2898702 #########   Accuracy:  0.8572251\n",
            "Epoch:  361\n",
            "Loss:  0.29133976 #########   Accuracy:  0.8570716\n",
            "Epoch:  362\n",
            "Loss:  0.29049885 #########   Accuracy:  0.8583385\n",
            "Epoch:  363\n",
            "Loss:  0.29024732 #########   Accuracy:  0.85868394\n",
            "Epoch:  364\n",
            "Loss:  0.2878155 #########   Accuracy:  0.8664005\n",
            "Epoch:  365\n",
            "Loss:  0.28675568 #########   Accuracy:  0.87357956\n",
            "Epoch:  366\n",
            "Loss:  0.2869965 #########   Accuracy:  0.87273496\n",
            "Epoch:  367\n",
            "Loss:  0.28825948 #########   Accuracy:  0.86743706\n",
            "Epoch:  368\n",
            "Loss:  0.29169542 #########   Accuracy:  0.86202395\n",
            "Epoch:  369\n",
            "Loss:  0.2911684 #########   Accuracy:  0.860258\n",
            "Epoch:  370\n",
            "Loss:  0.29081935 #########   Accuracy:  0.8580697\n",
            "Epoch:  371\n",
            "Loss:  0.29014823 #########   Accuracy:  0.8614097\n",
            "Epoch:  372\n",
            "Loss:  0.29008058 #########   Accuracy:  0.8598357\n",
            "Epoch:  373\n",
            "Loss:  0.29451203 #########   Accuracy:  0.8568028\n",
            "Epoch:  374\n",
            "Loss:  0.2905433 #########   Accuracy:  0.8618704\n",
            "Epoch:  375\n",
            "Loss:  0.2909614 #########   Accuracy:  0.8648265\n",
            "Epoch:  376\n",
            "Loss:  0.28544492 #########   Accuracy:  0.8705083\n",
            "Epoch:  377\n",
            "Loss:  0.2874351 #########   Accuracy:  0.872543\n",
            "Epoch:  378\n",
            "Loss:  0.29095373 #########   Accuracy:  0.87104577\n",
            "Epoch:  379\n",
            "Loss:  0.2947643 #########   Accuracy:  0.86855036\n",
            "Epoch:  380\n",
            "Loss:  0.29653654 #########   Accuracy:  0.86601657\n",
            "Epoch:  381\n",
            "Loss:  0.28602028 #########   Accuracy:  0.8739251\n",
            "Epoch:  382\n",
            "Loss:  0.29573026 #########   Accuracy:  0.8655175\n",
            "Epoch:  383\n",
            "Loss:  0.2976855 #########   Accuracy:  0.86091065\n",
            "Epoch:  384\n",
            "Loss:  0.28480598 #########   Accuracy:  0.8728885\n",
            "Epoch:  385\n",
            "Loss:  0.29901856 #########   Accuracy:  0.862523\n",
            "Epoch:  386\n",
            "Loss:  0.30206397 #########   Accuracy:  0.85757065\n",
            "Epoch:  387\n",
            "Loss:  0.2885938 #########   Accuracy:  0.86540234\n",
            "Epoch:  388\n",
            "Loss:  0.30256343 #########   Accuracy:  0.8583385\n",
            "Epoch:  389\n",
            "Loss:  0.28881207 #########   Accuracy:  0.86774415\n",
            "Epoch:  390\n",
            "Loss:  0.29468223 #########   Accuracy:  0.8644426\n",
            "Epoch:  391\n",
            "Loss:  0.28641877 #########   Accuracy:  0.8729653\n",
            "Epoch:  392\n",
            "Loss:  0.28919584 #########   Accuracy:  0.8722743\n",
            "Epoch:  393\n",
            "Loss:  0.29474974 #########   Accuracy:  0.86709154\n",
            "Epoch:  394\n",
            "Loss:  0.2838161 #########   Accuracy:  0.87388664\n",
            "Epoch:  395\n",
            "Loss:  0.29276273 #########   Accuracy:  0.86686116\n",
            "Epoch:  396\n",
            "Loss:  0.28526336 #########   Accuracy:  0.8729269\n",
            "Epoch:  397\n",
            "Loss:  0.2866633 #########   Accuracy:  0.87104577\n",
            "Epoch:  398\n",
            "Loss:  0.2871731 #########   Accuracy:  0.8712377\n",
            "Epoch:  399\n",
            "Loss:  0.28139615 #########   Accuracy:  0.87442416\n",
            "Epoch:  400\n",
            "Loss:  0.28422716 #########   Accuracy:  0.8726198\n",
            "Epoch:  401\n",
            "Loss:  0.28151676 #########   Accuracy:  0.87446254\n",
            "Epoch:  402\n",
            "Loss:  0.2823653 #########   Accuracy:  0.87427056\n",
            "Epoch:  403\n",
            "Loss:  0.28160572 #########   Accuracy:  0.8739251\n",
            "Epoch:  404\n",
            "Loss:  0.2798538 #########   Accuracy:  0.8746545\n",
            "Epoch:  405\n",
            "Loss:  0.27976817 #########   Accuracy:  0.8746545\n",
            "Epoch:  406\n",
            "Loss:  0.27956873 #########   Accuracy:  0.8748081\n",
            "Epoch:  407\n",
            "Loss:  0.27834564 #########   Accuracy:  0.87469286\n",
            "Epoch:  408\n",
            "Loss:  0.27795684 #########   Accuracy:  0.87615174\n",
            "Epoch:  409\n",
            "Loss:  0.27713624 #########   Accuracy:  0.8757678\n",
            "Epoch:  410\n",
            "Loss:  0.27677476 #########   Accuracy:  0.87653565\n",
            "Epoch:  411\n",
            "Loss:  0.27576256 #########   Accuracy:  0.8758446\n",
            "Epoch:  412\n",
            "Loss:  0.27487904 #########   Accuracy:  0.8751919\n",
            "Epoch:  413\n",
            "Loss:  0.27443814 #########   Accuracy:  0.8762285\n",
            "Epoch:  414\n",
            "Loss:  0.27460408 #########   Accuracy:  0.8762669\n",
            "Epoch:  415\n",
            "Loss:  0.2735429 #########   Accuracy:  0.87638205\n",
            "Epoch:  416\n",
            "Loss:  0.27330518 #########   Accuracy:  0.87530714\n",
            "Epoch:  417\n",
            "Loss:  0.27312067 #########   Accuracy:  0.8762669\n",
            "Epoch:  418\n",
            "Loss:  0.27390134 #########   Accuracy:  0.8736947\n",
            "Epoch:  419\n",
            "Loss:  0.27703923 #########   Accuracy:  0.8704699\n",
            "Epoch:  420\n",
            "Loss:  0.27284026 #########   Accuracy:  0.87469286\n",
            "Epoch:  421\n",
            "Loss:  0.29078844 #########   Accuracy:  0.8621775\n",
            "Epoch:  422\n",
            "Loss:  0.29217336 #########   Accuracy:  0.863406\n",
            "Epoch:  423\n",
            "Loss:  0.31331652 #########   Accuracy:  0.8630221\n",
            "Epoch:  424\n",
            "Loss:  0.31975883 #########   Accuracy:  0.8634828\n",
            "Epoch:  425\n",
            "Loss:  0.31197214 #########   Accuracy:  0.8632141\n",
            "Epoch:  426\n",
            "Loss:  0.29556105 #########   Accuracy:  0.8637899\n",
            "Epoch:  427\n",
            "Loss:  0.2798664 #########   Accuracy:  0.8638283\n",
            "Epoch:  428\n",
            "Loss:  0.27362278 #########   Accuracy:  0.8735412\n",
            "Epoch:  429\n",
            "Loss:  0.32332557 #########   Accuracy:  0.8617168\n",
            "Epoch:  430\n",
            "Loss:  0.27755922 #########   Accuracy:  0.8669764\n",
            "Epoch:  431\n",
            "Loss:  0.287787 #########   Accuracy:  0.86367476\n",
            "Epoch:  432\n",
            "Loss:  0.29963782 #########   Accuracy:  0.8611026\n",
            "Epoch:  433\n",
            "Loss:  0.30375847 #########   Accuracy:  0.8616784\n",
            "Epoch:  434\n",
            "Loss:  0.3025827 #########   Accuracy:  0.8621391\n",
            "Epoch:  435\n",
            "Loss:  0.29626954 #########   Accuracy:  0.8631757\n",
            "Epoch:  436\n",
            "Loss:  0.29008713 #########   Accuracy:  0.8642122\n",
            "Epoch:  437\n",
            "Loss:  0.28615928 #########   Accuracy:  0.8626766\n",
            "Epoch:  438\n",
            "Loss:  0.28511617 #########   Accuracy:  0.8602196\n",
            "Epoch:  439\n",
            "Loss:  0.29297212 #########   Accuracy:  0.8638667\n",
            "Epoch:  440\n",
            "Loss:  0.2836216 #########   Accuracy:  0.85922146\n",
            "Epoch:  441\n",
            "Loss:  0.28621596 #########   Accuracy:  0.8595669\n",
            "Epoch:  442\n",
            "Loss:  0.29362735 #########   Accuracy:  0.8519272\n",
            "Epoch:  443\n",
            "Loss:  0.28300554 #########   Accuracy:  0.86164004\n",
            "Epoch:  444\n",
            "Loss:  0.287989 #########   Accuracy:  0.85780096\n",
            "Epoch:  445\n",
            "Loss:  0.29872602 #########   Accuracy:  0.85165846\n",
            "Epoch:  446\n",
            "Loss:  0.284225 #########   Accuracy:  0.8608722\n",
            "Epoch:  447\n",
            "Loss:  0.289338 #########   Accuracy:  0.8564957\n",
            "Epoch:  448\n",
            "Loss:  0.2958788 #########   Accuracy:  0.8519656\n",
            "Epoch:  449\n",
            "Loss:  0.28273463 #########   Accuracy:  0.8600277\n",
            "Epoch:  450\n",
            "Loss:  0.2969033 #########   Accuracy:  0.8506987\n",
            "Epoch:  451\n",
            "Loss:  0.29221454 #########   Accuracy:  0.8548833\n",
            "Epoch:  452\n",
            "Loss:  0.28710487 #########   Accuracy:  0.86789775\n",
            "Epoch:  453\n",
            "Loss:  0.28996614 #########   Accuracy:  0.8680513\n",
            "Epoch:  454\n",
            "Loss:  0.2812382 #########   Accuracy:  0.86855036\n",
            "Epoch:  455\n",
            "Loss:  0.2889445 #########   Accuracy:  0.8601812\n",
            "Epoch:  456\n",
            "Loss:  0.27961552 #########   Accuracy:  0.86582464\n",
            "Epoch:  457\n",
            "Loss:  0.28850582 #########   Accuracy:  0.85757065\n",
            "Epoch:  458\n",
            "Loss:  0.29269627 #########   Accuracy:  0.8568796\n",
            "Epoch:  459\n",
            "Loss:  0.28493577 #########   Accuracy:  0.8606419\n",
            "Epoch:  460\n",
            "Loss:  0.28977895 #########   Accuracy:  0.8587223\n",
            "Epoch:  461\n",
            "Loss:  0.28316495 #########   Accuracy:  0.86198556\n",
            "Epoch:  462\n",
            "Loss:  0.28851607 #########   Accuracy:  0.85780096\n",
            "Epoch:  463\n",
            "Loss:  0.2821774 #########   Accuracy:  0.8623695\n",
            "Epoch:  464\n",
            "Loss:  0.2865078 #########   Accuracy:  0.85883754\n",
            "Epoch:  465\n",
            "Loss:  0.2810455 #########   Accuracy:  0.8634444\n",
            "Epoch:  466\n",
            "Loss:  0.2831613 #########   Accuracy:  0.8619472\n",
            "Epoch:  467\n",
            "Loss:  0.2842781 #########   Accuracy:  0.8616784\n",
            "Epoch:  468\n",
            "Loss:  0.2788066 #########   Accuracy:  0.86643887\n",
            "Epoch:  469\n",
            "Loss:  0.28048372 #########   Accuracy:  0.86728346\n",
            "Epoch:  470\n",
            "Loss:  0.2790308 #########   Accuracy:  0.8695101\n",
            "Epoch:  471\n",
            "Loss:  0.2785565 #########   Accuracy:  0.8693566\n",
            "Epoch:  472\n",
            "Loss:  0.2790883 #########   Accuracy:  0.8705467\n",
            "Epoch:  473\n",
            "Loss:  0.27856216 #########   Accuracy:  0.87150645\n",
            "Epoch:  474\n",
            "Loss:  0.2779662 #########   Accuracy:  0.8708538\n",
            "Epoch:  475\n",
            "Loss:  0.2770365 #########   Accuracy:  0.87238944\n",
            "Epoch:  476\n",
            "Loss:  0.2762804 #########   Accuracy:  0.8741554\n",
            "Epoch:  477\n",
            "Loss:  0.27662522 #########   Accuracy:  0.87430894\n",
            "Epoch:  478\n",
            "Loss:  0.27637315 #########   Accuracy:  0.87427056\n",
            "Epoch:  479\n",
            "Loss:  0.2761565 #########   Accuracy:  0.8750768\n",
            "Epoch:  480\n",
            "Loss:  0.27525774 #########   Accuracy:  0.8758062\n",
            "Epoch:  481\n",
            "Loss:  0.27502182 #########   Accuracy:  0.8756527\n",
            "Epoch:  482\n",
            "Loss:  0.27541566 #########   Accuracy:  0.8754607\n",
            "Epoch:  483\n",
            "Loss:  0.2745521 #########   Accuracy:  0.87526876\n",
            "Epoch:  484\n",
            "Loss:  0.2743733 #########   Accuracy:  0.8746545\n",
            "Epoch:  485\n",
            "Loss:  0.2743163 #########   Accuracy:  0.8745777\n",
            "Epoch:  486\n",
            "Loss:  0.2738755 #########   Accuracy:  0.8738099\n",
            "Epoch:  487\n",
            "Loss:  0.2735033 #########   Accuracy:  0.87427056\n",
            "Epoch:  488\n",
            "Loss:  0.27319127 #########   Accuracy:  0.87400186\n",
            "Epoch:  489\n",
            "Loss:  0.27298784 #########   Accuracy:  0.8743858\n",
            "Epoch:  490\n",
            "Loss:  0.27263024 #########   Accuracy:  0.87357956\n",
            "Epoch:  491\n",
            "Loss:  0.27233347 #########   Accuracy:  0.87384826\n",
            "Epoch:  492\n",
            "Loss:  0.2722306 #########   Accuracy:  0.8741554\n",
            "Epoch:  493\n",
            "Loss:  0.2716896 #########   Accuracy:  0.87446254\n",
            "Epoch:  494\n",
            "Loss:  0.27136233 #########   Accuracy:  0.8750768\n",
            "Epoch:  495\n",
            "Loss:  0.27137536 #########   Accuracy:  0.87557584\n",
            "Epoch:  496\n",
            "Loss:  0.2710175 #########   Accuracy:  0.87592137\n",
            "Epoch:  497\n",
            "Loss:  0.2708488 #########   Accuracy:  0.8764972\n",
            "Epoch:  498\n",
            "Loss:  0.2706294 #########   Accuracy:  0.8763053\n",
            "Epoch:  499\n",
            "Loss:  0.2706517 #########   Accuracy:  0.87695795\n",
            "Epoch:  500\n",
            "Loss:  0.2714844 #########   Accuracy:  0.87557584\n",
            "Epoch:  501\n",
            "Loss:  0.27343535 #########   Accuracy:  0.8761901\n",
            "Epoch:  502\n",
            "Loss:  0.28022245 #########   Accuracy:  0.8736947\n",
            "Epoch:  503\n",
            "Loss:  0.2747127 #########   Accuracy:  0.8751919\n",
            "Epoch:  504\n",
            "Loss:  0.2701915 #########   Accuracy:  0.8764588\n",
            "Epoch:  505\n",
            "Loss:  0.27198905 #########   Accuracy:  0.8654791\n",
            "Epoch:  506\n",
            "Loss:  0.27549958 #########   Accuracy:  0.8748081\n",
            "Epoch:  507\n",
            "Loss:  0.2855387 #########   Accuracy:  0.86693794\n",
            "Epoch:  508\n",
            "Loss:  0.27710286 #########   Accuracy:  0.8693566\n",
            "Epoch:  509\n",
            "Loss:  0.27282152 #########   Accuracy:  0.8726966\n",
            "Epoch:  510\n",
            "Loss:  0.27666432 #########   Accuracy:  0.8696637\n",
            "Epoch:  511\n",
            "Loss:  0.27502605 #########   Accuracy:  0.8713913\n",
            "Epoch:  512\n",
            "Loss:  0.27213967 #########   Accuracy:  0.87342596\n",
            "Epoch:  513\n",
            "Loss:  0.27153587 #########   Accuracy:  0.8750384\n",
            "Epoch:  514\n",
            "Loss:  0.27243018 #########   Accuracy:  0.8754223\n",
            "Epoch:  515\n",
            "Loss:  0.27148795 #########   Accuracy:  0.8762669\n",
            "Epoch:  516\n",
            "Loss:  0.27168334 #########   Accuracy:  0.87695795\n",
            "Epoch:  517\n",
            "Loss:  0.27029595 #########   Accuracy:  0.8784168\n",
            "Epoch:  518\n",
            "Loss:  0.26920968 #########   Accuracy:  0.880106\n",
            "Epoch:  519\n",
            "Loss:  0.26910934 #########   Accuracy:  0.8811809\n",
            "Epoch:  520\n",
            "Loss:  0.26805052 #########   Accuracy:  0.8775722\n",
            "Epoch:  521\n",
            "Loss:  0.26610792 #########   Accuracy:  0.87427056\n",
            "Epoch:  522\n",
            "Loss:  0.2674519 #########   Accuracy:  0.8680897\n",
            "Epoch:  523\n",
            "Loss:  0.2652302 #########   Accuracy:  0.87611336\n",
            "Epoch:  524\n",
            "Loss:  0.26500598 #########   Accuracy:  0.879914\n",
            "Epoch:  525\n",
            "Loss:  0.2655656 #########   Accuracy:  0.8815264\n",
            "Epoch:  526\n",
            "Loss:  0.26532722 #########   Accuracy:  0.8804515\n",
            "Epoch:  527\n",
            "Loss:  0.2649746 #########   Accuracy:  0.8819871\n",
            "Epoch:  528\n",
            "Loss:  0.26557577 #########   Accuracy:  0.8787239\n",
            "Epoch:  529\n",
            "Loss:  0.26605415 #########   Accuracy:  0.87722665\n",
            "Epoch:  530\n",
            "Loss:  0.26530135 #########   Accuracy:  0.87768734\n",
            "Epoch:  531\n",
            "Loss:  0.26324105 #########   Accuracy:  0.8811041\n",
            "Epoch:  532\n",
            "Loss:  0.26279604 #########   Accuracy:  0.8790694\n",
            "Epoch:  533\n",
            "Loss:  0.26136827 #########   Accuracy:  0.8811809\n",
            "Epoch:  534\n",
            "Loss:  0.26051858 #########   Accuracy:  0.88141125\n",
            "Epoch:  535\n",
            "Loss:  0.26095486 #########   Accuracy:  0.88125765\n",
            "Epoch:  536\n",
            "Loss:  0.26177642 #########   Accuracy:  0.88037467\n",
            "Epoch:  537\n",
            "Loss:  0.26357025 #########   Accuracy:  0.8795301\n",
            "Epoch:  538\n",
            "Loss:  0.26895824 #########   Accuracy:  0.87473124\n",
            "Epoch:  539\n",
            "Loss:  0.26773667 #########   Accuracy:  0.8770347\n",
            "Epoch:  540\n",
            "Loss:  0.263432 #########   Accuracy:  0.8779945\n",
            "Epoch:  541\n",
            "Loss:  0.26172867 #########   Accuracy:  0.8791078\n",
            "Epoch:  542\n",
            "Loss:  0.2680772 #########   Accuracy:  0.87404025\n",
            "Epoch:  543\n",
            "Loss:  0.27100158 #########   Accuracy:  0.8737331\n",
            "Epoch:  544\n",
            "Loss:  0.25936267 #########   Accuracy:  0.88214064\n",
            "Epoch:  545\n",
            "Loss:  0.2647868 #########   Accuracy:  0.87784094\n",
            "Epoch:  546\n",
            "Loss:  0.27333626 #########   Accuracy:  0.8705851\n",
            "Epoch:  547\n",
            "Loss:  0.26268223 #########   Accuracy:  0.8790694\n",
            "Epoch:  548\n",
            "Loss:  0.268061 #########   Accuracy:  0.87611336\n",
            "Epoch:  549\n",
            "Loss:  0.2660445 #########   Accuracy:  0.8749616\n",
            "Epoch:  550\n",
            "Loss:  0.26457778 #########   Accuracy:  0.87718827\n",
            "Epoch:  551\n",
            "Loss:  0.26057026 #########   Accuracy:  0.88125765\n",
            "Epoch:  552\n",
            "Loss:  0.2690225 #########   Accuracy:  0.8739635\n",
            "Epoch:  553\n",
            "Loss:  0.2630088 #########   Accuracy:  0.8779945\n",
            "Epoch:  554\n",
            "Loss:  0.26316646 #########   Accuracy:  0.87718827\n",
            "Epoch:  555\n",
            "Loss:  0.2618275 #########   Accuracy:  0.87764895\n",
            "Epoch:  556\n",
            "Loss:  0.26019958 #########   Accuracy:  0.8808738\n",
            "Epoch:  557\n",
            "Loss:  0.26481485 #########   Accuracy:  0.8775722\n",
            "Epoch:  558\n",
            "Loss:  0.2588415 #########   Accuracy:  0.88018274\n",
            "Epoch:  559\n",
            "Loss:  0.26250565 #########   Accuracy:  0.87680435\n",
            "Epoch:  560\n",
            "Loss:  0.26194575 #########   Accuracy:  0.87699634\n",
            "Epoch:  561\n",
            "Loss:  0.25802422 #########   Accuracy:  0.88221747\n",
            "Epoch:  562\n",
            "Loss:  0.2608263 #########   Accuracy:  0.8797988\n",
            "Epoch:  563\n",
            "Loss:  0.25704086 #########   Accuracy:  0.88263977\n",
            "Epoch:  564\n",
            "Loss:  0.25749236 #########   Accuracy:  0.8811425\n",
            "Epoch:  565\n",
            "Loss:  0.25846457 #########   Accuracy:  0.8795685\n",
            "Epoch:  566\n",
            "Loss:  0.2565115 #########   Accuracy:  0.88210225\n",
            "Epoch:  567\n",
            "Loss:  0.25609684 #########   Accuracy:  0.88225585\n",
            "Epoch:  568\n",
            "Loss:  0.25654933 #########   Accuracy:  0.88144964\n",
            "Epoch:  569\n",
            "Loss:  0.25577518 #########   Accuracy:  0.8824094\n",
            "Epoch:  570\n",
            "Loss:  0.25756067 #########   Accuracy:  0.88125765\n",
            "Epoch:  571\n",
            "Loss:  0.25937626 #########   Accuracy:  0.8802595\n",
            "Epoch:  572\n",
            "Loss:  0.26184732 #########   Accuracy:  0.8771499\n",
            "Epoch:  573\n",
            "Loss:  0.26495084 #########   Accuracy:  0.87657404\n",
            "Epoch:  574\n",
            "Loss:  0.26185274 #########   Accuracy:  0.87929976\n",
            "Epoch:  575\n",
            "Loss:  0.2590797 #########   Accuracy:  0.88214064\n",
            "Epoch:  576\n",
            "Loss:  0.25645208 #########   Accuracy:  0.88167995\n",
            "Epoch:  577\n",
            "Loss:  0.26166612 #########   Accuracy:  0.87853193\n",
            "Epoch:  578\n",
            "Loss:  0.27268443 #########   Accuracy:  0.8733876\n",
            "Epoch:  579\n",
            "Loss:  0.27269605 #########   Accuracy:  0.87238944\n",
            "Epoch:  580\n",
            "Loss:  0.2654954 #########   Accuracy:  0.8773418\n",
            "Epoch:  581\n",
            "Loss:  0.27163592 #########   Accuracy:  0.8753455\n",
            "Epoch:  582\n",
            "Loss:  0.27088743 #########   Accuracy:  0.8742322\n",
            "Epoch:  583\n",
            "Loss:  0.26621273 #########   Accuracy:  0.8775338\n",
            "Epoch:  584\n",
            "Loss:  0.2642782 #########   Accuracy:  0.8791078\n",
            "Epoch:  585\n",
            "Loss:  0.26730603 #########   Accuracy:  0.8786471\n",
            "Epoch:  586\n",
            "Loss:  0.26382655 #########   Accuracy:  0.8791078\n",
            "Epoch:  587\n",
            "Loss:  0.25859275 #########   Accuracy:  0.8813345\n",
            "Epoch:  588\n",
            "Loss:  0.26553273 #########   Accuracy:  0.8790694\n",
            "Epoch:  589\n",
            "Loss:  0.26187325 #########   Accuracy:  0.8774954\n",
            "Epoch:  590\n",
            "Loss:  0.25863948 #########   Accuracy:  0.8806434\n",
            "Epoch:  591\n",
            "Loss:  0.26143506 #########   Accuracy:  0.87807125\n",
            "Epoch:  592\n",
            "Loss:  0.26183918 #########   Accuracy:  0.8785703\n",
            "Epoch:  593\n",
            "Loss:  0.25951934 #########   Accuracy:  0.8823326\n",
            "Epoch:  594\n",
            "Loss:  0.25883037 #########   Accuracy:  0.8811041\n",
            "Epoch:  595\n",
            "Loss:  0.262342 #########   Accuracy:  0.8802211\n",
            "Epoch:  596\n",
            "Loss:  0.2575074 #########   Accuracy:  0.88171834\n",
            "Epoch:  597\n",
            "Loss:  0.2557765 #########   Accuracy:  0.8836763\n",
            "Epoch:  598\n",
            "Loss:  0.25974005 #########   Accuracy:  0.88179517\n",
            "Epoch:  599\n",
            "Loss:  0.25678843 #########   Accuracy:  0.8832156\n",
            "Epoch:  600\n",
            "Loss:  0.25455785 #########   Accuracy:  0.8834459\n",
            "Epoch:  601\n",
            "Loss:  0.25468832 #########   Accuracy:  0.8835995\n",
            "Epoch:  602\n",
            "Loss:  0.25522837 #########   Accuracy:  0.88356113\n",
            "Epoch:  603\n",
            "Loss:  0.25408223 #########   Accuracy:  0.88417536\n",
            "Epoch:  604\n",
            "Loss:  0.2526372 #########   Accuracy:  0.8847128\n",
            "Epoch:  605\n",
            "Loss:  0.2545238 #########   Accuracy:  0.8838682\n",
            "Epoch:  606\n",
            "Loss:  0.2553538 #########   Accuracy:  0.88263977\n",
            "Epoch:  607\n",
            "Loss:  0.2545963 #########   Accuracy:  0.882371\n",
            "Epoch:  608\n",
            "Loss:  0.25272614 #########   Accuracy:  0.88478965\n",
            "Epoch:  609\n",
            "Loss:  0.2517643 #########   Accuracy:  0.88440573\n",
            "Epoch:  610\n",
            "Loss:  0.2539689 #########   Accuracy:  0.88313884\n",
            "Epoch:  611\n",
            "Loss:  0.25749376 #########   Accuracy:  0.8806818\n",
            "Epoch:  612\n",
            "Loss:  0.25886893 #########   Accuracy:  0.8794149\n",
            "Epoch:  613\n",
            "Loss:  0.25663173 #########   Accuracy:  0.88060504\n",
            "Epoch:  614\n",
            "Loss:  0.25495282 #########   Accuracy:  0.88394505\n",
            "Epoch:  615\n",
            "Loss:  0.25872234 #########   Accuracy:  0.8806818\n",
            "Epoch:  616\n",
            "Loss:  0.2640452 #########   Accuracy:  0.87323403\n",
            "Epoch:  617\n",
            "Loss:  0.25416142 #########   Accuracy:  0.88352275\n",
            "Epoch:  618\n",
            "Loss:  0.25511768 #########   Accuracy:  0.88356113\n",
            "Epoch:  619\n",
            "Loss:  0.26095062 #########   Accuracy:  0.87853193\n",
            "Epoch:  620\n",
            "Loss:  0.25837877 #########   Accuracy:  0.88298523\n",
            "Epoch:  621\n",
            "Loss:  0.25348434 #########   Accuracy:  0.8843289\n",
            "Epoch:  622\n",
            "Loss:  0.25825188 #########   Accuracy:  0.8794149\n",
            "Epoch:  623\n",
            "Loss:  0.2557542 #########   Accuracy:  0.88440573\n",
            "Epoch:  624\n",
            "Loss:  0.2541026 #########   Accuracy:  0.8853655\n",
            "Epoch:  625\n",
            "Loss:  0.2513622 #########   Accuracy:  0.8845593\n",
            "Epoch:  626\n",
            "Loss:  0.2547231 #########   Accuracy:  0.88398343\n",
            "Epoch:  627\n",
            "Loss:  0.25483793 #########   Accuracy:  0.8836763\n",
            "Epoch:  628\n",
            "Loss:  0.25222602 #########   Accuracy:  0.8845209\n",
            "Epoch:  629\n",
            "Loss:  0.25329646 #########   Accuracy:  0.8828317\n",
            "Epoch:  630\n",
            "Loss:  0.25270286 #########   Accuracy:  0.8830236\n",
            "Epoch:  631\n",
            "Loss:  0.2509317 #########   Accuracy:  0.8847512\n",
            "Epoch:  632\n",
            "Loss:  0.25015852 #########   Accuracy:  0.8861717\n",
            "Epoch:  633\n",
            "Loss:  0.25509116 #########   Accuracy:  0.88467443\n",
            "Epoch:  634\n",
            "Loss:  0.25126016 #########   Accuracy:  0.8847512\n",
            "Epoch:  635\n",
            "Loss:  0.25172597 #########   Accuracy:  0.8845209\n",
            "Epoch:  636\n",
            "Loss:  0.2553371 #########   Accuracy:  0.88164157\n",
            "Epoch:  637\n",
            "Loss:  0.25517505 #########   Accuracy:  0.8819871\n",
            "Epoch:  638\n",
            "Loss:  0.2520744 #########   Accuracy:  0.88459766\n",
            "Epoch:  639\n",
            "Loss:  0.25021458 #########   Accuracy:  0.885711\n",
            "Epoch:  640\n",
            "Loss:  0.24872182 #########   Accuracy:  0.885711\n",
            "Epoch:  641\n",
            "Loss:  0.24984334 #########   Accuracy:  0.8845593\n",
            "Epoch:  642\n",
            "Loss:  0.25301427 #########   Accuracy:  0.8840602\n",
            "Epoch:  643\n",
            "Loss:  0.2530497 #########   Accuracy:  0.8824478\n",
            "Epoch:  644\n",
            "Loss:  0.2528394 #########   Accuracy:  0.88267815\n",
            "Epoch:  645\n",
            "Loss:  0.25426167 #########   Accuracy:  0.88125765\n",
            "Epoch:  646\n",
            "Loss:  0.25071555 #########   Accuracy:  0.88390666\n",
            "Epoch:  647\n",
            "Loss:  0.24872279 #########   Accuracy:  0.8861717\n",
            "Epoch:  648\n",
            "Loss:  0.24948868 #########   Accuracy:  0.88501996\n",
            "Epoch:  649\n",
            "Loss:  0.25589976 #########   Accuracy:  0.87999076\n",
            "Epoch:  650\n",
            "Loss:  0.26986754 #########   Accuracy:  0.87277335\n",
            "Epoch:  651\n",
            "Loss:  0.26022944 #########   Accuracy:  0.87972206\n",
            "Epoch:  652\n",
            "Loss:  0.25945714 #########   Accuracy:  0.880106\n",
            "Epoch:  653\n",
            "Loss:  0.26207215 #########   Accuracy:  0.8754223\n",
            "Epoch:  654\n",
            "Loss:  0.25295052 #########   Accuracy:  0.8813345\n",
            "Epoch:  655\n",
            "Loss:  0.26241657 #########   Accuracy:  0.8762285\n",
            "Epoch:  656\n",
            "Loss:  0.26237983 #########   Accuracy:  0.87511516\n",
            "Epoch:  657\n",
            "Loss:  0.2533374 #########   Accuracy:  0.8824478\n",
            "Epoch:  658\n",
            "Loss:  0.26008043 #########   Accuracy:  0.8791078\n",
            "Epoch:  659\n",
            "Loss:  0.25759304 #########   Accuracy:  0.8789926\n",
            "Epoch:  660\n",
            "Loss:  0.25311846 #########   Accuracy:  0.8815264\n",
            "Epoch:  661\n",
            "Loss:  0.2589037 #########   Accuracy:  0.8800676\n",
            "Epoch:  662\n",
            "Loss:  0.25758764 #########   Accuracy:  0.87895423\n",
            "Epoch:  663\n",
            "Loss:  0.25458935 #########   Accuracy:  0.8807202\n",
            "Epoch:  664\n",
            "Loss:  0.2572094 #########   Accuracy:  0.8791078\n",
            "Epoch:  665\n",
            "Loss:  0.2527027 #########   Accuracy:  0.88144964\n",
            "Epoch:  666\n",
            "Loss:  0.2535584 #########   Accuracy:  0.88121927\n",
            "Epoch:  667\n",
            "Loss:  0.2544761 #########   Accuracy:  0.88079697\n",
            "Epoch:  668\n",
            "Loss:  0.25602868 #########   Accuracy:  0.8799524\n",
            "Epoch:  669\n",
            "Loss:  0.25045517 #########   Accuracy:  0.8834459\n",
            "Epoch:  670\n",
            "Loss:  0.25132132 #########   Accuracy:  0.8828317\n",
            "Epoch:  671\n",
            "Loss:  0.25385943 #########   Accuracy:  0.8805283\n",
            "Epoch:  672\n",
            "Loss:  0.24883465 #########   Accuracy:  0.88417536\n",
            "Epoch:  673\n",
            "Loss:  0.25123033 #########   Accuracy:  0.88252455\n",
            "Epoch:  674\n",
            "Loss:  0.25113255 #########   Accuracy:  0.88214064\n",
            "Epoch:  675\n",
            "Loss:  0.24727078 #########   Accuracy:  0.8851351\n",
            "Epoch:  676\n",
            "Loss:  0.24975036 #########   Accuracy:  0.88382983\n",
            "Epoch:  677\n",
            "Loss:  0.24698755 #########   Accuracy:  0.8850967\n",
            "Epoch:  678\n",
            "Loss:  0.24754833 #########   Accuracy:  0.8847512\n",
            "Epoch:  679\n",
            "Loss:  0.24714653 #########   Accuracy:  0.8854039\n",
            "Epoch:  680\n",
            "Loss:  0.24687846 #########   Accuracy:  0.88563424\n",
            "Epoch:  681\n",
            "Loss:  0.24620676 #########   Accuracy:  0.8860181\n",
            "Epoch:  682\n",
            "Loss:  0.24600294 #########   Accuracy:  0.88548064\n",
            "Epoch:  683\n",
            "Loss:  0.24575531 #########   Accuracy:  0.885711\n",
            "Epoch:  684\n",
            "Loss:  0.24503285 #########   Accuracy:  0.885519\n",
            "Epoch:  685\n",
            "Loss:  0.2448891 #########   Accuracy:  0.8857878\n",
            "Epoch:  686\n",
            "Loss:  0.24465755 #########   Accuracy:  0.8864404\n",
            "Epoch:  687\n",
            "Loss:  0.2445324 #########   Accuracy:  0.885711\n",
            "Epoch:  688\n",
            "Loss:  0.24480818 #########   Accuracy:  0.8859797\n",
            "Epoch:  689\n",
            "Loss:  0.24417756 #########   Accuracy:  0.8869395\n",
            "Epoch:  690\n",
            "Loss:  0.24400899 #########   Accuracy:  0.8874002\n",
            "Epoch:  691\n",
            "Loss:  0.24381283 #########   Accuracy:  0.8859413\n",
            "Epoch:  692\n",
            "Loss:  0.24306938 #########   Accuracy:  0.88667077\n",
            "Epoch:  693\n",
            "Loss:  0.2441764 #########   Accuracy:  0.8860949\n",
            "Epoch:  694\n",
            "Loss:  0.24478808 #########   Accuracy:  0.88501996\n",
            "Epoch:  695\n",
            "Loss:  0.24547178 #########   Accuracy:  0.885519\n",
            "Epoch:  696\n",
            "Loss:  0.2456878 #########   Accuracy:  0.886402\n",
            "Epoch:  697\n",
            "Loss:  0.2446002 #########   Accuracy:  0.88544226\n",
            "Epoch:  698\n",
            "Loss:  0.24517778 #########   Accuracy:  0.88482803\n",
            "Epoch:  699\n",
            "Loss:  0.24340203 #########   Accuracy:  0.8872466\n",
            "Epoch:  700\n",
            "Loss:  0.2424014 #########   Accuracy:  0.8868627\n",
            "Epoch:  701\n",
            "Loss:  0.24328819 #########   Accuracy:  0.88674754\n",
            "Epoch:  702\n",
            "Loss:  0.24870503 #########   Accuracy:  0.8872082\n",
            "Epoch:  703\n",
            "Loss:  0.24771513 #########   Accuracy:  0.88348436\n",
            "Epoch:  704\n",
            "Loss:  0.2479489 #########   Accuracy:  0.88256294\n",
            "Epoch:  705\n",
            "Loss:  0.24379295 #########   Accuracy:  0.887285\n",
            "Epoch:  706\n",
            "Loss:  0.2497283 #########   Accuracy:  0.88709307\n",
            "Epoch:  707\n",
            "Loss:  0.24468245 #########   Accuracy:  0.8855958\n",
            "Epoch:  708\n",
            "Loss:  0.2525081 #########   Accuracy:  0.88206387\n",
            "Epoch:  709\n",
            "Loss:  0.24991193 #########   Accuracy:  0.8847512\n",
            "Epoch:  710\n",
            "Loss:  0.2752852 #########   Accuracy:  0.8760365\n",
            "Epoch:  711\n",
            "Loss:  0.29181713 #########   Accuracy:  0.8629837\n",
            "Epoch:  712\n",
            "Loss:  0.27443862 #########   Accuracy:  0.8704699\n",
            "Epoch:  713\n",
            "Loss:  0.28235322 #########   Accuracy:  0.86855036\n",
            "Epoch:  714\n",
            "Loss:  0.27172908 #########   Accuracy:  0.8746161\n",
            "Epoch:  715\n",
            "Loss:  0.27425513 #########   Accuracy:  0.87365633\n",
            "Epoch:  716\n",
            "Loss:  0.2647555 #########   Accuracy:  0.8773802\n",
            "Epoch:  717\n",
            "Loss:  0.27000672 #########   Accuracy:  0.87549907\n",
            "Epoch:  718\n",
            "Loss:  0.26885414 #########   Accuracy:  0.8753839\n",
            "Epoch:  719\n",
            "Loss:  0.26432943 #########   Accuracy:  0.8783784\n",
            "Epoch:  720\n",
            "Loss:  0.26454782 #########   Accuracy:  0.87615174\n",
            "Epoch:  721\n",
            "Loss:  0.2645139 #########   Accuracy:  0.8760365\n",
            "Epoch:  722\n",
            "Loss:  0.2618719 #########   Accuracy:  0.87722665\n",
            "Epoch:  723\n",
            "Loss:  0.25987816 #########   Accuracy:  0.880106\n",
            "Epoch:  724\n",
            "Loss:  0.25528803 #########   Accuracy:  0.88225585\n",
            "Epoch:  725\n",
            "Loss:  0.2633559 #########   Accuracy:  0.8796453\n",
            "Epoch:  726\n",
            "Loss:  0.25503153 #########   Accuracy:  0.8808738\n",
            "Epoch:  727\n",
            "Loss:  0.25441936 #########   Accuracy:  0.88256294\n",
            "Epoch:  728\n",
            "Loss:  0.25408262 #########   Accuracy:  0.88306206\n",
            "Epoch:  729\n",
            "Loss:  0.25360465 #########   Accuracy:  0.88356113\n",
            "Epoch:  730\n",
            "Loss:  0.25490883 #########   Accuracy:  0.88171834\n",
            "Epoch:  731\n",
            "Loss:  0.25188458 #########   Accuracy:  0.884137\n",
            "Epoch:  732\n",
            "Loss:  0.25105762 #########   Accuracy:  0.88459766\n",
            "Epoch:  733\n",
            "Loss:  0.24949065 #########   Accuracy:  0.8849048\n",
            "Epoch:  734\n",
            "Loss:  0.24971326 #########   Accuracy:  0.88421375\n",
            "Epoch:  735\n",
            "Loss:  0.24958786 #########   Accuracy:  0.8860565\n",
            "Epoch:  736\n",
            "Loss:  0.24965467 #########   Accuracy:  0.8857494\n",
            "Epoch:  737\n",
            "Loss:  0.2479171 #########   Accuracy:  0.88478965\n",
            "Epoch:  738\n",
            "Loss:  0.24726237 #########   Accuracy:  0.8848664\n",
            "Epoch:  739\n",
            "Loss:  0.2470324 #########   Accuracy:  0.8854039\n",
            "Epoch:  740\n",
            "Loss:  0.24695747 #########   Accuracy:  0.88632524\n",
            "Epoch:  741\n",
            "Loss:  0.2466879 #########   Accuracy:  0.8860181\n",
            "Epoch:  742\n",
            "Loss:  0.24619287 #########   Accuracy:  0.885519\n",
            "Epoch:  743\n",
            "Loss:  0.2453575 #########   Accuracy:  0.8866324\n",
            "Epoch:  744\n",
            "Loss:  0.24511111 #########   Accuracy:  0.8862101\n",
            "Epoch:  745\n",
            "Loss:  0.2445265 #########   Accuracy:  0.88755375\n",
            "Epoch:  746\n",
            "Loss:  0.24447219 #########   Accuracy:  0.88628685\n",
            "Epoch:  747\n",
            "Loss:  0.24380961 #########   Accuracy:  0.8861333\n",
            "Epoch:  748\n",
            "Loss:  0.24356486 #########   Accuracy:  0.8870163\n",
            "Epoch:  749\n",
            "Loss:  0.24313454 #########   Accuracy:  0.886594\n",
            "Epoch:  750\n",
            "Loss:  0.24331826 #########   Accuracy:  0.8868243\n",
            "Epoch:  751\n",
            "Loss:  0.24323976 #########   Accuracy:  0.88590294\n",
            "Epoch:  752\n",
            "Loss:  0.24404626 #########   Accuracy:  0.8858262\n",
            "Epoch:  753\n",
            "Loss:  0.24724276 #########   Accuracy:  0.88417536\n",
            "Epoch:  754\n",
            "Loss:  0.25387576 #########   Accuracy:  0.8805283\n",
            "Epoch:  755\n",
            "Loss:  0.25138998 #########   Accuracy:  0.8836379\n",
            "Epoch:  756\n",
            "Loss:  0.2453473 #########   Accuracy:  0.88525033\n",
            "Epoch:  757\n",
            "Loss:  0.24626587 #########   Accuracy:  0.8838682\n",
            "Epoch:  758\n",
            "Loss:  0.25339732 #########   Accuracy:  0.880106\n",
            "Epoch:  759\n",
            "Loss:  0.26851824 #########   Accuracy:  0.8746161\n",
            "Epoch:  760\n",
            "Loss:  0.25935242 #########   Accuracy:  0.8783016\n",
            "Epoch:  761\n",
            "Loss:  0.254037 #########   Accuracy:  0.8812961\n",
            "Epoch:  762\n",
            "Loss:  0.2600231 #########   Accuracy:  0.8754607\n",
            "Epoch:  763\n",
            "Loss:  0.25216353 #########   Accuracy:  0.88141125\n",
            "Epoch:  764\n",
            "Loss:  0.2602368 #########   Accuracy:  0.8774954\n",
            "Epoch:  765\n",
            "Loss:  0.26476863 #########   Accuracy:  0.8735412\n",
            "Epoch:  766\n",
            "Loss:  0.25028655 #########   Accuracy:  0.8830236\n",
            "Epoch:  767\n",
            "Loss:  0.25869152 #########   Accuracy:  0.8786087\n",
            "Epoch:  768\n",
            "Loss:  0.25141257 #########   Accuracy:  0.88056666\n",
            "Epoch:  769\n",
            "Loss:  0.25446075 #########   Accuracy:  0.8799524\n",
            "Epoch:  770\n",
            "Loss:  0.26101637 #########   Accuracy:  0.8770731\n",
            "Epoch:  771\n",
            "Loss:  0.25010076 #########   Accuracy:  0.8819871\n",
            "Epoch:  772\n",
            "Loss:  0.2524133 #########   Accuracy:  0.8804899\n",
            "Epoch:  773\n",
            "Loss:  0.25496933 #########   Accuracy:  0.88014436\n",
            "Epoch:  774\n",
            "Loss:  0.24730006 #########   Accuracy:  0.88394505\n",
            "Epoch:  775\n",
            "Loss:  0.25009215 #########   Accuracy:  0.88313884\n",
            "Epoch:  776\n",
            "Loss:  0.24630617 #########   Accuracy:  0.8851735\n",
            "Epoch:  777\n",
            "Loss:  0.24912968 #########   Accuracy:  0.88336915\n",
            "Epoch:  778\n",
            "Loss:  0.24883726 #########   Accuracy:  0.88348436\n",
            "Epoch:  779\n",
            "Loss:  0.24446389 #########   Accuracy:  0.8855574\n",
            "Epoch:  780\n",
            "Loss:  0.24847019 #########   Accuracy:  0.88336915\n",
            "Epoch:  781\n",
            "Loss:  0.24509384 #########   Accuracy:  0.8849432\n",
            "Epoch:  782\n",
            "Loss:  0.2449765 #########   Accuracy:  0.8853655\n",
            "Epoch:  783\n",
            "Loss:  0.24544588 #########   Accuracy:  0.8850967\n",
            "Epoch:  784\n",
            "Loss:  0.24416336 #########   Accuracy:  0.8858262\n",
            "Epoch:  785\n",
            "Loss:  0.24285468 #########   Accuracy:  0.88624847\n",
            "Epoch:  786\n",
            "Loss:  0.2425586 #########   Accuracy:  0.886402\n",
            "Epoch:  787\n",
            "Loss:  0.2417792 #########   Accuracy:  0.88674754\n",
            "Epoch:  788\n",
            "Loss:  0.24275886 #########   Accuracy:  0.8868243\n",
            "Epoch:  789\n",
            "Loss:  0.24183737 #########   Accuracy:  0.88628685\n",
            "Epoch:  790\n",
            "Loss:  0.24287473 #########   Accuracy:  0.88628685\n",
            "Epoch:  791\n",
            "Loss:  0.24299534 #########   Accuracy:  0.8878609\n",
            "Epoch:  792\n",
            "Loss:  0.24331638 #########   Accuracy:  0.8882832\n",
            "Epoch:  793\n",
            "Loss:  0.2420727 #########   Accuracy:  0.88667077\n",
            "Epoch:  794\n",
            "Loss:  0.24307318 #########   Accuracy:  0.88563424\n",
            "Epoch:  795\n",
            "Loss:  0.24049182 #########   Accuracy:  0.8864404\n",
            "Epoch:  796\n",
            "Loss:  0.23992409 #########   Accuracy:  0.8882448\n",
            "Epoch:  797\n",
            "Loss:  0.23968795 #########   Accuracy:  0.8888974\n",
            "Epoch:  798\n",
            "Loss:  0.24090476 #########   Accuracy:  0.888168\n",
            "Epoch:  799\n",
            "Loss:  0.2442483 #########   Accuracy:  0.88820636\n",
            "Epoch:  800\n",
            "Loss:  0.23988834 #########   Accuracy:  0.8878993\n",
            "Epoch:  801\n",
            "Loss:  0.24350469 #########   Accuracy:  0.8847128\n",
            "Epoch:  802\n",
            "Loss:  0.24211252 #########   Accuracy:  0.8865556\n",
            "Epoch:  803\n",
            "Loss:  0.24128133 #########   Accuracy:  0.88759214\n",
            "Epoch:  804\n",
            "Loss:  0.24197781 #########   Accuracy:  0.8885135\n",
            "Epoch:  805\n",
            "Loss:  0.2377605 #########   Accuracy:  0.8882832\n",
            "Epoch:  806\n",
            "Loss:  0.24187824 #########   Accuracy:  0.8865556\n",
            "Epoch:  807\n",
            "Loss:  0.23849809 #########   Accuracy:  0.88843673\n",
            "Epoch:  808\n",
            "Loss:  0.24323316 #########   Accuracy:  0.8866324\n",
            "Epoch:  809\n",
            "Loss:  0.24171247 #########   Accuracy:  0.8865172\n",
            "Epoch:  810\n",
            "Loss:  0.23721737 #########   Accuracy:  0.88882065\n",
            "Epoch:  811\n",
            "Loss:  0.23770243 #########   Accuracy:  0.88951164\n",
            "Epoch:  812\n",
            "Loss:  0.23905864 #########   Accuracy:  0.8878225\n",
            "Epoch:  813\n",
            "Loss:  0.23995718 #########   Accuracy:  0.88716984\n",
            "Epoch:  814\n",
            "Loss:  0.2396411 #########   Accuracy:  0.8869779\n",
            "Epoch:  815\n",
            "Loss:  0.23726822 #########   Accuracy:  0.8893965\n",
            "Epoch:  816\n",
            "Loss:  0.23605417 #########   Accuracy:  0.8895885\n",
            "Epoch:  817\n",
            "Loss:  0.23831147 #########   Accuracy:  0.88839835\n",
            "Epoch:  818\n",
            "Loss:  0.23944332 #########   Accuracy:  0.8882832\n",
            "Epoch:  819\n",
            "Loss:  0.23868653 #########   Accuracy:  0.8872082\n",
            "Epoch:  820\n",
            "Loss:  0.2370192 #########   Accuracy:  0.8887055\n",
            "Epoch:  821\n",
            "Loss:  0.23644952 #########   Accuracy:  0.8895501\n",
            "Epoch:  822\n",
            "Loss:  0.23662028 #########   Accuracy:  0.88905096\n",
            "Epoch:  823\n",
            "Loss:  0.23809154 #########   Accuracy:  0.8881296\n",
            "Epoch:  824\n",
            "Loss:  0.23932694 #########   Accuracy:  0.8869395\n",
            "Epoch:  825\n",
            "Loss:  0.23754463 #########   Accuracy:  0.8888974\n",
            "Epoch:  826\n",
            "Loss:  0.23757762 #########   Accuracy:  0.8888974\n",
            "Epoch:  827\n",
            "Loss:  0.23617643 #########   Accuracy:  0.8902027\n",
            "Epoch:  828\n",
            "Loss:  0.23548964 #########   Accuracy:  0.88993394\n",
            "Epoch:  829\n",
            "Loss:  0.23559265 #########   Accuracy:  0.88928133\n",
            "Epoch:  830\n",
            "Loss:  0.23625846 #########   Accuracy:  0.8880912\n",
            "Epoch:  831\n",
            "Loss:  0.23573145 #########   Accuracy:  0.8893581\n",
            "Epoch:  832\n",
            "Loss:  0.23465294 #########   Accuracy:  0.88989556\n",
            "Epoch:  833\n",
            "Loss:  0.23442446 #########   Accuracy:  0.89031786\n",
            "Epoch:  834\n",
            "Loss:  0.23449223 #########   Accuracy:  0.88993394\n",
            "Epoch:  835\n",
            "Loss:  0.23431149 #########   Accuracy:  0.8904331\n",
            "Epoch:  836\n",
            "Loss:  0.23394899 #########   Accuracy:  0.89089376\n",
            "Epoch:  837\n",
            "Loss:  0.23448823 #########   Accuracy:  0.88947326\n",
            "Epoch:  838\n",
            "Loss:  0.23575002 #########   Accuracy:  0.88882065\n",
            "Epoch:  839\n",
            "Loss:  0.24174014 #########   Accuracy:  0.88624847\n",
            "Epoch:  840\n",
            "Loss:  0.24881206 #########   Accuracy:  0.8806434\n",
            "Epoch:  841\n",
            "Loss:  0.24681261 #########   Accuracy:  0.8860949\n",
            "Epoch:  842\n",
            "Loss:  0.23789194 #########   Accuracy:  0.889742\n",
            "Epoch:  843\n",
            "Loss:  0.24798729 #########   Accuracy:  0.88417536\n",
            "Epoch:  844\n",
            "Loss:  0.25923303 #########   Accuracy:  0.8777257\n",
            "Epoch:  845\n",
            "Loss:  0.2630752 #########   Accuracy:  0.87277335\n",
            "Epoch:  846\n",
            "Loss:  0.25149566 #########   Accuracy:  0.88083535\n",
            "Epoch:  847\n",
            "Loss:  0.2551272 #########   Accuracy:  0.88018274\n",
            "Epoch:  848\n",
            "Loss:  0.24781191 #########   Accuracy:  0.88379145\n",
            "Epoch:  849\n",
            "Loss:  0.2519199 #########   Accuracy:  0.88298523\n",
            "Epoch:  850\n",
            "Loss:  0.24495792 #########   Accuracy:  0.8844825\n",
            "Epoch:  851\n",
            "Loss:  0.24509755 #########   Accuracy:  0.8845209\n",
            "Epoch:  852\n",
            "Loss:  0.24458209 #########   Accuracy:  0.88501996\n",
            "Epoch:  853\n",
            "Loss:  0.24510826 #########   Accuracy:  0.8862101\n",
            "Epoch:  854\n",
            "Loss:  0.23900388 #########   Accuracy:  0.88820636\n",
            "Epoch:  855\n",
            "Loss:  0.24164733 #########   Accuracy:  0.8874002\n",
            "Epoch:  856\n",
            "Loss:  0.2410455 #########   Accuracy:  0.886594\n",
            "Epoch:  857\n",
            "Loss:  0.23722307 #########   Accuracy:  0.88878226\n",
            "Epoch:  858\n",
            "Loss:  0.23966703 #########   Accuracy:  0.88755375\n",
            "Epoch:  859\n",
            "Loss:  0.23869626 #########   Accuracy:  0.8870547\n",
            "Epoch:  860\n",
            "Loss:  0.23589166 #########   Accuracy:  0.8890126\n",
            "Epoch:  861\n",
            "Loss:  0.2366862 #########   Accuracy:  0.89012593\n",
            "Epoch:  862\n",
            "Loss:  0.23668371 #########   Accuracy:  0.88878226\n",
            "Epoch:  863\n",
            "Loss:  0.23496604 #########   Accuracy:  0.8899723\n",
            "Epoch:  864\n",
            "Loss:  0.23512343 #########   Accuracy:  0.8891662\n",
            "Epoch:  865\n",
            "Loss:  0.2356832 #########   Accuracy:  0.88962686\n",
            "Epoch:  866\n",
            "Loss:  0.23353688 #########   Accuracy:  0.8903946\n",
            "Epoch:  867\n",
            "Loss:  0.2340633 #########   Accuracy:  0.89008754\n",
            "Epoch:  868\n",
            "Loss:  0.23367631 #########   Accuracy:  0.8900108\n",
            "Epoch:  869\n",
            "Loss:  0.23295194 #########   Accuracy:  0.8905482\n",
            "Epoch:  870\n",
            "Loss:  0.23311639 #########   Accuracy:  0.8897804\n",
            "Epoch:  871\n",
            "Loss:  0.23276508 #########   Accuracy:  0.8913928\n",
            "Epoch:  872\n",
            "Loss:  0.23203225 #########   Accuracy:  0.890625\n",
            "Epoch:  873\n",
            "Loss:  0.23169206 #########   Accuracy:  0.89089376\n",
            "Epoch:  874\n",
            "Loss:  0.23163858 #########   Accuracy:  0.8914696\n",
            "Epoch:  875\n",
            "Loss:  0.23151782 #########   Accuracy:  0.890625\n",
            "Epoch:  876\n",
            "Loss:  0.23107895 #########   Accuracy:  0.89077854\n",
            "Epoch:  877\n",
            "Loss:  0.23076043 #########   Accuracy:  0.89120084\n",
            "Epoch:  878\n",
            "Loss:  0.23063622 #########   Accuracy:  0.891508\n",
            "Epoch:  879\n",
            "Loss:  0.23043591 #########   Accuracy:  0.89158475\n",
            "Epoch:  880\n",
            "Loss:  0.23066124 #########   Accuracy:  0.89154637\n",
            "Epoch:  881\n",
            "Loss:  0.23040035 #########   Accuracy:  0.8913928\n",
            "Epoch:  882\n",
            "Loss:  0.22999477 #########   Accuracy:  0.8910473\n",
            "Epoch:  883\n",
            "Loss:  0.23050386 #########   Accuracy:  0.8916999\n",
            "Epoch:  884\n",
            "Loss:  0.23049392 #########   Accuracy:  0.89047146\n",
            "Epoch:  885\n",
            "Loss:  0.23211254 #########   Accuracy:  0.8912777\n",
            "Epoch:  886\n",
            "Loss:  0.23671237 #########   Accuracy:  0.8885519\n",
            "Epoch:  887\n",
            "Loss:  0.25169203 #########   Accuracy:  0.88167995\n",
            "Epoch:  888\n",
            "Loss:  0.25081435 #########   Accuracy:  0.8800676\n",
            "Epoch:  889\n",
            "Loss:  0.23975447 #########   Accuracy:  0.8870547\n",
            "Epoch:  890\n",
            "Loss:  0.24642721 #########   Accuracy:  0.88352275\n",
            "Epoch:  891\n",
            "Loss:  0.24402602 #########   Accuracy:  0.88820636\n",
            "Epoch:  892\n",
            "Loss:  0.23732741 #########   Accuracy:  0.8900108\n",
            "Epoch:  893\n",
            "Loss:  0.24164289 #########   Accuracy:  0.8880912\n",
            "Epoch:  894\n",
            "Loss:  0.25198326 #########   Accuracy:  0.8849048\n",
            "Epoch:  895\n",
            "Loss:  0.24427606 #########   Accuracy:  0.88421375\n",
            "Epoch:  896\n",
            "Loss:  0.24934736 #########   Accuracy:  0.8840218\n",
            "Epoch:  897\n",
            "Loss:  0.24257399 #########   Accuracy:  0.8856726\n",
            "Epoch:  898\n",
            "Loss:  0.24532753 #########   Accuracy:  0.88797605\n",
            "Epoch:  899\n",
            "Loss:  0.23999248 #########   Accuracy:  0.88993394\n",
            "Epoch:  900\n",
            "Loss:  0.239597 #########   Accuracy:  0.8864788\n",
            "Epoch:  901\n",
            "Loss:  0.24285807 #########   Accuracy:  0.88425213\n",
            "Epoch:  902\n",
            "Loss:  0.23593384 #########   Accuracy:  0.8893581\n",
            "Epoch:  903\n",
            "Loss:  0.23654315 #########   Accuracy:  0.8908169\n",
            "Epoch:  904\n",
            "Loss:  0.23621173 #########   Accuracy:  0.8898188\n",
            "Epoch:  905\n",
            "Loss:  0.23300399 #########   Accuracy:  0.8903946\n",
            "Epoch:  906\n",
            "Loss:  0.23152213 #########   Accuracy:  0.8919303\n",
            "Epoch:  907\n",
            "Loss:  0.23231672 #########   Accuracy:  0.8904331\n",
            "Epoch:  908\n",
            "Loss:  0.23092158 #########   Accuracy:  0.8918151\n",
            "Epoch:  909\n",
            "Loss:  0.2320408 #########   Accuracy:  0.89135444\n",
            "Epoch:  910\n",
            "Loss:  0.23036456 #########   Accuracy:  0.8913928\n",
            "Epoch:  911\n",
            "Loss:  0.23089825 #########   Accuracy:  0.8910473\n",
            "Epoch:  912\n",
            "Loss:  0.23015799 #########   Accuracy:  0.8913928\n",
            "Epoch:  913\n",
            "Loss:  0.2312267 #########   Accuracy:  0.89131606\n",
            "Epoch:  914\n",
            "Loss:  0.23256901 #########   Accuracy:  0.89154637\n",
            "Epoch:  915\n",
            "Loss:  0.23197983 #########   Accuracy:  0.8914312\n",
            "Epoch:  916\n",
            "Loss:  0.2289728 #########   Accuracy:  0.89177674\n",
            "Epoch:  917\n",
            "Loss:  0.23127882 #########   Accuracy:  0.8907018\n",
            "Epoch:  918\n",
            "Loss:  0.23100369 #########   Accuracy:  0.89285165\n",
            "Epoch:  919\n",
            "Loss:  0.23134783 #########   Accuracy:  0.8933507\n",
            "Epoch:  920\n",
            "Loss:  0.22933146 #########   Accuracy:  0.8908169\n",
            "Epoch:  921\n",
            "Loss:  0.23153779 #########   Accuracy:  0.88835996\n",
            "Epoch:  922\n",
            "Loss:  0.22899881 #########   Accuracy:  0.8925061\n",
            "Epoch:  923\n",
            "Loss:  0.23203422 #########   Accuracy:  0.8918151\n",
            "Epoch:  924\n",
            "Loss:  0.23207739 #########   Accuracy:  0.8905482\n",
            "Epoch:  925\n",
            "Loss:  0.23305652 #########   Accuracy:  0.8899723\n",
            "Epoch:  926\n",
            "Loss:  0.24733524 #########   Accuracy:  0.8840602\n",
            "Epoch:  927\n",
            "Loss:  0.2502127 #########   Accuracy:  0.88060504\n",
            "Epoch:  928\n",
            "Loss:  0.24045546 #########   Accuracy:  0.8876305\n",
            "Epoch:  929\n",
            "Loss:  0.24501352 #########   Accuracy:  0.88563424\n",
            "Epoch:  930\n",
            "Loss:  0.25632545 #########   Accuracy:  0.88256294\n",
            "Epoch:  931\n",
            "Loss:  0.25306922 #########   Accuracy:  0.88356113\n",
            "Epoch:  932\n",
            "Loss:  0.24392863 #########   Accuracy:  0.8847512\n",
            "Epoch:  933\n",
            "Loss:  0.24871102 #########   Accuracy:  0.8805283\n",
            "Epoch:  934\n",
            "Loss:  0.24056439 #########   Accuracy:  0.8878609\n",
            "Epoch:  935\n",
            "Loss:  0.24085075 #########   Accuracy:  0.8873234\n",
            "Epoch:  936\n",
            "Loss:  0.24298067 #########   Accuracy:  0.8848664\n",
            "Epoch:  937\n",
            "Loss:  0.23740663 #########   Accuracy:  0.8888974\n",
            "Epoch:  938\n",
            "Loss:  0.2423727 #########   Accuracy:  0.8889358\n",
            "Epoch:  939\n",
            "Loss:  0.23893243 #########   Accuracy:  0.886402\n",
            "Epoch:  940\n",
            "Loss:  0.23688506 #########   Accuracy:  0.8880528\n",
            "Epoch:  941\n",
            "Loss:  0.24208546 #########   Accuracy:  0.88908935\n",
            "Epoch:  942\n",
            "Loss:  0.23565908 #########   Accuracy:  0.8891662\n",
            "Epoch:  943\n",
            "Loss:  0.23602848 #########   Accuracy:  0.88755375\n",
            "Epoch:  944\n",
            "Loss:  0.23441133 #########   Accuracy:  0.8893965\n",
            "Epoch:  945\n",
            "Loss:  0.2347404 #########   Accuracy:  0.89074016\n",
            "Epoch:  946\n",
            "Loss:  0.23346135 #########   Accuracy:  0.8912392\n",
            "Epoch:  947\n",
            "Loss:  0.23327716 #########   Accuracy:  0.8898572\n",
            "Epoch:  948\n",
            "Loss:  0.23621666 #########   Accuracy:  0.8910089\n",
            "Epoch:  949\n",
            "Loss:  0.23041497 #########   Accuracy:  0.89158475\n",
            "Epoch:  950\n",
            "Loss:  0.23401596 #########   Accuracy:  0.8876689\n",
            "Epoch:  951\n",
            "Loss:  0.23247474 #########   Accuracy:  0.88970363\n",
            "Epoch:  952\n",
            "Loss:  0.23017006 #########   Accuracy:  0.8925445\n",
            "Epoch:  953\n",
            "Loss:  0.23106289 #########   Accuracy:  0.8930052\n",
            "Epoch:  954\n",
            "Loss:  0.22863907 #########   Accuracy:  0.8919303\n",
            "Epoch:  955\n",
            "Loss:  0.22930364 #########   Accuracy:  0.8923526\n",
            "Epoch:  956\n",
            "Loss:  0.23514685 #########   Accuracy:  0.8918919\n",
            "Epoch:  957\n",
            "Loss:  0.22719415 #########   Accuracy:  0.89323556\n",
            "Epoch:  958\n",
            "Loss:  0.2346094 #########   Accuracy:  0.8894349\n",
            "Epoch:  959\n",
            "Loss:  0.23113474 #########   Accuracy:  0.893082\n",
            "Epoch:  960\n",
            "Loss:  0.23373136 #########   Accuracy:  0.8925061\n",
            "Epoch:  961\n",
            "Loss:  0.23136342 #########   Accuracy:  0.8908554\n",
            "Epoch:  962\n",
            "Loss:  0.23509991 #########   Accuracy:  0.88839835\n",
            "Epoch:  963\n",
            "Loss:  0.23157953 #########   Accuracy:  0.8903946\n",
            "Epoch:  964\n",
            "Loss:  0.23148817 #########   Accuracy:  0.89262134\n",
            "Epoch:  965\n",
            "Loss:  0.23146778 #########   Accuracy:  0.893082\n",
            "Epoch:  966\n",
            "Loss:  0.22912402 #########   Accuracy:  0.89116246\n",
            "Epoch:  967\n",
            "Loss:  0.23630121 #########   Accuracy:  0.8887055\n",
            "Epoch:  968\n",
            "Loss:  0.23284474 #########   Accuracy:  0.89135444\n",
            "Epoch:  969\n",
            "Loss:  0.23151006 #########   Accuracy:  0.89327395\n",
            "Epoch:  970\n",
            "Loss:  0.23299672 #########   Accuracy:  0.8910089\n",
            "Epoch:  971\n",
            "Loss:  0.2289207 #########   Accuracy:  0.8918535\n",
            "Epoch:  972\n",
            "Loss:  0.23235567 #########   Accuracy:  0.89242935\n",
            "Epoch:  973\n",
            "Loss:  0.22826792 #########   Accuracy:  0.8931204\n",
            "Epoch:  974\n",
            "Loss:  0.2300835 #########   Accuracy:  0.88966525\n",
            "Epoch:  975\n",
            "Loss:  0.23058277 #########   Accuracy:  0.8904331\n",
            "Epoch:  976\n",
            "Loss:  0.2276139 #########   Accuracy:  0.8931588\n",
            "Epoch:  977\n",
            "Loss:  0.23188215 #########   Accuracy:  0.89200705\n",
            "Epoch:  978\n",
            "Loss:  0.22934994 #########   Accuracy:  0.89135444\n",
            "Epoch:  979\n",
            "Loss:  0.24119711 #########   Accuracy:  0.8866324\n",
            "Epoch:  980\n",
            "Loss:  0.23095763 #########   Accuracy:  0.89031786\n",
            "Epoch:  981\n",
            "Loss:  0.23721138 #########   Accuracy:  0.8910089\n",
            "Epoch:  982\n",
            "Loss:  0.24539466 #########   Accuracy:  0.8842905\n",
            "Epoch:  983\n",
            "Loss:  0.25801072 #########   Accuracy:  0.87530714\n",
            "Epoch:  984\n",
            "Loss:  0.25063655 #########   Accuracy:  0.8831772\n",
            "Epoch:  985\n",
            "Loss:  0.25082195 #########   Accuracy:  0.88501996\n",
            "Epoch:  986\n",
            "Loss:  0.24994159 #########   Accuracy:  0.8820255\n",
            "Epoch:  987\n",
            "Loss:  0.24041013 #########   Accuracy:  0.8851351\n",
            "Epoch:  988\n",
            "Loss:  0.2392861 #########   Accuracy:  0.88709307\n",
            "Epoch:  989\n",
            "Loss:  0.2520663 #########   Accuracy:  0.88171834\n",
            "Epoch:  990\n",
            "Loss:  0.2383382 #########   Accuracy:  0.88970363\n",
            "Epoch:  991\n",
            "Loss:  0.2411529 #########   Accuracy:  0.8878225\n",
            "Epoch:  992\n",
            "Loss:  0.23997542 #########   Accuracy:  0.8861717\n",
            "Epoch:  993\n",
            "Loss:  0.23729512 #########   Accuracy:  0.89047146\n",
            "Epoch:  994\n",
            "Loss:  0.235342 #########   Accuracy:  0.89089376\n",
            "Epoch:  995\n",
            "Loss:  0.24186337 #########   Accuracy:  0.8872082\n",
            "Epoch:  996\n",
            "Loss:  0.23679997 #########   Accuracy:  0.8876689\n",
            "Epoch:  997\n",
            "Loss:  0.23615426 #########   Accuracy:  0.8908169\n",
            "Epoch:  998\n",
            "Loss:  0.23727416 #########   Accuracy:  0.8883216\n",
            "Epoch:  999\n",
            "Loss:  0.2392552 #########   Accuracy:  0.8872082\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDvzyT2PU2xi",
        "colab_type": "text"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHuu4U2AU16f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 947
        },
        "outputId": "84ef0651-266a-4612-bedf-5867096f0d9e"
      },
      "source": [
        "print(\"Test Accuracy: \", accuracy(y_test,classifier(X_test)).numpy() )\n",
        "print(A_test)\n",
        "print(\"Delta DP: \", delta_dp(classifier, dataset_test, A_test ))"
      ],
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy:  0.83919907\n",
            "tf.Tensor(\n",
            "[[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " ...\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]], shape=(16281, 1), dtype=float32)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-194-e895fcf85d7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test Accuracy: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Delta DP: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta_dp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA_test\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-193-a2e87fae308e>\u001b[0m in \u001b[0;36mdelta_dp\u001b[0;34m(model, dataset, sesn_atrr)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m   \u001b[0mDP_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDP_0_y_\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubtract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msesn_atrr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m   \u001b[0mDP_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDP_1_y_\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0msesn_atrr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmultiply\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_dispatch_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmul\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   5855\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5856\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5857\u001b[0;31m       \u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5858\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5859\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [26048,1] vs. [16281,1] [Op:Mul]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8Foivp84cdd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}